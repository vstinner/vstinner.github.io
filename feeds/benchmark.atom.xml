<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Victor Stinner blog 3 - benchmark</title><link href="https://vstinner.github.io/" rel="alternate"></link><link href="https://vstinner.github.io/feeds/benchmark.atom.xml" rel="self"></link><id>https://vstinner.github.io/</id><updated>2017-03-29T00:40:00+02:00</updated><entry><title>speed.python.org results: March 2017</title><link href="https://vstinner.github.io/speed-python-org-march-2017.html" rel="alternate"></link><published>2017-03-29T00:40:00+02:00</published><updated>2017-03-29T00:40:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:vstinner.github.io,2017-03-29:/speed-python-org-march-2017.html</id><summary type="html">&lt;p&gt;In feburary 2017, CPython from Bitbucket with Mercurial moved to GitHub with
Git: read &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-February/147381.html"&gt;[Python-Dev] CPython is now on GitHub&lt;/a&gt; by
Brett Cannon.&lt;/p&gt;
&lt;p&gt;In 2016, I worked on speed.python.org to automate running benchmarks and make
benchmarks more stable. At the end, I had a single command to:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;tune â€¦&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;In feburary 2017, CPython from Bitbucket with Mercurial moved to GitHub with
Git: read &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-February/147381.html"&gt;[Python-Dev] CPython is now on GitHub&lt;/a&gt; by
Brett Cannon.&lt;/p&gt;
&lt;p&gt;In 2016, I worked on speed.python.org to automate running benchmarks and make
benchmarks more stable. At the end, I had a single command to:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;tune the system for benchmarks&lt;/li&gt;
&lt;li&gt;compile CPython using LTO+PGO&lt;/li&gt;
&lt;li&gt;install CPython&lt;/li&gt;
&lt;li&gt;install performance&lt;/li&gt;
&lt;li&gt;run performance&lt;/li&gt;
&lt;li&gt;upload results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But my tools were written for Mercurial and speed.python.org uses Mercurial
revisions as keys for changes. Since the CPython repository was converted to
Git, I have to remove all old results and run again old benchmarks. But before
removing everyhing, I took screenshots of the most interesting pages. It would
prefer to keep a copy of all data, but it would require to write new tools
and I am not motivated to do that.&lt;/p&gt;
&lt;div class="section" id="python-3-7-compared-to-python-2-7"&gt;
&lt;h2&gt;Python 3.7 compared to Python 2.7&lt;/h2&gt;
&lt;p&gt;Benchmarks where Python 3.7 is &lt;strong&gt;faster&lt;/strong&gt; than Python 2.7:&lt;/p&gt;
&lt;img alt="python37_faster_py27" src="https://vstinner.github.io/images/speed2017/python37_faster_py27.png" /&gt;
&lt;p&gt;Benchmarks where Python 3.7 is &lt;strong&gt;slower&lt;/strong&gt; than Python 2.7:&lt;/p&gt;
&lt;img alt="python37_slower_py27" src="https://vstinner.github.io/images/speed2017/python37_slower_py27.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="significant-optimizations"&gt;
&lt;h2&gt;Significant optimizations&lt;/h2&gt;
&lt;p&gt;CPython became regulary faster in 2016 on the following benchmarks.&lt;/p&gt;
&lt;p&gt;call_method, the main optimized was &lt;a class="reference external" href="https://bugs.python.org/issue26110"&gt;Speedup method calls 1.2x&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="call_method" src="https://vstinner.github.io/images/speed2017/call_method.png" /&gt;
&lt;p&gt;float:&lt;/p&gt;
&lt;img alt="float" src="https://vstinner.github.io/images/speed2017/float.png" /&gt;
&lt;p&gt;hexiom:&lt;/p&gt;
&lt;img alt="hexiom" src="https://vstinner.github.io/images/speed2017/hexiom.png" /&gt;
&lt;p&gt;nqueens:&lt;/p&gt;
&lt;img alt="nqueens" src="https://vstinner.github.io/images/speed2017/nqueens.png" /&gt;
&lt;p&gt;pickle_list, something happened near September 2016:&lt;/p&gt;
&lt;img alt="pickle_list" src="https://vstinner.github.io/images/speed2017/pickle_list.png" /&gt;
&lt;p&gt;richards:&lt;/p&gt;
&lt;img alt="richards" src="https://vstinner.github.io/images/speed2017/richards.png" /&gt;
&lt;p&gt;scimark_lu, I like the latest dot!&lt;/p&gt;
&lt;img alt="scimark_lu" src="https://vstinner.github.io/images/speed2017/scimark_lu.png" /&gt;
&lt;p&gt;scimark_sor:&lt;/p&gt;
&lt;img alt="scimark_sor" src="https://vstinner.github.io/images/speed2017/scimark_sor.png" /&gt;
&lt;p&gt;sympy_sum:&lt;/p&gt;
&lt;img alt="sympy_sum" src="https://vstinner.github.io/images/speed2017/sympy_sum.png" /&gt;
&lt;p&gt;telco is one of the most impressive, it became regulary faster:&lt;/p&gt;
&lt;img alt="telco" src="https://vstinner.github.io/images/speed2017/telco.png" /&gt;
&lt;p&gt;unpickle_list, something happened between March and May 2016:&lt;/p&gt;
&lt;img alt="unpickle_list" src="https://vstinner.github.io/images/speed2017/unpickle_list.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="the-enum-change"&gt;
&lt;h2&gt;The enum change&lt;/h2&gt;
&lt;p&gt;One change related to the &lt;tt class="docutils literal"&gt;enum&lt;/tt&gt; module had significant impact on the two
following benchmarks.&lt;/p&gt;
&lt;p&gt;python_startup:&lt;/p&gt;
&lt;img alt="python_startup" src="https://vstinner.github.io/images/speed2017/python_startup.png" /&gt;
&lt;p&gt;See &amp;quot;Python startup performance regression&amp;quot; section of &lt;a class="reference external" href="https://vstinner.github.io/contrib-cpython-2016q4.html"&gt;My contributions to
CPython during 2016 Q4&lt;/a&gt; for the
explanation on changes around September 2016.&lt;/p&gt;
&lt;p&gt;regex_compile became 1.2x slower (312 ms =&amp;gt; 376 ms: +20%) because constants
of the &lt;tt class="docutils literal"&gt;re&lt;/tt&gt; module became &lt;tt class="docutils literal"&gt;enum&lt;/tt&gt; objects: see &lt;a class="reference external" href="http://bugs.python.org/issue28082"&gt;convert re flags to (much
friendlier) IntFlag constants (issue #28082)&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="regex_compile" src="https://vstinner.github.io/images/speed2017/regex_compile.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="benchmarks-became-stable"&gt;
&lt;h2&gt;Benchmarks became stable&lt;/h2&gt;
&lt;p&gt;The following benchmarks are microbenchmarks which are impacted by many
external factors. It's hard to get stable results. I'm happy to see that
results are stable. I would say very stable compared to results when I started
to work on the project!&lt;/p&gt;
&lt;p&gt;call_simple:&lt;/p&gt;
&lt;img alt="call_simple" src="https://vstinner.github.io/images/speed2017/call_simple.png" /&gt;
&lt;p&gt;spectral_norm:&lt;/p&gt;
&lt;img alt="spectral_norm" src="https://vstinner.github.io/images/speed2017/spectral_norm.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="straight-line"&gt;
&lt;h2&gt;Straight line&lt;/h2&gt;
&lt;p&gt;It seems like no optimization had a significant impact on the following
benchmarks. You can also see that benchmarks became stable, so it's easier to
detect performance regression or significant optimization.&lt;/p&gt;
&lt;p&gt;dulwich_log:&lt;/p&gt;
&lt;img alt="dulwich_log" src="https://vstinner.github.io/images/speed2017/dulwich_log.png" /&gt;
&lt;p&gt;pidigits:&lt;/p&gt;
&lt;img alt="pidigits" src="https://vstinner.github.io/images/speed2017/pidigits.png" /&gt;
&lt;p&gt;sqlite_synth:&lt;/p&gt;
&lt;img alt="sqlite_synth" src="https://vstinner.github.io/images/speed2017/sqlite_synth.png" /&gt;
&lt;p&gt;Apart something around April 2016, tornado_http result is stable:&lt;/p&gt;
&lt;img alt="tornado_http" src="https://vstinner.github.io/images/speed2017/tornado_http.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="unstable-benchmarks"&gt;
&lt;h2&gt;Unstable benchmarks&lt;/h2&gt;
&lt;p&gt;After months of efforts to make everything stable, some benchmarks are still
unstable, even if temporary spikes are lower than before. See &lt;a class="reference external" href="https://vstinner.github.io/analysis-python-performance-issue.html"&gt;Analysis of a
Python performance issue&lt;/a&gt;
to see the size of previous tempoary performance spikes.&lt;/p&gt;
&lt;p&gt;regex_v8:&lt;/p&gt;
&lt;img alt="regex_v8" src="https://vstinner.github.io/images/speed2017/regex_v8.png" /&gt;
&lt;p&gt;scimark_sparse_mat_mult:&lt;/p&gt;
&lt;img alt="scimark_sparse_mat_mult" src="https://vstinner.github.io/images/speed2017/scimark_sparse_mat_mult.png" /&gt;
&lt;p&gt;unpickle_pure_python:&lt;/p&gt;
&lt;img alt="unpickle_pure_python" src="https://vstinner.github.io/images/speed2017/unpickle_pure_python.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="boring-results"&gt;
&lt;h2&gt;Boring results&lt;/h2&gt;
&lt;p&gt;There is nothing interesting to say on the following benchmark results.&lt;/p&gt;
&lt;p&gt;2to3:&lt;/p&gt;
&lt;img alt="2to3" src="https://vstinner.github.io/images/speed2017/2to3.png" /&gt;
&lt;p&gt;crypto_pyaes:&lt;/p&gt;
&lt;img alt="crypto_pyaes" src="https://vstinner.github.io/images/speed2017/crypto_pyaes.png" /&gt;
&lt;p&gt;deltablue:&lt;/p&gt;
&lt;img alt="deltablue" src="https://vstinner.github.io/images/speed2017/deltablue.png" /&gt;
&lt;p&gt;logging_silent:&lt;/p&gt;
&lt;img alt="logging_silent" src="https://vstinner.github.io/images/speed2017/logging_silent.png" /&gt;
&lt;p&gt;mako:&lt;/p&gt;
&lt;img alt="mako" src="https://vstinner.github.io/images/speed2017/mako.png" /&gt;
&lt;p&gt;xml_etree_process:&lt;/p&gt;
&lt;img alt="xml_etree_process" src="https://vstinner.github.io/images/speed2017/xml_etree_process.png" /&gt;
&lt;p&gt;xml_etre_iterparse:&lt;/p&gt;
&lt;img alt="xml_etre_iterparse" src="https://vstinner.github.io/images/speed2017/xml_etre_iterparse.png" /&gt;
&lt;/div&gt;
</content><category term="benchmark"></category><category term="benchmark"></category></entry><entry><title>Analysis of a Python performance issue</title><link href="https://vstinner.github.io/analysis-python-performance-issue.html" rel="alternate"></link><published>2016-11-19T00:30:00+01:00</published><updated>2016-11-19T00:30:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:vstinner.github.io,2016-11-19:/analysis-python-performance-issue.html</id><summary type="html">&lt;p&gt;I am working on the CPython benchmark suite (&lt;a class="reference external" href="https://github.com/python/performance"&gt;performance&lt;/a&gt;) and I run the benchmark suite to
upload results to &lt;a class="reference external" href="http://speed.python.org/"&gt;speed.python.org&lt;/a&gt;. While
analying results, I noticed a temporary peak on the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt;
benchmark at October 19th:&lt;/p&gt;
&lt;img alt="call_method microbenchmark" src="https://vstinner.github.io/images/call_method.png" /&gt;
&lt;p&gt;The graphic shows the performance of the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt; microbenchmark between
Feb 29, 2016 â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am working on the CPython benchmark suite (&lt;a class="reference external" href="https://github.com/python/performance"&gt;performance&lt;/a&gt;) and I run the benchmark suite to
upload results to &lt;a class="reference external" href="http://speed.python.org/"&gt;speed.python.org&lt;/a&gt;. While
analying results, I noticed a temporary peak on the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt;
benchmark at October 19th:&lt;/p&gt;
&lt;img alt="call_method microbenchmark" src="https://vstinner.github.io/images/call_method.png" /&gt;
&lt;p&gt;The graphic shows the performance of the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt; microbenchmark between
Feb 29, 2016 and November 17, 2016 on the &lt;tt class="docutils literal"&gt;default&lt;/tt&gt; branch of CPython. The average
is around 17.2 ms, whereas the peak is at 29.0 ms: &lt;strong&gt;68% slower&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;The server has two &amp;quot;Intel(R) Xeon(R) CPU X5680  &amp;#64; 3.33GHz&amp;quot; CPUs, total: 24
logical cores (12 physical cores with HyperThreading). This CPU was launched in
2010 and based on the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Gulftown"&gt;Westmere-EP microarchitecture&lt;/a&gt;. Westmere-EP is based on Westmere,
which is the 32 nm shrink of the Nehalem microarchitecture.&lt;/p&gt;
&lt;div class="section" id="reproduce-results"&gt;
&lt;h2&gt;Reproduce results&lt;/h2&gt;
&lt;p&gt;Before going too far, the first step is to validate that results are
reproductible: reboot the computer, recompile Python, run again the benchmark.&lt;/p&gt;
&lt;p&gt;Instead of running the full benchmark suite, install Python, ..., we will run
directly the benchmark manually using the Python freshly built in its source
code directory.&lt;/p&gt;
&lt;p&gt;Interesting dots on the graphic (can be seen at speed.python.org, not on the
screenshot):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;678fe178da0d, Oct 09, 17.0 ms: &amp;quot;Fast&amp;quot;&lt;/li&gt;
&lt;li&gt;1ce50f7027c1, Oct 19, 28.9 ms: &amp;quot;Slow&amp;quot;&lt;/li&gt;
&lt;li&gt;36af3566b67a, Nov 3, 16.9 ms: Fast again&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I use the following directories:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;~/perf: GitHub haypo/perf project&lt;/li&gt;
&lt;li&gt;~/performance: GitHub python/performance project&lt;/li&gt;
&lt;li&gt;~/cpython: Mercurial CPython repository&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tune the system for benchmarks:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo python3 -m perf system tune
&lt;/pre&gt;
&lt;p&gt;Note: all &lt;tt class="docutils literal"&gt;system&lt;/tt&gt; commands in this article are optional. They help to reduce
the operating system jitter (make benchmarks more reliablee).&lt;/p&gt;
&lt;p&gt;Fast:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 678fe178da0d
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ mv python python-fast
$ PYTHONPATH=~/perf ./python-fast ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 17.0 ms +- 0.1 ms
&lt;/pre&gt;
&lt;p&gt;Slow:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 1ce50f7027c1
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ mv python python-slow
$ PYTHONPATH=~/perf ./python-slow ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 29.3 ms +- 0.9 ms
&lt;/pre&gt;
&lt;p&gt;We reproduced the significant benchmark result: 17 ms =&amp;gt; 29 ms.&lt;/p&gt;
&lt;p&gt;I use &lt;tt class="docutils literal"&gt;./configure&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;make clean&lt;/tt&gt; instead of incremental compilation,
&lt;tt class="docutils literal"&gt;make&lt;/tt&gt; command, to avoid compilation errors, and to avoid potential side
effects only caused by the incremental compilation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="analysis-with-the-linux-perf-tool"&gt;
&lt;h2&gt;Analysis with the Linux perf tool&lt;/h2&gt;
&lt;p&gt;To collect perf events, we will run the benchmark with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--worker&lt;/span&gt;&lt;/tt&gt; to run a
single process and with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-w0&lt;/span&gt; &lt;span class="pre"&gt;-n100&lt;/span&gt;&lt;/tt&gt; to run the benchmark long enough: 100
samples means at least 10 seconds (a single sample takes at least 100 ms).&lt;/p&gt;
&lt;p&gt;First, reset the system configuration to reset the Linux perf configuration:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo python3 -m perf system reset
&lt;/pre&gt;
&lt;p&gt;Note: &lt;tt class="docutils literal"&gt;python3 &lt;span class="pre"&gt;-m&lt;/span&gt; perf system tune&lt;/tt&gt; reduces the sampling rate of Linux perf
to reduce operating system jitter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="perf-stat"&gt;
&lt;h2&gt;perf stat&lt;/h2&gt;
&lt;p&gt;Command to get general statistics on the benchmark:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ perf stat ./python-slow ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -v -w0 -n100
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Fast&amp;quot; results:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for ./python-fast:

      3773.585194 task-clock (msec)         #    0.998 CPUs utilized
              369 context-switches          #    0.098 K/sec
                0 cpu-migrations            #    0.000 K/sec
            8,300 page-faults               #    0.002 M/sec
   12,981,234,867 cycles                    #    3.440 GHz                     [83.27%]
    1,460,980,720 stalled-cycles-frontend   #   11.25% frontend cycles idle    [83.36%]
      435,806,788 stalled-cycles-backend    #    3.36% backend  cycles idle    [66.72%]
   29,982,530,201 instructions              #    2.31  insns per cycle
                                            #    0.05  stalled cycles per insn [83.40%]
    5,613,631,616 branches                  # 1487.612 M/sec                   [83.40%]
       16,006,564 branch-misses             #    0.29% of all branches         [83.27%]

      3.780064486 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Slow&amp;quot; results:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for ./python-slow:

      5906.239860 task-clock (msec)         #    0.998 CPUs utilized
              556 context-switches          #    0.094 K/sec
                0 cpu-migrations            #    0.000 K/sec
            8,393 page-faults               #    0.001 M/sec
   20,651,474,102 cycles                    #    3.497 GHz                     [83.36%]
    8,480,803,345 stalled-cycles-frontend   #   41.07% frontend cycles idle    [83.37%]
    4,247,826,420 stalled-cycles-backend    #   20.57% backend  cycles idle    [66.64%]
   30,011,465,614 instructions              #    1.45  insns per cycle
                                            #    0.28  stalled cycles per insn [83.32%]
    5,612,485,730 branches                  #  950.264 M/sec                   [83.36%]
       13,584,136 branch-misses             #    0.24% of all branches         [83.29%]

      5.915402403 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;Significant differences, Fast =&amp;gt; Slow:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Instruction per cycle: 2.31 =&amp;gt; 1.45&lt;/li&gt;
&lt;li&gt;stalled-cycles-frontend: &lt;strong&gt;11.25% =&amp;gt; 41.07%&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;stalled-cycles-backend: &lt;strong&gt;3.36% =&amp;gt; 20.57%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The increase of stalled cycles is interesting. Since the code is supposed to be
identical, it probably means that fetching instructions is slower. It sounds
like an issue with CPU caches.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="statistics-on-the-cpu-l1-instruction-cache"&gt;
&lt;h2&gt;Statistics on the CPU L1 instruction cache&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf list&lt;/tt&gt; command can be used to get the name of events collecting
statistics on the CPU L1 instruction cache:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ perf list | grep L1
  L1-icache-loads                                    [Hardware cache event]
  L1-icache-load-misses                              [Hardware cache event]
  (...)
&lt;/pre&gt;
&lt;p&gt;Collect statistics on the CPU L1 instruction cache:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PYTHONPATH=~/perf perf stat -e L1-icache-loads,L1-icache-load-misses ./python-slow ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -w0 -n10
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Fast&amp;quot; statistics:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for './python-fast (...)':

   10,134,106,571 L1-icache-loads
       10,917,606 L1-icache-load-misses     #    0.11% of all L1-icache hits

      3.775067668 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Slow&amp;quot; statistics:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for './python-slow (...)':

   10,753,371,258 L1-icache-loads
      848,511,308 L1-icache-load-misses     #    7.89% of all L1-icache hits

      6.020490449 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;Cache misses on the L1 cache: &lt;strong&gt;0.1%&lt;/strong&gt; (Fast) =&amp;gt; &lt;strong&gt;8.0%&lt;/strong&gt; (Slow).&lt;/p&gt;
&lt;p&gt;The slow Python has &lt;strong&gt;71.7x more L1 cache misses&lt;/strong&gt; than the fast Python! It can
explain the significant performance drop.&lt;/p&gt;
&lt;div class="section" id="perf-report"&gt;
&lt;h3&gt;perf report&lt;/h3&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf record&lt;/tt&gt; command can be used to collect statistics on the functions
where the benchmark spends most of its time. Commands:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PYTHONPATH=~/perf perf record ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -v -w0 -n100
perf report
&lt;/pre&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
40.27%  python  python              [.] _PyEval_EvalFrameDefault
10.30%  python  python              [.] call_function
10.21%  python  python              [.] PyFrame_New
 8.56%  python  python              [.] frame_dealloc
 5.51%  python  python              [.] PyObject_GenericGetAttr
 (...)
&lt;/pre&gt;
&lt;p&gt;More than 64% of the time is spent in these 5 functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="system-tune"&gt;
&lt;h3&gt;system tune&lt;/h3&gt;
&lt;p&gt;To run benchmark, tune again the system for benchmarks:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo python3 -m perf system tune
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="hg-bisect"&gt;
&lt;h2&gt;hg bisect&lt;/h2&gt;
&lt;p&gt;To find the revision which introduces the performance slowdown, we use a
shell script to automate the bisection of the Mercurial history.&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;cmd.sh&lt;/tt&gt; script checking if a revision is fast or slow:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
set -e -x
./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
rm -f json
PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -o json -v
PYTHONPATH=~/perf python3 cmd.py json
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;cmd.sh&lt;/tt&gt; uses the following &lt;tt class="docutils literal"&gt;cmd.py&lt;/tt&gt; script which checks if the benchmark
is slow: if it takes longer than 23 ms (average between 17 ans 29 ms):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
import perf, sys
bench = perf.Benchmark.load('json')
bad = (29 + 17) / 2.0
ms = bench.median() * 1e3
if ms &amp;gt;= bad:
    print(&amp;quot;BAD! %.1f ms &amp;gt;= %.1f ms&amp;quot; % (ms, bad))
    sys.exit(1)
else:
    print(&amp;quot;good: %.1f ms &amp;lt; %.1f ms&amp;quot; % (ms, bad))
&lt;/pre&gt;
&lt;p&gt;In the bisection, &amp;quot;good&amp;quot; means &amp;quot;fast&amp;quot; (17 ms), whereas &amp;quot;bad&amp;quot; means &amp;quot;slow&amp;quot; (29
ms).  The peak, revision 1ce50f7027c1, is used as the first &amp;quot;bad&amp;quot; revision. The
previous fast revision before the peak is 678fe178da0d, our first &amp;quot;good&amp;quot;
revision.&lt;/p&gt;
&lt;p&gt;Commands to identify the first revision which introduced the slowdown:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg bisect --reset
hg bisect -b 1ce50f7027c1
hg bisect -g 678fe178da0d
time hg bisect -c ./cmd.sh
&lt;/pre&gt;
&lt;p&gt;3 min 52 sec later:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
The first bad revision is:
changeset:   104531:83877018ef97
parent:      104528:ce85a1f129e3
parent:      104530:2d352bf2b228
user:        Serhiy Storchaka &amp;lt;storchaka&amp;#64;gmail.com&amp;gt;
date:        Tue Oct 18 13:27:54 2016 +0300
files:       Misc/NEWS
description:
Issue #23782: Fixed possible memory leak in _PyTraceback_Add() and exception
loss in PyTraceBack_Here().
&lt;/pre&gt;
&lt;p&gt;Thank you &lt;tt class="docutils literal"&gt;hg bisect&lt;/tt&gt;! I love this tool.&lt;/p&gt;
&lt;p&gt;Even if I trust &lt;tt class="docutils literal"&gt;hg bisect&lt;/tt&gt;, I don't trust benchmarks, so I recheck manually:&lt;/p&gt;
&lt;p&gt;Slow:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 83877018ef97
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 29.4 ms +- 1.8 ms
&lt;/pre&gt;
&lt;p&gt;Use &lt;tt class="docutils literal"&gt;hg parents&lt;/tt&gt; to get the latest fast revision:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg parents -r 83877018ef97
changeset:   104528:ce85a1f129e3
(...)

changeset:   104530:2d352bf2b228
branch:      3.6
(...)
&lt;/pre&gt;
&lt;p&gt;Check the parent:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r ce85a1f129e3
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 17.1 ms +- 0.1 ms
&lt;/pre&gt;
&lt;p&gt;The revision ce85a1f129e3 is fast and the following revision 83877018ef97 is
slow. &lt;strong&gt;The revision 83877018ef97 introduced the slowdown&lt;/strong&gt;.  We found it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="analysis-of-the-revision-introducing-the-slowdown"&gt;
&lt;h2&gt;Analysis of the revision introducing the slowdown&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://hg.python.org/cpython/rev/83877018ef97/"&gt;revision 83877018ef97&lt;/a&gt;
changes two files: Misc/NEWS and Python/traceback.c. The NEWS file is only
documentation and so must not impact performances.  Python/traceback.c is part
of the C code and so is more interesting.&lt;/p&gt;
&lt;p&gt;The commit only changes two C functions: &lt;tt class="docutils literal"&gt;PyTraceBack_Here()&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;_PyTraceback_Add()&lt;/tt&gt;, but &lt;tt class="docutils literal"&gt;perf report&lt;/tt&gt; didn't show these functions as &amp;quot;hot&amp;quot;.
In fact, these functions are never called by the benchmark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The commit doesn't touch the C code used in the benchmark.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Unrelated C change impacting performances reminds me my previous &lt;a class="reference external" href="https://vstinner.github.io/journey-to-stable-benchmark-deadcode.html"&gt;deadcode
horror story&lt;/a&gt;. The performance
difference is probably caused by &lt;strong&gt;&amp;quot;code placement&amp;quot;&lt;/strong&gt;: &lt;tt class="docutils literal"&gt;perf stat&lt;/tt&gt; showed a
significant increase of the cache miss rate on the L1 instruction cache.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="use-gcc-attribute-hot"&gt;
&lt;h2&gt;Use GCC __attribute__((hot))&lt;/h2&gt;
&lt;p&gt;Using PGO compilation was the solution for deadcode, but PGO doesn't work on
Ubuntu 14.04 (the OS used by the benchmark server, speed-python) and PGO seems
to make benchmarks less reliable.&lt;/p&gt;
&lt;p&gt;I wanted to try something else: mark hot functions using the GCC
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((hot))&lt;/span&gt;&lt;/tt&gt; attribute. PGO compilation does this automatically.&lt;/p&gt;
&lt;p&gt;This attribute only has an impact on the code placement: where functions are
loaded in memory. The flag declares functions in the &lt;tt class="docutils literal"&gt;.text.hot&lt;/tt&gt; ELF section
rather than the &lt;tt class="docutils literal"&gt;.text&lt;/tt&gt; ELF section. Grouping hot functions in the same
functions helps to reduce the distance between functions and so enhance the
usage of CPU caches.&lt;/p&gt;
&lt;p&gt;I wrote and then pushed a patch in the &lt;a class="reference external" href="http://bugs.python.org/issue28618"&gt;issue #28618&lt;/a&gt;: &amp;quot;Decorate hot functions using
__attribute__((hot)) to optimize Python&amp;quot;.&lt;/p&gt;
&lt;p&gt;The patch marks 6 functions as hot:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;_PyEval_EvalFrameDefault()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;call_function()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;_PyFunction_FastCall()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;PyFrame_New()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;frame_dealloc()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;PyErr_Occurred()&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's try the patch:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 83877018ef97
$ wget https://hg.python.org/cpython/raw-rev/59b91b4e9506 -O patch
$ patch -p1 &amp;lt; patch
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 16.7 ms +- 0.3 ms
&lt;/pre&gt;
&lt;p&gt;It's easy to make mistakes and benchmarks are always suprising, so let's retry
without the patch:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 83877018ef97
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 29.3 ms +- 0.6 ms
&lt;/pre&gt;
&lt;p&gt;The check confirms that the GCC attribute fixed the issue!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;On modern Intel CPUs, the code placement can have a major impact on the
performance of microbenchmarks.&lt;/p&gt;
&lt;p&gt;The GCC &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((hot))&lt;/span&gt;&lt;/tt&gt; attribute can be used manually to make &amp;quot;hot
functions&amp;quot; close in memory to enhance the usage of CPU caches.&lt;/p&gt;
&lt;p&gt;To know more about the impact of code placement, see the very good talk of Zia
Ansari (Intel) at the LLVM Developers' Meeting 2016: &lt;a class="reference external" href="https://llvmdevelopersmeetingbay2016.sched.org/event/8YzY/causes-of-performance-instability-due-to-code-placement-in-x86"&gt;Causes of Performance
Swings Due to Code Placement in IA&lt;/a&gt;.
He describes well &amp;quot;performance swings&amp;quot; like the one described in this article
and explains how CPUs work internally and how code placement impacts CPU
performances.&lt;/p&gt;
&lt;/div&gt;
</content><category term="benchmark"></category><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>Intel CPUs (part 2): Turbo Boost, temperature, frequency and Pstate C0 bug</title><link href="https://vstinner.github.io/intel-cpus-part2.html" rel="alternate"></link><published>2016-09-23T23:00:00+02:00</published><updated>2016-09-23T23:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:vstinner.github.io,2016-09-23:/intel-cpus-part2.html</id><summary type="html">&lt;p class="first last"&gt;Intel CPUs (part 2): Turbo Boost, temperature, frequency and Pstate C0 bug&lt;/p&gt;
</summary><content type="html">&lt;p&gt;My first article &lt;a class="reference external" href="https://vstinner.github.io/intel-cpus.html"&gt;Intel CPUs&lt;/a&gt; is a general
introduction on modern CPU technologies having an impact on benchmarks.&lt;/p&gt;
&lt;p&gt;This second article is much more concrete with numbers and a concrete bug
having a major impact on benchmarks: a benchmark suddenly becomes 2x faster!&lt;/p&gt;
&lt;p&gt;I will tell you how I first noticed the bug, which tests I ran to analyze the
issue, how I found commands to reproduce the bug, and finally how I identified
the bug.&lt;/p&gt;
&lt;div class="section" id="glitch-in-benchmarks"&gt;
&lt;h2&gt;&amp;quot;Glitch&amp;quot; in benchmarks&lt;/h2&gt;
&lt;p&gt;Last week I ran a benchmark to check if enabling Profile Guided Optimization
(PGO) when compiling Python makes benchmark results less stable. I recompiled
Python 5 times, and after each compilation I ran a benchmark. I tested
different commands and options to compile Python. Everything was fine until
the last benchmark of the last compilation. &lt;strong&gt;The benchmark suddenly became 2
times faster.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hopefully, my perf module collects a lot of metadata. I was able to analyze
in depth what happened.&lt;/p&gt;
&lt;p&gt;The &amp;quot;glitch&amp;quot; occurred in a benchmark having 400 runs (benchmark run in 400
different processes), between the run 105 (20.3 ms) and the run 106
(11.0 ms).&lt;/p&gt;
&lt;p&gt;I noticed that the CPU temperature was between 69Â°C and 72Â°C until the run 105,
and then decreased to from 69Â°C to 58Â°C.&lt;/p&gt;
&lt;p&gt;The system load slowly increased from 1.25 up to 1.62 around the run 108 and
then slowly decreased to 1.00.&lt;/p&gt;
&lt;p&gt;The system was not idle while the benchmark was running. I was working on the
PC too! But according to timestamps, it seems like the glitch was close to when
I stopped working. When I stopped working, I closed all applications (except of
the benchmark running in background) and turned of my two monitors.&lt;/p&gt;
&lt;p&gt;Well, at this point, it's hard to correlate for sure an event with the major
performance change.&lt;/p&gt;
&lt;p&gt;So I started to analyze different factors affecting CPUs and benchmarks: Turbo
Boost, CPU temperature and CPU frequency.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="impact-of-turbo-boost-on-benchmarks"&gt;
&lt;h2&gt;Impact of Turbo Boost on benchmarks&lt;/h2&gt;
&lt;p&gt;Without Turbo Boost, the maximum frequency of the &amp;quot;Intel(R) Core(TM) i7-3520M
CPU &amp;#64; 2.90GHz&amp;quot; of my laptop is 2.9 GHz. With Turbo Boost, the maximum
frequency is 3.6 GHz if only one core is active, or 3.4 GHz otherwise:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ sudo cpupower frequency-info
  ...
  boost state support:
    Supported: yes
    Active: yes
    3400 MHz max turbo 4 active cores
    3400 MHz max turbo 3 active cores
    3400 MHz max turbo 2 active cores
    3600 MHz max turbo 1 active cores
&lt;/pre&gt;
&lt;p&gt;I ran the bm_call_simple.py microbenchmark (CPU-bound) of performance 0.2.2.&lt;/p&gt;
&lt;p&gt;Turbo Boost disabled:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1 physical CPU active: 2.9 GHz, Median +- std dev: 14.6 ms +- 0.3 ms&lt;/li&gt;
&lt;li&gt;2 physical CPU active: 2.9 GHz, Median +- std dev: 14.7 ms +- 0.5 ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Turbo Boost enabled:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1 physical CPU active: 3.6 GHz, Median +- std dev: 11.8 ms +- 0.3 ms&lt;/li&gt;
&lt;li&gt;2 physical CPU active: 3.4 GHz, Median +- std dev: 12.4 ms +- 0.1 ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The maximum performance boost is 19% faster&lt;/strong&gt; (14.6 ms =&amp;gt; 11.8 ms), the
minimum boost if 15% faster (14.6 ms =&amp;gt; 12.4 ms).&lt;/p&gt;
&lt;p&gt;Hum, I don't think that Turbo Boost can explain the bug.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="impact-of-the-cpu-temperature-on-benchmarks"&gt;
&lt;h2&gt;Impact of the CPU temperature on benchmarks&lt;/h2&gt;
&lt;p&gt;The CPU temperature is mentionned in Intel Turbo Boost documentation as a
factor used to decide which P-state will be used. I always wanted to check how
the CPU temperature impacts its performance.&lt;/p&gt;
&lt;div class="section" id="burn-the-cpu-of-my-desktop-pc"&gt;
&lt;h3&gt;Burn the CPU of my desktop PC&lt;/h3&gt;
&lt;p&gt;CPU of my desktop PC: &amp;quot;Intel(R) Core(TM) i7-2600 CPU &amp;#64; 3.40GHz&amp;quot;.&lt;/p&gt;
&lt;p&gt;I used my &lt;a class="reference external" href="https://github.com/vstinner/misc/blob/master/bin/system_load.py"&gt;system_load.py script&lt;/a&gt; to generate a
system load higher than 10.&lt;/p&gt;
&lt;p&gt;When the fan is cooling correctly the CPU, all CPU run at 3.4 GHz (Turbo Boost
was disabled) and the CPU temperature is 66Â°C.&lt;/p&gt;
&lt;p&gt;I used a simple sheet of paper to block the fan of my CPU. Yeah, I really
wanted to &lt;a class="reference external" href="https://www.youtube.com/watch?v=Xf0VuRG7MN4"&gt;burn my CPU&lt;/a&gt;! More
seriously, I checked the CPU temperature every second using the &lt;tt class="docutils literal"&gt;sensors&lt;/tt&gt;
command and was prepared to unblock the fan if sometimes gone wrong.&lt;/p&gt;
&lt;img alt="Sheet of paper blocking the CPU fan" src="https://vstinner.github.io/images/paper_blocks_cpu_fan.jpg" /&gt;
&lt;p&gt;After one minute, the CPU reached 97Â°C. I expected a system crash, smoke or
something worse, but I was disappointed. &lt;strong&gt;At 97Â°C, I was still able to use my
computer as everything was fine. The CPU was slowly down automatically to the
minimum CPU frequency: 1533 MHz&lt;/strong&gt; according to turbostat (the minimum frequency
of this CPU is 1.6 GHz).&lt;/p&gt;
&lt;p&gt;When I unblocked the fan, the temperature decreased quickly to go back to its
previous state (62Â°C) and the CPU frequency quickly increased to 3.4 GHz as
well.&lt;/p&gt;
&lt;p&gt;My Intel CPU is really impressive! I didn't expect such very efficient
protection against overheating!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="burn-my-laptop-cpu"&gt;
&lt;h3&gt;Burn my laptop CPU&lt;/h3&gt;
&lt;p&gt;I used my system_load.py script to get a system load over 200. I also opened 4
tabs in Firefox playing Youtube videos to stress also the GPU which is
integrated into the CPU (IGP) on such laptop.&lt;/p&gt;
&lt;img alt="Stress test playing Youtube videos in Firefox, CPU at 102Â°" src="https://vstinner.github.io/images/burn_cpu_firefox.jpg" /&gt;
&lt;p&gt;With such crazy stress test, the CPU temperature was &amp;quot;only&amp;quot; 83Â°C.&lt;/p&gt;
&lt;p&gt;Using a simple tissue, I closed the air hole used by the CPU fan. &lt;strong&gt;When the
CPU temperature increased from 100Â°C to 101Â°C, the CPU frequency started slowly
to decrease from 3391 MHz to 3077 MHz&lt;/strong&gt; (with steps between 10 MHz and 50 MHz
every second, or something like that).&lt;/p&gt;
&lt;p&gt;When pushing hard the tissue and waiting longer than 5 minutes, the CPU
temperature increased up to 102Â°C, but the CPU frequency was only decreased
from 3.4 GHz (Turbo Mode with 4 active logical CPUs) to 3.1 GHz.&lt;/p&gt;
&lt;p&gt;The maximum frequency is 2.9 GHz. Frequencies higher than 2.9 GHz means that
the Turbo Mode was enabled! It means that &lt;strong&gt;even with overheating, the CPU is
still fine and able to &amp;quot;overclock&amp;quot; itself!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, I was disapointed. With a CPU at 102Â°C, my laptop was still super fast
and reactive.  It seems like mobile CPUs handle even better overheating than
desktop CPUs (which is not something suprising at all).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="impact-of-the-cpu-frequency-on-benchmarks"&gt;
&lt;h2&gt;Impact of the CPU frequency on benchmarks&lt;/h2&gt;
&lt;p&gt;I ran the bm_call_simple.py microbenchmark (CPU-bound) of performance 0.2.2
on my desktop PC.&lt;/p&gt;
&lt;p&gt;Command to set the frequency of CPU 0 to the minimum frequency (1.6 GHz):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_min_freq|sudo tee  /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq
1600000
&lt;/pre&gt;
&lt;p&gt;Command to set the frequency of CPU 0 to the maximum frequency (3.4 GHz):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq|sudo tee  /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq
3400000
&lt;/pre&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;CPU running at 1.6 GHz (min freq): Median +- std dev: 27.7 ms +- 0.7 ms&lt;/li&gt;
&lt;li&gt;CPU running at 3.4 GHz (min freq): Median +- std dev: 12.9 ms +- 0.2 ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The impact of the CPU frequency is quite obvious: &lt;strong&gt;when the CPU frequency is
doubled, the performance is also doubled&lt;/strong&gt;. The benchmark is 53% faster (27.7
ms =&amp;gt; 12.9 ms).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bug-reproduced-and-then-identified-in-the-linux-cpu-driver"&gt;
&lt;h2&gt;Bug reproduced and then identified in the Linux CPU driver&lt;/h2&gt;
&lt;p&gt;Two days ago, I ran a very simple &amp;quot;timeit&amp;quot; microbenchmark to try to bisect a
performance regression in Python 3.6 on &lt;tt class="docutils literal"&gt;functools.partial&lt;/tt&gt;. Again, suddenly,
the microbenchmark became 2x faster!&lt;/p&gt;
&lt;p&gt;But this time, I found something: I noticed that running or stopping &lt;tt class="docutils literal"&gt;cpupower
monitor&lt;/tt&gt; and/or &lt;tt class="docutils literal"&gt;turbostat&lt;/tt&gt; can &amp;quot;enable&amp;quot; or &amp;quot;disable&amp;quot; the bug.&lt;/p&gt;
&lt;p&gt;After a lot of tests, I understood that running the benchmark with turbostat
&amp;quot;disables&amp;quot; the bug, whereas running &amp;quot;cpupower monitor&amp;quot; while running a
benchmark enables the bug.&lt;/p&gt;
&lt;p&gt;I reported the bug in the Fedora bug tracker, on the component kernel:
&lt;a class="reference external" href="https://bugzilla.redhat.com/show_bug.cgi?id=1378529"&gt;intel_pstate C0 bug on isolated CPUs with the performance governor and
NOHZ_FULL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It seems like the bug is related to CPU isolation and NOHZ_FULL. The NOHZ_FULL
option is able to fully disable the scheduler clock interruption  on isolated
CPUs. I understood the the &lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; driver uses a callback on the
scheduler to update the Pstate of the CPU. According to an Intel engineer, the
&lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; driver was never tested with CPU isolation.&lt;/p&gt;
&lt;p&gt;The issue is not fully analyzed yet, but at least I succeeded to write a list
of commands to reproduce it with a success rate of 100% :-) Moreover, the Intel
engineer suggested to add an extra parameter to the Linux kernel command
(&lt;tt class="docutils literal"&gt;rcu_nocbs=3,7&lt;/tt&gt;) line which works around the issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This article describes how I found and then identified a bug in the Linux
driver of my CPU.&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The maximum speedup of Turbo Boost is 20%&lt;/li&gt;
&lt;li&gt;Overheating on a dekstop PC can decrease the CPU frequency to its minimum
(half of the maximum in my case) which imply a slowdown of 50%&lt;/li&gt;
&lt;li&gt;A bug in the Linux CPU driver changes suddenly the CPU frequency from its
minimum to maximum (or the opposite) which means a speedup of 50%
(or slowdown of 50%)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;To get stable benchmarks, the safest fix for all these issues is probably to
set the CPU frequency of the CPUs used by benchmarks to the minimum.&lt;/strong&gt;
It seems like nothing can reduce the frequency of a CPU below its minimum.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When running benchmarks, raw timings and CPU performance don't matter. Only
comparisons between benchmark results and stable performances matter.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</content><category term="benchmark"></category><category term="optimization"></category><category term="benchmark"></category><category term="cpu"></category></entry><entry><title>Intel CPUs: P-state, C-state, Turbo Boost, CPU frequency, etc.</title><link href="https://vstinner.github.io/intel-cpus.html" rel="alternate"></link><published>2016-07-15T12:00:00+02:00</published><updated>2016-07-15T12:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:vstinner.github.io,2016-07-15:/intel-cpus.html</id><summary type="html">&lt;p class="first last"&gt;Intel CPUs: Hyper-threading, Turbo Boost, CPU frequency, etc.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Ten years ago, most computers were desktop computers designed for best
performances and their CPU frequency was fixed. Nowadays, most devices are
embedded and use &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Low-power_electronics"&gt;low power consumption&lt;/a&gt; processors like ARM
CPUs. The power consumption now matters more than performance peaks.&lt;/p&gt;
&lt;p&gt;Intel CPUs evolved from a single core to multiple physical cores in the same
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/CPU_socket"&gt;package&lt;/a&gt; and got new features:
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Hyper-threading"&gt;Hyper-threading&lt;/a&gt; to run two
threads on the same physical core and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Turbo_Boost"&gt;Turbo Boost&lt;/a&gt; to maximum performances.
CPU cores can be completely turned off (CPU HALT, frequency of 0) temporarily to
reduce the power consumption, and the frequency of cores changes regulary
depending on many factors like the workload and temperature. The power
consumption is now an important part in the design of modern CPUs.&lt;/p&gt;
&lt;p&gt;Warning! This article is a summary of what I learnt last weeks from random
articles. It may be full of mistakes, don't hesitate to report them, so I can
enhance the article! It's hard to find simple articles explaining performances
of modern Intel CPUs, so I tried to write mine.&lt;/p&gt;
&lt;div class="section" id="tools-used-in-this-article"&gt;
&lt;h2&gt;Tools used in this article&lt;/h2&gt;
&lt;p&gt;This article mentions various tools. Commands to install them on Fedora 24:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;dnf install &lt;span class="pre"&gt;-y&lt;/span&gt; &lt;span class="pre"&gt;util-linux&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;lscpu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;dnf install &lt;span class="pre"&gt;-y&lt;/span&gt; &lt;span class="pre"&gt;kernel-tools&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://linux.die.net/man/1/cpupower"&gt;cpupower&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;turbostat&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;sudo dnf install &lt;span class="pre"&gt;-y&lt;/span&gt; &lt;span class="pre"&gt;msr-tools&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;rdmsr&lt;/li&gt;
&lt;li&gt;wrmsr&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other interesting tools, not used in this article: i7z (sadly no more
maintained), lshw, dmidecode, sensors.&lt;/p&gt;
&lt;p&gt;The sensors tool is supposed to report the current CPU voltage, but it doesn't
provide this information on my computers. At least, it gives the temperature of
different components, but also the speed of fans.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-of-intel-cpus"&gt;
&lt;h2&gt;Example of Intel CPUs&lt;/h2&gt;
&lt;div class="section" id="my-laptop-cpu-proc-cpuinfo"&gt;
&lt;h3&gt;My laptop CPU: /proc/cpuinfo&lt;/h3&gt;
&lt;p&gt;On Linux, the most common way to retrieve information on the CPU is to read
&lt;tt class="docutils literal"&gt;/proc/cpuinfo&lt;/tt&gt;. Example on my laptop:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cat /proc/cpuinfo
processor  : 0
vendor_id  : GenuineIntel
model name : Intel(R) Core(TM) i7-3520M CPU &amp;#64; 2.90GHz
cpu MHz    : 1200.214
...

processor  : 1
vendor_id  : GenuineIntel
model name : Intel(R) Core(TM) i7-3520M CPU &amp;#64; 2.90GHz
cpu MHz    : 3299.882
...
&lt;/pre&gt;
&lt;p&gt;&amp;quot;i7-3520M&amp;quot; CPU is a model designed for Mobile Platforms (see the &amp;quot;M&amp;quot; suffix).
It was built in 2012 and is the third generation of the Intel i7
microarchitecture: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Ivy_Bridge_(microarchitecture)"&gt;Ivy Bridge&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The CPU has two physical cores, I disabled HyperThreading in the BIOS.&lt;/p&gt;
&lt;p&gt;The first strange thing is that the CPU announces &amp;quot;2.90 GHz&amp;quot; but Linux reports
1.2 GHz on the first core, and 3.3 GHz on the second core. 3.3 GHz is greater
than 2.9 GHz!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="my-desktop-cpu-cpu-topology-with-lscpu"&gt;
&lt;h3&gt;My desktop CPU: CPU topology with lscpu&lt;/h3&gt;
&lt;p&gt;cpuinfo:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
smithers$ cat /proc/cpuinfo
processor   : 0
physical id : 0
core id     : 0
...
model name  : Intel(R) Core(TM) i7-2600 CPU &amp;#64; 3.40GHz
cpu cores   : 4
...

processor   : 1
physical id : 0
core id     : 1
...

(...)

processor   : 7
physical id : 0
core id     : 3
...
&lt;/pre&gt;
&lt;p&gt;The CPU i7-2600 is the 2nd generation: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sandy_Bridge"&gt;Sandy Bridge microarchitecture&lt;/a&gt;. There are 8 logical cores and 4
physical cores (so with Hyper-threading).&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;lscpu&lt;/tt&gt; renders a short table which helps to understand the CPU topology:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
smithers$ lscpu -a -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE MAXMHZ    MINMHZ
0   0    0      0    0:0:0:0       yes    3800.0000 1600.0000
1   0    0      1    1:1:1:0       yes    3800.0000 1600.0000
2   0    0      2    2:2:2:0       yes    3800.0000 1600.0000
3   0    0      3    3:3:3:0       yes    3800.0000 1600.0000
4   0    0      0    0:0:0:0       yes    3800.0000 1600.0000
5   0    0      1    1:1:1:0       yes    3800.0000 1600.0000
6   0    0      2    2:2:2:0       yes    3800.0000 1600.0000
7   0    0      3    3:3:3:0       yes    3800.0000 1600.0000
&lt;/pre&gt;
&lt;p&gt;There are 8 logical CPUs (&lt;tt class="docutils literal"&gt;CPU &lt;span class="pre"&gt;0..7&lt;/span&gt;&lt;/tt&gt;), all on the same node (&lt;tt class="docutils literal"&gt;NODE 0&lt;/tt&gt;) and
the same socket (&lt;tt class="docutils literal"&gt;SOCKET 0&lt;/tt&gt;).  There are only 4 physical cores (&lt;tt class="docutils literal"&gt;CORE
&lt;span class="pre"&gt;0..3&lt;/span&gt;&lt;/tt&gt;). For example, the physical core &lt;tt class="docutils literal"&gt;2&lt;/tt&gt; is made of the two logical CPUs:
&lt;tt class="docutils literal"&gt;2&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;6&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Using the &lt;tt class="docutils literal"&gt;L1d:L1i:L2:L3&lt;/tt&gt; column, we can see that each pair of two logical
cores share the same physical core caches for levels 1 (L1 data, L1
instruction) and 2 (L2).  All physical cores share the same cache level 3 (L3).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="p-states"&gt;
&lt;h2&gt;P-states&lt;/h2&gt;
&lt;p&gt;A new CPU driver &lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; was added to the Linux kernel 3.9 (April
2009). First, it only supported SandyBridge CPUs (2nd generation), Linux 3.10
extended it to Ivybridge generation CPUs (3rd gen), and so on and so forth.&lt;/p&gt;
&lt;p&gt;This driver supports recent features and thermal control of modern Intel CPUs.
Its name comes from P-states.&lt;/p&gt;
&lt;p&gt;The processor P-state is the capability of running the processor at different
voltage and/or frequency levels. Generally, P0 is the highest state resulting
in maximum performance, while P1, P2, and so on, will save power but at some
penalty to CPU performance.&lt;/p&gt;
&lt;p&gt;It is possible to force the legacy CPU driver (&lt;tt class="docutils literal"&gt;acpi_cpufreq&lt;/tt&gt;) using
&lt;tt class="docutils literal"&gt;intel_pstate=disable&lt;/tt&gt; option in the kernel command line.&lt;/p&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/cpu-freq/intel-pstate.txt"&gt;Documentation of the intel-pstate driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://plus.google.com/+ArjanvandeVen/posts/dLn9T4ehywL"&gt;Some basics on CPU P states on Intel processors&lt;/a&gt; (2013) by Arjan
van de Ven (Intel)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://events.linuxfoundation.org/sites/events/files/slides/LinuxConEurope_2015.pdf"&gt;Balancing Power and Performance in the Linux Kernel&lt;/a&gt;
talk at LinuxCon Europe 2015 by Kristen Accardi (Intel)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://software.intel.com/en-us/blogs/2008/05/29/what-exactly-is-a-p-state-pt-1"&gt;What exactly is a P-state? (Pt. 1)&lt;/a&gt;
(2008) by Taylor K. (Intel)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="idle-states-c-states"&gt;
&lt;h2&gt;Idle states: C-states&lt;/h2&gt;
&lt;p&gt;C-states are idle power saving states, in contrast to P-states, which are
execution power saving states.&lt;/p&gt;
&lt;p&gt;During a P-state, the processor is still executing instructions, whereas during
a C-state (other than C0), the processor is idle, meaning that nothing is
executing.&lt;/p&gt;
&lt;p&gt;C-states:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;C0 is the operational state, meaning that the CPU is doing useful work&lt;/li&gt;
&lt;li&gt;C1 is the first idle state&lt;/li&gt;
&lt;li&gt;C2 is the second idle state: The external I/O Controller Hub blocks
interrupts to the processor.&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a logical processor is idle (C-state except of C0), its frequency is
typically 0 (HALT).&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;cpupower &lt;span class="pre"&gt;idle-info&lt;/span&gt;&lt;/tt&gt; command lists supported C-states:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cpupower idle-info
CPUidle driver: intel_idle
CPUidle governor: menu
analyzing CPU 0:

Number of idle states: 6
Available idle states: POLL C1-IVB C1E-IVB C3-IVB C6-IVB C7-IVB
...
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;cpupower monitor&lt;/tt&gt; shows statistics on C-states:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
smithers$ sudo cpupower monitor -m Idle_Stats
    |Idle_Stats
CPU | POLL | C1-S | C1E- | C3-S | C6-S
   0|  0,00|  0,19|  0,09|  0,58| 96,23
   4|  0,00|  0,00|  0,00|  0,00| 99,90
   1|  0,00|  2,34|  0,00|  0,00| 97,63
   5|  0,00|  0,00|  0,17|  0,00| 98,02
   2|  0,00|  0,00|  0,00|  0,00|  0,00
   6|  0,00|  0,00|  0,00|  0,00|  0,00
   3|  0,00|  0,00|  0,00|  0,00|  0,00
   7|  0,00|  0,00|  0,00|  0,00| 49,97
&lt;/pre&gt;
&lt;p&gt;See also: &lt;a class="reference external" href="https://software.intel.com/en-us/articles/power-management-states-p-states-c-states-and-package-c-states"&gt;Power Management States: P-States, C-States, and Package C-States&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;Turbo Boost&lt;/h2&gt;
&lt;p&gt;In 2005, Intel introduced &lt;a class="reference external" href="https://en.wikipedia.org/wiki/SpeedStep"&gt;SpeedStep&lt;/a&gt;, a serie of dynamic frequency
scaling technologies to reduce the power consumption of laptop CPUs. Turbo
Boost is an enhancement of these technologies, now also used on desktop and
server CPUs.&lt;/p&gt;
&lt;p&gt;Turbo Boost allows to run one or many CPU cores to higher P-states than usual.
The maximum P-state is constrained by the following factors:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The number of active cores (in C0 or C1 state)&lt;/li&gt;
&lt;li&gt;The estimated current consumption of the processor (Imax)&lt;/li&gt;
&lt;li&gt;The estimated power consumption (TDP - Thermal Design Power) of processor&lt;/li&gt;
&lt;li&gt;The temperature of the processor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example on my laptop:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cat /proc/cpuinfo
model name : Intel(R) Core(TM) i7-3520M CPU &amp;#64; 2.90GHz
...

selma$ sudo cpupower frequency-info
analyzing CPU 0:
  driver: intel_pstate
  ...
  boost state support:
    Supported: yes
    Active: yes
    3400 MHz max turbo 4 active cores
    3400 MHz max turbo 3 active cores
    3400 MHz max turbo 2 active cores
    3600 MHz max turbo 1 active cores
&lt;/pre&gt;
&lt;p&gt;The CPU base frequency is 2.9 GHz. If more than one physical cores is &amp;quot;active&amp;quot;
(busy), their frequency can be increased up to 3.4 GHz. If only 1 physical core
is active, its frequency can be increased up to 3.6 GHz.&lt;/p&gt;
&lt;p&gt;In this example, Turbo Boost is supported and active.&lt;/p&gt;
&lt;p&gt;See also the &lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/cpu-freq/boost.txt"&gt;Linux cpu-freq documentation on CPU boost&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="turbo-boost-msr"&gt;
&lt;h3&gt;Turbo Boost MSR&lt;/h3&gt;
&lt;p&gt;The bit 38 of the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Model-specific_register"&gt;Model-specific register
(MSR)&lt;/a&gt; &lt;tt class="docutils literal"&gt;0x1a0&lt;/tt&gt; can
be used to check if the Turbo Boost is enabled:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ sudo rdmsr -f 38:38 0x1a0
0
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;0&lt;/tt&gt; means that Turbo Boost is enabled, whereas &lt;tt class="docutils literal"&gt;1&lt;/tt&gt; means disabled (no
turbo). (The &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-f&lt;/span&gt; 38:38&lt;/tt&gt; option asks to only display the bit 38.)&lt;/p&gt;
&lt;p&gt;If the command doesn't work, you may have to load the &lt;tt class="docutils literal"&gt;msr&lt;/tt&gt; kernel module:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo modprobe msr
&lt;/pre&gt;
&lt;p&gt;Note: I'm not sure that all Intel CPU uses the same MSR.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="intel-state-no-turbo"&gt;
&lt;h3&gt;intel_state/no_turbo&lt;/h3&gt;
&lt;p&gt;Turbo Boost can also be disabled at runtime in the &lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; driver.&lt;/p&gt;
&lt;p&gt;Check if Turbo Boost is enabled:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cat /sys/devices/system/cpu/intel_pstate/no_turbo
0
&lt;/pre&gt;
&lt;p&gt;where &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; means that Turbo Boost is enabled. Disable Turbo Boost:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ echo 1|sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="cpu-flag-ida"&gt;
&lt;h3&gt;CPU flag &amp;quot;ida&amp;quot;&lt;/h3&gt;
&lt;p&gt;It looks like the Turbo Boost status (supported or not) can also be read by the
CPUID(6): &amp;quot;Thermal/Power Management&amp;quot;. It gives access to the flag &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Dynamic_Acceleration"&gt;Intel
Dynamic Acceleration (IDA)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;ida&lt;/tt&gt; flag can also be seen in CPU flags of &lt;tt class="docutils literal"&gt;/proc/cpuinfo&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="read-the-cpu-frequency"&gt;
&lt;h2&gt;Read the CPU frequency&lt;/h2&gt;
&lt;p&gt;General information using &lt;tt class="docutils literal"&gt;cpupower &lt;span class="pre"&gt;frequency-info&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cpupower -c 0 frequency-info
analyzing CPU 0:
  driver: intel_pstate
  ...
  hardware limits: 1.20 GHz - 3.60 GHz
  ...
&lt;/pre&gt;
&lt;p&gt;The frequency of CPUs is between 1.2 GHz and 3.6 GHz (the base frequency is
2.9 GHz on this CPU).&lt;/p&gt;
&lt;div class="section" id="get-the-frequency-of-cpus-turbostat"&gt;
&lt;h3&gt;Get the frequency of CPUs: turbostat&lt;/h3&gt;
&lt;p&gt;It looks like the most reliable way to get a relialistic estimation of the CPUs
frequency is to use the tool &lt;tt class="docutils literal"&gt;turbostat&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ sudo turbostat
     CPU Avg_MHz   Busy% Bzy_MHz TSC_MHz
       -     224    7.80    2878    2893
       0     448   15.59    2878    2893
       1       0    0.01    2762    2893
     CPU Avg_MHz   Busy% Bzy_MHz TSC_MHz
       -     139    5.65    2469    2893
       0     278   11.29    2469    2893
       1       0    0.01    2686    2893
    ...
&lt;/pre&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;Avg_MHz&lt;/tt&gt;: average frequency, based on APERF&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;Busy%&lt;/tt&gt;: CPU usage in percent&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;Bzy_MHz&lt;/tt&gt;: busy frequency, based on MPERF&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;TSC_MHz&lt;/tt&gt;: fixed frequency, TSC stands for &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Time_Stamp_Counter"&gt;Time Stamp Counter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;APERF (average) and MPERF (maximum) are MSR registers that can provide feedback
on current CPU frequency.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-tools-to-get-the-cpu-frequency"&gt;
&lt;h3&gt;Other tools to get the CPU frequency&lt;/h3&gt;
&lt;p&gt;It looks like the following tools are less reliable to estimate the CPU
frequency.&lt;/p&gt;
&lt;p&gt;cpuinfo:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ grep MHz /proc/cpuinfo
cpu MHz : 1372.289
cpu MHz : 3401.042
&lt;/pre&gt;
&lt;p&gt;In April 2016, Len Brown proposed a patch modifying cpuinfo to use APERF and
MPERF MSR to estimate the CPU frequency: &lt;a class="reference external" href="https://lkml.org/lkml/2016/4/1/7"&gt;x86: Calculate MHz using APERF/MPERF
for cpuinfo and scaling_cur_freq&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;tsc&lt;/tt&gt; clock source logs the CPU frequency in kernel logs:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ dmesg|grep 'MHz processor'
[    0.000000] tsc: Detected 2893.331 MHz processor
&lt;/pre&gt;
&lt;p&gt;cpupower frequency-info:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ for core in $(seq 0 1); do sudo cpupower -c $core frequency-info|grep 'current CPU'; done
  current CPU frequency: 3.48 GHz (asserted by call to hardware)
  current CPU frequency: 3.40 GHz (asserted by call to hardware)
&lt;/pre&gt;
&lt;p&gt;cpupower monitor:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ sudo cpupower monitor -m 'Mperf'
    |Mperf
CPU | C0   | Cx   | Freq
   0|  4.77| 95.23|  1924
   1|  0.01| 99.99|  1751
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Modern Intel CPUs use various technologies to provide best performances without
killing the power consumption. It became harder to monitor and understand CPU
performances, than with older CPUs, since the performance now depends on much
more factors.&lt;/p&gt;
&lt;p&gt;It also becomes common to get an integrated graphics processor (IGP) in the
same package, which makes the exact performance even more complex to predict,
since the IGP produces heat and so has an impact on the CPU P-state.&lt;/p&gt;
&lt;p&gt;I should also explain that P-state are &amp;quot;voted&amp;quot; between CPU cores, but I didn't
understand this part. I'm not sure that understanding the exact algorithm
matters much. I tried to not give too much information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="annex-amt-and-the-me-power-management-coprocessor"&gt;
&lt;h2&gt;Annex: AMT and the ME (power management coprocessor)&lt;/h2&gt;
&lt;p&gt;Computers with Intel vPro technology includes &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Active_Management_Technology"&gt;Intel Active Management
Technology (AMT)&lt;/a&gt;: &amp;quot;hardware
and firmware technology for remote out-of-band management of personal
computers&amp;quot;. AMT has many features which includes power management.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Active_Management_Technology#Hardware"&gt;Management Engine (ME)&lt;/a&gt;
is the hardware part: an isolated and protected coprocessor, embedded as a
non-optional part in all current (as of 2015) Intel chipsets. The coprocessor
is a special 32-bit ARC microprocessor (RISC architecture) that's physically
located inside the PCH chipset (or MCH on older chipsets). The coprocessor can
for example be found on Intel MCH chipsets Q35 and Q45.&lt;/p&gt;
&lt;p&gt;See &lt;a class="reference external" href="https://boingboing.net/2016/06/15/intel-x86-processors-ship-with.html"&gt;Intel x86s hide another CPU that can take over your machine (you can't
audit it)&lt;/a&gt; for
more information on the coprocessor.&lt;/p&gt;
&lt;p&gt;More recently, the Intel Xeon Phi CPU (2016) also includes a coprocessor for
power management. I didn't understand if it is the same coprocessor or not.&lt;/p&gt;
&lt;/div&gt;
</content><category term="benchmark"></category><category term="optimization"></category><category term="benchmark"></category><category term="cpu"></category></entry><entry><title>Visualize the system noise using perf and CPU isolation</title><link href="https://vstinner.github.io/perf-visualize-system-noise-with-cpu-isolation.html" rel="alternate"></link><published>2016-06-16T13:30:00+02:00</published><updated>2016-06-16T13:30:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:vstinner.github.io,2016-06-16:/perf-visualize-system-noise-with-cpu-isolation.html</id><summary type="html">&lt;p&gt;I developed a new &lt;a class="reference external" href="http://perf.readthedocs.io/"&gt;perf module&lt;/a&gt; designed to run
stable benchmarks, give fine control on benchmark parameters and compute
statistics on results. With such tool, it becomes simple to &lt;em&gt;visualize&lt;/em&gt;
sources of noise. The CPU isolation will be used to visualize the system noise.
Running a benchmark on isolated CPUs â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I developed a new &lt;a class="reference external" href="http://perf.readthedocs.io/"&gt;perf module&lt;/a&gt; designed to run
stable benchmarks, give fine control on benchmark parameters and compute
statistics on results. With such tool, it becomes simple to &lt;em&gt;visualize&lt;/em&gt;
sources of noise. The CPU isolation will be used to visualize the system noise.
Running a benchmark on isolated CPUs isolates it from the system noise.&lt;/p&gt;
&lt;div class="section" id="isolate-cpus"&gt;
&lt;h2&gt;Isolate CPUs&lt;/h2&gt;
&lt;p&gt;My computer has 4 physical CPU cores. I isolated half of them using
&lt;tt class="docutils literal"&gt;isolcpus=2,3&lt;/tt&gt; parameter of the Linux kernel. I modified manually the command
line in GRUB to add this parameter.&lt;/p&gt;
&lt;p&gt;Check that CPUs are isolated:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/isolated
2-3
&lt;/pre&gt;
&lt;p&gt;The CPU supports HyperThreading, but I disabled it in the BIOS.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="run-a-benchmark"&gt;
&lt;h2&gt;Run a benchmark&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf&lt;/tt&gt; module automatically detects and uses isolated CPU cores. I will
use the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--affinity=0,1&lt;/span&gt;&lt;/tt&gt; option to force running the benchmark on the CPUs
which are not isolated.&lt;/p&gt;
&lt;p&gt;Microbenchmark with and without CPU isolation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf.timeit --json-file=timeit_isolcpus.json --verbose -s 'x=1; y=2' 'x+y'
Pin process to isolated CPUs: 2-3
.........................
Median +- std dev: 36.6 ns +- 0.1 ns (25 runs x 3 samples x 10^7 loops; 1 warmup)

$ python3 -m perf.timeit --affinity=0,1 --json-file=timeit_no_isolcpus.json --verbose -s 'x=1; y=2' 'x+y'
Pin process to CPUs: 0-1
.........................
Median +- std dev: 36.7 ns +- 1.3 ns (25 runs x 3 samples x 10^7 loops; 1 warmup)
&lt;/pre&gt;
&lt;p&gt;My computer was not 100% idle, I was using it while the benchmarks were
running.&lt;/p&gt;
&lt;p&gt;The median is almost the same (36.6 ns and 36.7 ns). The first major difference
is the standard deviation: it is much larger without CPU isolation: 0.1 ns =&amp;gt;
1.3 ns (13x larger).&lt;/p&gt;
&lt;p&gt;Just in case, check manually CPU affinity in metadata:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf show timeit_isolcpus.json --metadata | grep cpu
- cpu_affinity: 2-3 (isolated)
- cpu_count: 4
- cpu_model_name: Intel(R) Core(TM) i7-2600 CPU &amp;#64; 3.40GHz

$ python3 -m perf show timeit_no_isolcpus.json --metadata | grep cpu_affinity
- cpu_affinity: 0-1
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="statistics"&gt;
&lt;h2&gt;Statistics&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf stats&lt;/tt&gt; command computes statistics on the distribution of samples:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf stats timeit_isolcpus.json
Number of samples: 75

Minimum: 36.5 ns (-0.1%)
Median +- std dev: 36.6 ns +- 0.1 ns (36.5 ns .. 36.7 ns)
Maximum: 36.7 ns (+0.4%)

$ python3 -m perf stats timeit_no_isolcpus.json
Number of samples: 75

Minimum: 36.5 ns (-0.5%)
Median +- std dev: 36.7 ns +- 1.3 ns (35.4 ns .. 38.0 ns)
Maximum: 43.0 ns (+17.0%)
&lt;/pre&gt;
&lt;p&gt;The minimum is the same. The second major difference is the maximum: it is much
larger without CPU isolation: 36.7 ns (+0.4%) =&amp;gt; 43.0 ns (+17.0%).&lt;/p&gt;
&lt;p&gt;The difference between the maximum and the median is 63x larger without CPU
isolation: 0.1 ns (&lt;tt class="docutils literal"&gt;36.7 - 36.6&lt;/tt&gt;) =&amp;gt; 6.3 ns (&lt;tt class="docutils literal"&gt;43.0 - 36.7&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;Depending on the system load, a single sample of the microbenchmark is up to
17% slower (maximum of 43.0 ns with a median of 36.7 ns) without CPU isolation.
The difference is smaller with CPU isolation: only 0.4% slower (for the
maximum, and 0.1% faster for the minimum).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="histogram"&gt;
&lt;h2&gt;Histogram&lt;/h2&gt;
&lt;p&gt;Another way to analyze the distribution of samples is to render an histogram:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf hist --bins=8 timeit_isolcpus.json timeit_no_isolcpus.json
[ timeit_isolcpus ]
36.1 ns: 75 ################################################
36.9 ns:  0 |
37.7 ns:  0 |
38.5 ns:  0 |
39.3 ns:  0 |
40.1 ns:  0 |
40.9 ns:  0 |
41.7 ns:  0 |
42.5 ns:  0 |

[ timeit_no_isolcpus ]
36.1 ns: 52 ################################################
36.9 ns: 13 ############
37.7 ns:  1 #
38.5 ns:  4 ####
39.3 ns:  2 ##
40.1 ns:  0 |
40.9 ns:  1 #
41.7 ns:  0 |
42.5 ns:  2 ##
&lt;/pre&gt;
&lt;p&gt;I choose the number of bars to get a small histogram and to get all samples of
the first benchmark on the same bar. With 8 bars, each bar is a range of 0.8
ns.&lt;/p&gt;
&lt;p&gt;The last major difference is the shape of these histogram. Without CPU
isolation, there is a &amp;quot;long tail&amp;quot; at the right of the median: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Outlier"&gt;outliers&lt;/a&gt; in the range [37.7 ns; 42.5 ns].
The outliers come from the &amp;quot;noise&amp;quot; caused by the multitasking system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf&lt;/tt&gt; module provides multiple tools to analyze the distribution of
benchmark samples. Three tools show a major difference without CPU isolation
compared to results with CPU isolation:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Standard deviation: 13x larger without isolation&lt;/li&gt;
&lt;li&gt;Maximum: difference to median 63x larger without isolation&lt;/li&gt;
&lt;li&gt;Shape of the histogram: long tail at the right of the median&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It explains why CPU isolation helps to make benchmarks more stable.&lt;/p&gt;
&lt;/div&gt;
</content><category term="benchmark"></category><category term="benchmark"></category></entry><entry><title>My journey to stable benchmark, part 3 (average)</title><link href="https://vstinner.github.io/journey-to-stable-benchmark-average.html" rel="alternate"></link><published>2016-05-23T23:00:00+02:00</published><updated>2016-05-23T23:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:vstinner.github.io,2016-05-23:/journey-to-stable-benchmark-average.html</id><summary type="html">&lt;p class="first last"&gt;My journey to stable benchmark, part 3 (average)&lt;/p&gt;
</summary><content type="html">&lt;a class="reference external image-reference" href="https://www.flickr.com/photos/stanzim/11100202065/"&gt;&lt;img alt="Fog" src="https://vstinner.github.io/images/fog.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Stable benchmarks are so close, but ...&lt;/em&gt;&lt;/p&gt;
&lt;div class="section" id="address-space-layout-randomization"&gt;
&lt;h2&gt;Address Space Layout Randomization&lt;/h2&gt;
&lt;p&gt;When I started to work on removing the noise of the system, I was told that
disabling &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Address_space_layout_randomization"&gt;Address Space Layout Randomization (ASLR)&lt;/a&gt; makes
benchmarks more stable.&lt;/p&gt;
&lt;p&gt;I followed this advice without trying to understand it. We will see in this
article that it was a bad idea, but I had to hit other issues to really
understand the root issue with disabling ASLR.&lt;/p&gt;
&lt;p&gt;Example of command to see the effect of ASLR, the first number of the output is
the start address of the heap memory:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python -c 'import os; os.system(&amp;quot;grep heap /proc/%s/maps&amp;quot; % os.getpid())'
55e6a716c000-55e6a7235000 rw-p 00000000 00:00 0                          [heap]
&lt;/pre&gt;
&lt;p&gt;Heap address of 3 runs with ASLR enabled (random):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;55e6a716c000&lt;/li&gt;
&lt;li&gt;561c218eb000&lt;/li&gt;
&lt;li&gt;55e6f628f000&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disable ASLR:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo bash -c 'echo 0 &amp;gt;| /proc/sys/kernel/randomize_va_space'
&lt;/pre&gt;
&lt;p&gt;Heap addresses of 3 runs with ASLR disabled (all the same):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;555555756000&lt;/li&gt;
&lt;li&gt;555555756000&lt;/li&gt;
&lt;li&gt;555555756000&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: To reenable ASLR, it's better to use the value 2, the value 1 only
partially enables the feature:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo bash -c 'echo 2 &amp;gt;| /proc/sys/kernel/randomize_va_space'
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="python-randomized-hash-function"&gt;
&lt;h2&gt;Python randomized hash function&lt;/h2&gt;
&lt;p&gt;With &lt;a class="reference external" href="https://vstinner.github.io/journey-to-stable-benchmark-system.html"&gt;system tuning  (part 1)&lt;/a&gt;, a
&lt;a class="reference external" href="https://vstinner.github.io/journey-to-stable-benchmark-deadcode.html"&gt;Python compiled with PGO (part 2)&lt;/a&gt;
and ASLR disabled, I still I failed to get the same result when running
manually &lt;tt class="docutils literal"&gt;bm_call_simple.py&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;On Python 3, the hash function is now randomized by default: &lt;a class="reference external" href="http://bugs.python.org/issue13703"&gt;issue #13703&lt;/a&gt;. The problem is that for a
microbenchmark, the number of hash collisions of an &amp;quot;hot&amp;quot; dictionary has a
non-negligible impact on performances.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; environment variable can be used to get a fixed hash
function. Example with the patch:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ PYTHONHASHSEED=1 taskset -c 1 ./python bm_call_simple.py -n 1
0.198
$ PYTHONHASHSEED=2 taskset -c 1 ./python bm_call_simple.py -n 1
0.201
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1
0.207
$ PYTHONHASHSEED=4 taskset -c 1 ./python bm_call_simple.py -n 1
0.187
$ PYTHONHASHSEED=5 taskset -c 1 ./python bm_call_simple.py -n 1
0.180
&lt;/pre&gt;
&lt;p&gt;Timings of the reference python:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ PYTHONHASHSEED=1 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.204
$ PYTHONHASHSEED=2 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.206
$ PYTHONHASHSEED=3 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.195
$ PYTHONHASHSEED=4 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.192
$ PYTHONHASHSEED=5 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.187
&lt;/pre&gt;
&lt;p&gt;The minimums is 180 ms for the reference and 186 ms for the patch. The patched
Python is 3% faster, yeah!&lt;/p&gt;
&lt;p&gt;Wait. What if we only test PYTHONHASHSEED from 1 to 3? In this case, the
minimum is 195 ms for the reference and 198 ms for the patch. The patched
Python becomes 2% slower, oh no!&lt;/p&gt;
&lt;p&gt;Faster? Slower? Who is right?&lt;/p&gt;
&lt;p&gt;Maybe I should write a script to find a &lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; value for which my
patch is always faster :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="command-line-and-environment-variables"&gt;
&lt;h2&gt;Command line and environment variables&lt;/h2&gt;
&lt;p&gt;Well, let's say that we will use a fixed PYTHONHASHSEED value. Anyway, my
patch doesn't touch the hash function. So it doesn't matter.&lt;/p&gt;
&lt;p&gt;While running benchmarks, I noticed differences when running the benchmark from
a different directory:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cd /home/haypo/prog/python/fastcall
$ PYTHONHASHSEED=3 taskset -c 1 pgo/python ../benchmarks/performance/bm_call_simple.py -n 1
0.215

$ cd /home/haypo/prog/python/benchmarks
$ PYTHONHASHSEED=3 taskset -c 1 ../fastcall/pgo/python ../benchmarks/performance/bm_call_simple.py -n 1
0.203

$ cd /home/haypo/prog/python
$ PYTHONHASHSEED=3 taskset -c 1 fastcall/pgo/python benchmarks/performance/bm_call_simple.py -n 1
0.200
&lt;/pre&gt;
&lt;p&gt;In fact, a different command line is enough so get different results (added
arguments are ignored):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1
0.201
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1
0.198
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1 arg2 arg3
0.203
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1 arg2 arg3 arg4 arg5
0.206
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1 arg2 arg3 arg4 arg5 arg6
0.210
&lt;/pre&gt;
&lt;p&gt;I also noticed minor differences when the environment changes (added variables
are ignored):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 1
0.201
$ taskset -c 1 env -i PYTHONHASHSEED=3 VAR1=1 VAR2=2 VAR3=3 VAR4=4 ./python bm_call_simple.py -n 1
0.202
$ taskset -c 1 env -i PYTHONHASHSEED=3 VAR1=1 VAR2=2 VAR3=3 VAR4=4 VAR5=5 ./python bm_call_simple.py -n 1
0.198
&lt;/pre&gt;
&lt;p&gt;Using &lt;tt class="docutils literal"&gt;strace&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;ltrace&lt;/tt&gt;, I saw the memory addresses are different when
something (command line, env var, etc.) changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="average-and-standard-deviation"&gt;
&lt;h2&gt;Average and standard deviation&lt;/h2&gt;
&lt;p&gt;Basically, it looks like a lot of &amp;quot;external factors&amp;quot; have an impact on the
exact memory addresses, even if ASRL is disabled and PYTHONHASHSEED is set. I
started to think how to get &lt;em&gt;exactly&lt;/em&gt; the same command line, the same
environment (easy), the same current directory (easy), etc. The problem is that
it's just not possible to control all external factors (having an effect on the
exact memory addresses).&lt;/p&gt;
&lt;p&gt;Maybe I was plain wrong from the beginning and ASLR must be enabled,
as the default on Linux:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.198
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.202
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.199
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.207
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.200
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.201
&lt;/pre&gt;
&lt;p&gt;These results look &amp;quot;random&amp;quot;. Yes, they are. It's exactly the purpose of ASLR.&lt;/p&gt;
&lt;p&gt;But how can we compare performances if results are random? Take the minimum?&lt;/p&gt;
&lt;p&gt;No! You must never (ever again) use the minimum for benchmarking! Compute the
average and some statistics like the standard deviation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3
Python 3.4.3
&amp;gt;&amp;gt;&amp;gt; timings=[0.198, 0.202, 0.199, 0.207, 0.200, 0.201]
&amp;gt;&amp;gt;&amp;gt; import statistics
&amp;gt;&amp;gt;&amp;gt; statistics.mean(timings)
0.2011666666666667
&amp;gt;&amp;gt;&amp;gt; statistics.stdev(timings)
0.0031885210782848245
&lt;/pre&gt;
&lt;p&gt;On this example, the average is 201 ms +/- 3 ms. IMHO the standard deviation is
quite small (reliable) which means that my benchmark is stable. To get a good
distribution, it's better to have many samples. It looks like at least 25
processes are needed. Each process tests a different memory layout and a
different hash function.&lt;/p&gt;
&lt;p&gt;Result of 5 runs, each run uses 25 processes (ASLR enabled, random hash
function):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Average: 205.2 ms +/- 3.0 ms (min: 201.1 ms, max: 214.9 ms)&lt;/li&gt;
&lt;li&gt;Average: 205.6 ms +/- 3.3 ms (min: 201.4 ms, max: 216.5 ms)&lt;/li&gt;
&lt;li&gt;Average: 206.0 ms +/- 3.9 ms (min: 201.1 ms, max: 215.3 ms)&lt;/li&gt;
&lt;li&gt;Average: 205.7 ms +/- 3.6 ms (min: 201.5 ms, max: 217.8 ms)&lt;/li&gt;
&lt;li&gt;Average: 206.4 ms +/- 3.5 ms (min: 201.9 ms, max: 214.9 ms)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While memory layout and hash functions are random again, the result looks
&lt;em&gt;less&lt;/em&gt; random, and so more reliable, than before!&lt;/p&gt;
&lt;p&gt;With ASLR enabled, the effect of the environment variables, command line and
current directory is negligible on the (average) result.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-average-solves-issues-with-uniform-random-noises"&gt;
&lt;h2&gt;The average solves issues with uniform random noises&lt;/h2&gt;
&lt;p&gt;The user will run the application with default system settings which means
ASLR enabled and Python hash function randomized. Running a benchmark in one
specific environment is a mistake because it is not representative of the
performance in practice.&lt;/p&gt;
&lt;p&gt;Computing the average and standard deviation &amp;quot;fixes&amp;quot; the issue with hash
randomization. It's much better to use random hash functions and compute the
average, than using a fixed hash function (setting &lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; variable
to a value).&lt;/p&gt;
&lt;p&gt;Oh wow, already 3 big articles explaing how to get stable benchmarks. Please
tell me that it was the last one!  Nope, more is coming...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="annex-why-only-n1"&gt;
&lt;h2&gt;Annex: why only -n1?&lt;/h2&gt;
&lt;p&gt;In this article, I ran &lt;tt class="docutils literal"&gt;bm_call_simple.py&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-n&lt;/span&gt; 1&lt;/tt&gt; with only run one
iteration.&lt;/p&gt;
&lt;p&gt;Usually, a single iteration is not reliable at all, at least 50 iterations are
needed. But thanks to system tuning, compilation with PGO, ASRL disabled and
&lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; set, a single iteration is enough.&lt;/p&gt;
&lt;p&gt;Example of 3 runs, each with 3 iterations:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 3
0.201
0.201
0.201
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 3
0.201
0.201
0.201
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 3
0.201
0.201
0.201
&lt;/pre&gt;
&lt;p&gt;Always the same timing!&lt;/p&gt;
&lt;/div&gt;
</content><category term="benchmark"></category><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>My journey to stable benchmark, part 2 (deadcode)</title><link href="https://vstinner.github.io/journey-to-stable-benchmark-deadcode.html" rel="alternate"></link><published>2016-05-22T22:00:00+02:00</published><updated>2016-05-22T22:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:vstinner.github.io,2016-05-22:/journey-to-stable-benchmark-deadcode.html</id><summary type="html">&lt;p class="first last"&gt;My journey to stable benchmark, part 2 (deadcode)&lt;/p&gt;
</summary><content type="html">&lt;a class="reference external image-reference" href="https://www.flickr.com/photos/uw67/16875152403/"&gt;&lt;img alt="Snail" src="https://vstinner.github.io/images/snail.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;With &lt;a class="reference external" href="https://vstinner.github.io/journey-to-stable-benchmark-system.html"&gt;the system tuning (part 1)&lt;/a&gt;, I
expected to get very stable benchmarks and so I started to benchmark seriously
my &lt;a class="reference external" href="https://bugs.python.org/issue26814"&gt;FASTCALL branch&lt;/a&gt; of CPython (a new
calling convention avoiding temporary tuples).&lt;/p&gt;
&lt;p&gt;I was disappointed to get many slowdowns in the CPython benchmark suite. I
started to analyze why my change introduced performance regressions.&lt;/p&gt;
&lt;p&gt;I took my overall patch and slowly reverted more and more code to check which
changes introduced most of the slowdowns.&lt;/p&gt;
&lt;p&gt;I focused on the &lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; benchmark which does only one thing: call
Python functions which do nothing.  Making Python function calls slower would
be a big and inacceptable mistake of my work.&lt;/p&gt;
&lt;div class="section" id="linux-perf"&gt;
&lt;h2&gt;Linux perf&lt;/h2&gt;
&lt;p&gt;I started to learn how to use the great &lt;a class="reference external" href="https://perf.wiki.kernel.org/index.php/Main_Page"&gt;Linux perf&lt;/a&gt; tool to analyze why
&lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; was slower. I tried to find a major difference between my
reference python and the patched python.&lt;/p&gt;
&lt;p&gt;I analyzed cache misses on L1 instruction and data caches.  I analyzed stallen
CPU cycles. I analyzed all memory events, branch events, etc. Basically, I tried
all perf events and spent a lot of time to run benchmarks multiple times.&lt;/p&gt;
&lt;p&gt;By the way, I strongly suggest to use &lt;tt class="docutils literal"&gt;perf stat&lt;/tt&gt; using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--repeat&lt;/span&gt;&lt;/tt&gt;
command line option to get an average on multiple runs and see the standard
deviation. It helps to get more reliable numbers. I even wrote a Python script
implementing &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--repeat&lt;/span&gt;&lt;/tt&gt; (run perf multiple times, parse the output), before
seeing that it was already a builtin feature!&lt;/p&gt;
&lt;p&gt;Use &lt;tt class="docutils literal"&gt;perf list&lt;/tt&gt; to list all available (pre-defined) events.&lt;/p&gt;
&lt;p&gt;After many days, I decided to give up with perf.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cachegrind"&gt;
&lt;h2&gt;Cachegrind&lt;/h2&gt;
&lt;a class="reference external image-reference" href="http://valgrind.org/"&gt;&lt;img alt="Logo of the Valgrind project" src="https://vstinner.github.io/images/valgrind.png" /&gt;&lt;/a&gt;
&lt;p&gt;&lt;a class="reference external" href="http://valgrind.org/"&gt;Valgrind&lt;/a&gt; is a great tool known to detect memory
leaks, but it also contains gems like the &lt;a class="reference external" href="http://valgrind.org/docs/manual/cg-manual.html"&gt;Cachegrind tool&lt;/a&gt; which &lt;em&gt;simulates&lt;/em&gt; the
CPU caches.&lt;/p&gt;
&lt;p&gt;I used Cachegrind with the nice &lt;a class="reference external" href="http://kcachegrind.sourceforge.net/"&gt;Kcachegrind GUI&lt;/a&gt;. Sadly, I also failed to see anything
obvious in cache misses between the reference python and the patched python.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="strace-and-ltrace"&gt;
&lt;h2&gt;strace and ltrace&lt;/h2&gt;
&lt;img alt="strace and ltrace" src="https://vstinner.github.io/images/strace_ltrace.png" /&gt;
&lt;p&gt;I also tried &lt;tt class="docutils literal"&gt;strace&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;ltrace&lt;/tt&gt; tools to try to see a difference in the
execution of the reference and the patched pythons. I saw different memory
addresses, but no major difference which can explain a difference of the
timing.&lt;/p&gt;
&lt;p&gt;Morever, the hotcode simply does not call any syscall nor library
function. It's pure CPU-bound code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="compiler-options"&gt;
&lt;h2&gt;Compiler options&lt;/h2&gt;
&lt;a class="reference external image-reference" href="https://gcc.gnu.org/"&gt;&lt;img alt="GCC logo" class="align-right" src="https://vstinner.github.io/images/gcc.png" /&gt;&lt;/a&gt;
&lt;p&gt;I used &lt;a class="reference external" href="https://gcc.gnu.org/"&gt;GCC&lt;/a&gt; to build to code. Just in case, I tried
LLVM compiler, but it didn't &amp;quot;fix&amp;quot; the issue.&lt;/p&gt;
&lt;p&gt;I also tried different optimization levels: &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O0&lt;/span&gt;&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O1&lt;/span&gt;&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O2&lt;/span&gt;&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O3&lt;/span&gt;&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I read that the exact address of functions can have an impact on the CPU L1
cache: &lt;a class="reference external" href="https://stackoverflow.com/questions/19470873/why-does-gcc-generate-15-20-faster-code-if-i-optimize-for-size-instead-of-speed"&gt;Why does gcc generate 15-20% faster code if I optimize for size instead
of speed?&lt;/a&gt;.
I tried various values of the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-falign-functions=N&lt;/span&gt;&lt;/tt&gt; option (1, 2, 6, 12).&lt;/p&gt;
&lt;p&gt;I also tried &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fomit-pointer&lt;/span&gt;&lt;/tt&gt; (omit frame pointer) to record the callgraph with &lt;tt class="docutils literal"&gt;perf record&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I also tried &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-flto&lt;/span&gt;&lt;/tt&gt;: Link Time Optimization (LTO).&lt;/p&gt;
&lt;p&gt;These compiler options didn't fix the issue.&lt;/p&gt;
&lt;p&gt;The truth is out there.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; See also &lt;a class="reference external" href="https://lwn.net/Articles/534735/"&gt;Rethinking optimization for size&lt;/a&gt; article on Linux Weekly News (LWN):
&lt;em&gt;&amp;quot;Such an option has obvious value if one is compiling for a
space-constrained environment like a small device. But it turns out that, in
some situations, optimizing for space can also produce faster code.&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="when-cpython-performance-depends-on-dead-code"&gt;
&lt;h2&gt;When CPython performance depends on dead code&lt;/h2&gt;
&lt;p&gt;I continued to revert changes. At the end, my giant patch was reduced to very
few changes only adding code which was never called (at least, I was sure
that it was not called in the &lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; benchmark).&lt;/p&gt;
&lt;p&gt;Let me rephase: &lt;em&gt;adding dead code&lt;/em&gt; makes Python slower. What?&lt;/p&gt;
&lt;p&gt;A colleague suggested me to remove the body (replace it with &lt;tt class="docutils literal"&gt;return;&lt;/tt&gt;) of
added function: the code became faster. Ok, now I'm completely lost. To be
clear, I don't expect that adding dead code would have &lt;em&gt;any&lt;/em&gt; impact on the
performance.&lt;/p&gt;
&lt;p&gt;My email &lt;a class="reference external" href="https://mail.python.org/pipermail/speed/2016-April/000341.html"&gt;When CPython performance depends on dead code...&lt;/a&gt; explains how
to reproduce the issue and contains many information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="solution-pgo"&gt;
&lt;h2&gt;Solution: PGO&lt;/h2&gt;
&lt;p&gt;The solution is called Profiled Guided Optimization, &amp;quot;PGO&amp;quot;. Python build system
supports it in a single command: &lt;tt class="docutils literal"&gt;make &lt;span class="pre"&gt;profile-opt&lt;/span&gt;&lt;/tt&gt;. It profiles the
execution of the Python test suite.&lt;/p&gt;
&lt;p&gt;Using PGO, adding dead code has no more impact on the performance.&lt;/p&gt;
&lt;p&gt;With system tuning and PGO compilation, benchmarks must now be stable this
time, no? ... No, sorry, not yet. We will see more sources of noise in
following articles ;-)&lt;/p&gt;
&lt;/div&gt;
</content><category term="benchmark"></category><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>My journey to stable benchmark, part 1 (system)</title><link href="https://vstinner.github.io/journey-to-stable-benchmark-system.html" rel="alternate"></link><published>2016-05-21T16:50:00+02:00</published><updated>2016-05-21T16:50:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:vstinner.github.io,2016-05-21:/journey-to-stable-benchmark-system.html</id><summary type="html">&lt;p class="first last"&gt;My journey to stable benchmark, part 1&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="background"&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;In the CPython development, it became common to require the result of the
&lt;a class="reference external" href="https://hg.python.org/benchmarks"&gt;CPython benchmark suite&lt;/a&gt; (&amp;quot;The Grand
Unified Python Benchmark Suite&amp;quot;) to evaluate the effect of an optimization
patch. The minimum requirement is to not introduce performance regressions.&lt;/p&gt;
&lt;p&gt;I used the CPython benchmark suite and I had many bad surprises when trying to
analyze (understand) results. A change expected to be faster makes some
benchmarks slower without any obvious reason. At least, the change is expected
to be faster on some specific benchmarks, but have no impact on the other
benchmarks. The slowdown is usually between 5% and 10% slower. I am not
confortable with any kind of slowdown.&lt;/p&gt;
&lt;p&gt;Many benchmarks look unstable. The problem is to trust the overall report.
Some developers started to say that they learnt to ignore some benchmarks known
to be unstable.&lt;/p&gt;
&lt;p&gt;It's not the first time that I am totally disappointed by microbenchmark
results, so I decided to analyze completely the issue and go as deep as
possible to really understand the problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-to-get-stable-benchmarks-on-a-busy-linux-system"&gt;
&lt;h2&gt;How to get stable benchmarks on a busy Linux system&lt;/h2&gt;
&lt;p&gt;A common advice to get stable benchmark is to stay away the keyboard
(&amp;quot;freeze!&amp;quot;) and stop all other applications to only run one application, the
benchmark.&lt;/p&gt;
&lt;p&gt;Well, I'm working on a single computer and the full CPython benchmark suite
take up to 2 hours in rigorous mode. I just cannot stop working during 2 hours
to wait for the result of the benchmark. I like running benchmarks locally. It
is convenient to run benchmarks on the same computer used to develop.&lt;/p&gt;
&lt;p&gt;The goal here is to &amp;quot;remove the noise of the system&amp;quot;. Get the same result on a
busy system than an idle system. My simple &lt;a class="reference external" href="https://github.com/vstinner/misc/blob/master/bin/system_load.py"&gt;system_load.py&lt;/a&gt; program can be
used to increase the system load. For example, run &lt;tt class="docutils literal"&gt;system_load.py 10&lt;/tt&gt; in a
terminal to get at least a system load of 10 (busy system) and run the
benchmark in a different terminal. Use CTRL+c to stop &lt;tt class="docutils literal"&gt;system_load.py&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cpu-isolation"&gt;
&lt;h2&gt;CPU isolation&lt;/h2&gt;
&lt;p&gt;In 2016, it is common to get a CPU with multiple physical cores. For example,
my Intel CPU has 4 physical cores and 8 logical cores thanks to
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Hyper-threading"&gt;Hyper-Threading&lt;/a&gt;. It is
possible to configure the Linux kernel to not schedule processes on some CPUs
using the &amp;quot;CPU isolation&amp;quot; feature. It is the &lt;tt class="docutils literal"&gt;isolcpus&lt;/tt&gt; parameter of the
Linux command line, the value is a list of CPUs. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
isolcpus=2,3,6,7
&lt;/pre&gt;
&lt;p&gt;Check with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/isolated
2-3,6-7
&lt;/pre&gt;
&lt;p&gt;If you have Hyper-Threading, you must isolate the two logicial cores of each
isolated physical core. You can use the &lt;tt class="docutils literal"&gt;lscpu &lt;span class="pre"&gt;--all&lt;/span&gt; &lt;span class="pre"&gt;--extended&lt;/span&gt;&lt;/tt&gt; command to
identify physical cores. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ lscpu -a -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE MAXMHZ    MINMHZ
0   0    0      0    0:0:0:0       yes    5900,0000 1600,0000
1   0    0      1    1:1:1:0       yes    5900,0000 1600,0000
2   0    0      2    2:2:2:0       yes    5900,0000 1600,0000
3   0    0      3    3:3:3:0       yes    5900,0000 1600,0000
4   0    0      0    0:0:0:0       yes    5900,0000 1600,0000
5   0    0      1    1:1:1:0       yes    5900,0000 1600,0000
6   0    0      2    2:2:2:0       yes    5900,0000 1600,0000
7   0    0      3    3:3:3:0       yes    5900,0000 1600,0000
&lt;/pre&gt;
&lt;p&gt;The physical core &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; (CORE column) is made of two logical cores (CPU
column): &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;4&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="nohz-mode"&gt;
&lt;h2&gt;NOHZ mode&lt;/h2&gt;
&lt;p&gt;By default, the Linux kernel uses a scheduling-clock which interrupts the
running application &lt;tt class="docutils literal"&gt;HZ&lt;/tt&gt; times per second to run the scheduler. &lt;tt class="docutils literal"&gt;HZ&lt;/tt&gt; is
usually between 100 and 1000: time slice between 1 ms and 10 ms.&lt;/p&gt;
&lt;p&gt;Linux supports a &lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt"&gt;NOHZ mode&lt;/a&gt; which is able to
disable the scheduling-clock when the system is idle to reduce the power
consumption. Linux 3.10 introduces a &lt;a class="reference external" href="https://lwn.net/Articles/549580/"&gt;full ticketless mode&lt;/a&gt;, NOHZ full, which is able to disable the
scheduling-clock when only one application is running on a CPU.&lt;/p&gt;
&lt;p&gt;NOHZ full is disabled by default. It can be enabled with the &lt;tt class="docutils literal"&gt;nohz_full&lt;/tt&gt;
parameter of the Linux command line, the value is a list of CPUs. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
nohz_full=2,3,6,7
&lt;/pre&gt;
&lt;p&gt;Check with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/nohz_full
2-3,6-7
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="interrupts-irq"&gt;
&lt;h2&gt;Interrupts (IRQ)&lt;/h2&gt;
&lt;p&gt;The Linux kernel can also be configured to not run &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Interrupt_request_%28PC_architecture%29"&gt;interruptions (IRQ)&lt;/a&gt;
handlers on some CPUs using &lt;tt class="docutils literal"&gt;/proc/irq/default_smp_affinity&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;/proc/irq/&amp;lt;number&amp;gt;/smp_affinity&lt;/span&gt;&lt;/tt&gt; files. The value is not a list of CPUs but
a bitmask.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;/proc/interrupts&lt;/tt&gt; file can be read to see the number of interruptions
per CPU.&lt;/p&gt;
&lt;p&gt;Read the &lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/IRQ-affinity.txt"&gt;Linux SMP IRQ affinity&lt;/a&gt; documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-of-effect-of-cpu-isolation-on-a-microbenchmark"&gt;
&lt;h2&gt;Example of effect of CPU isolation on a microbenchmark&lt;/h2&gt;
&lt;p&gt;Example with Linux parameters:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
isolcpus=2,3,6,7 nohz_full=2,3,6,7
&lt;/pre&gt;
&lt;p&gt;Microbenchmark on an idle system (without CPU isolation):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 229 msec per loop
&lt;/pre&gt;
&lt;p&gt;Result on a busy system using &lt;tt class="docutils literal"&gt;system_load.py 10&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;find /&lt;/tt&gt; commands
running in other terminals:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 372 msec per loop
&lt;/pre&gt;
&lt;p&gt;The microbenchmark is 56% slower because of the high system load!&lt;/p&gt;
&lt;p&gt;Result on the same busy system but using isolated CPUs. The &lt;tt class="docutils literal"&gt;taskset&lt;/tt&gt; command
allows to pin an application to specific CPUs:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1,3 python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 230 msec per loop
&lt;/pre&gt;
&lt;p&gt;Just to check, new run without CPU isolation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 357 msec per loop
&lt;/pre&gt;
&lt;p&gt;The result with CPU isolation on a busy system is the same than the result an
idle system! CPU isolation removes most of the noise of the system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Great job Linux!&lt;/p&gt;
&lt;p&gt;Ok! Now, the benchmark is super stable, no? ...  Sorry, no, it's not stable yet.
I found a lot of other sources of &amp;quot;noise&amp;quot;.  We will see them in the following
articles ;-)&lt;/p&gt;
&lt;/div&gt;
</content><category term="benchmark"></category><category term="optimization"></category><category term="benchmark"></category></entry></feed>