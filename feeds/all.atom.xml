<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Haypo blog 2</title><link href="https://haypo.github.io/" rel="alternate"></link><link href="https://haypo.github.io/feeds/all.atom.xml" rel="self"></link><id>https://haypo.github.io/</id><updated>2017-10-19T16:00:00+02:00</updated><entry><title>My contributions to CPython during 2017 Q3: Part 3 (funny bugs)</title><link href="https://haypo.github.io/contrib-cpython-2017q3-part3.html" rel="alternate"></link><published>2017-10-19T16:00:00+02:00</published><updated>2017-10-19T16:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-10-19:/contrib-cpython-2017q3-part3.html</id><summary type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q3
(july, august, september), Part 3 (funny bugs).&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part2.html"&gt;My contributions to CPython during 2017 Q3: Part 2 (dangling
threads)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;FreeBSD bug: minor() device regression&lt;/li&gt;
&lt;li&gt;regrtest snowball effect when hunting memory leaks&lt;/li&gt;
&lt;li&gt;Bugfixes&lt;/li&gt;
&lt;li&gt;Other Changes&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="freebsd-bug-minor-device-regression"&gt;
&lt;h2&gt;FreeBSD bug: minor() device regression&lt;/h2&gt;
&lt;a class="reference external image-reference" href="https://www.freebsd.org/"&gt;&lt;img alt="Logo of the FreeBSD project" src="https://haypo.github.io/images/freebsd.png" /&gt;&lt;/a&gt;
&lt;p&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31044"&gt;bpo-31044&lt;/a&gt;: The …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q3
(july, august, september), Part 3 (funny bugs).&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part2.html"&gt;My contributions to CPython during 2017 Q3: Part 2 (dangling
threads)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;FreeBSD bug: minor() device regression&lt;/li&gt;
&lt;li&gt;regrtest snowball effect when hunting memory leaks&lt;/li&gt;
&lt;li&gt;Bugfixes&lt;/li&gt;
&lt;li&gt;Other Changes&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="freebsd-bug-minor-device-regression"&gt;
&lt;h2&gt;FreeBSD bug: minor() device regression&lt;/h2&gt;
&lt;a class="reference external image-reference" href="https://www.freebsd.org/"&gt;&lt;img alt="Logo of the FreeBSD project" src="https://haypo.github.io/images/freebsd.png" /&gt;&lt;/a&gt;
&lt;p&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31044"&gt;bpo-31044&lt;/a&gt;: The test_makedev() of
test_posix started to fail in the build 632 (Wed Jul 26 10:47:01 2017) of AMD64
FreeBSD CURRENT. The test failed on Debug, but also Non-Debug buildbots, in
master and 3.6 branches. It looks more like a change on the buildbot, maybe a
FreeBSD upgrade?&lt;/p&gt;
&lt;p&gt;Thanks to &lt;strong&gt;koobs&lt;/strong&gt;, I have a SSH access to the buildbot. I was able to
reproduce the bug manually. I noticed that minor() truncates most significant
bits.&lt;/p&gt;
&lt;p&gt;I continued my analysis and I found that, at May 23, the FreeBSD &lt;tt class="docutils literal"&gt;dev_t&lt;/tt&gt; type
changed from 32 bits to 64 bits in the kernel, but the &lt;tt class="docutils literal"&gt;minor()&lt;/tt&gt; userland
function was not updated.&lt;/p&gt;
&lt;p&gt;I reported a bug to FreeBSD: &lt;a class="reference external" href="https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=221048"&gt;Bug 221048 - minor() truncates device number to
32 bits, whereas dev_t type was extended to 64 bits&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the meanwhile, I skipped test_posix.test_makedev() on FreeBSD if &lt;tt class="docutils literal"&gt;dev_t&lt;/tt&gt;
is larger than 32-bit.&lt;/p&gt;
&lt;p&gt;Hopefully, the FreeBSD bug was quickly fixed!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="regrtest-snowball-effect-when-hunting-memory-leaks"&gt;
&lt;h2&gt;regrtest snowball effect when hunting memory leaks&lt;/h2&gt;
&lt;p&gt;While trying to fix all reference leaks on the new Windows and Linux &amp;quot;Refleaks&amp;quot;
buildbots, I reported the bug &lt;a class="reference external" href="https://bugs.python.org/issue31217"&gt;bpo-31217&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
test_code leaked [1, 1, 1] memory blocks, sum=3
&lt;/pre&gt;
&lt;p&gt;Two weeks after reporting the bug, I was able to reproduce the bug, but &lt;strong&gt;only
with Python compiled in 32-bit mode&lt;/strong&gt;. Strange.&lt;/p&gt;
&lt;p&gt;I spent one day to understand the bug. I removed as much as possible while
making sure that I can still reproduce the bug. At the end, I wrote &lt;a class="reference external" href="https://bugs.python.org/file47114/leak2.py"&gt;leak2.py&lt;/a&gt; which reproduces the bug with a
single import: &lt;tt class="docutils literal"&gt;import sys&lt;/tt&gt;. Even if the script is only 86 lines long, I was
still unable to understand the bug.&lt;/p&gt;
&lt;p&gt;My first hypothesis:&lt;/p&gt;
&lt;blockquote&gt;
It seems like the &amp;quot;leak&amp;quot; is the call to &lt;tt class="docutils literal"&gt;sys.getallocatedblocks()&lt;/tt&gt; which
creates a new integer, and the integer is kept alive between two loop
iterations.&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Antoine Pitrou&lt;/strong&gt; rejected it:&lt;/p&gt;
&lt;blockquote&gt;
I doubt it. If that was the case, the reference count would increase as
well.&lt;/blockquote&gt;
&lt;p&gt;It was Antoine Pitrou who understood the bug:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Ahah.
Actually, it's quite simple :-) On 64-bit Python:

&amp;gt;&amp;gt;&amp;gt; id(82914 - 82913) == id(1)
True

On 32-bit Python:

&amp;gt;&amp;gt;&amp;gt; id(82914 - 82913) == id(1)
False

So the first non-zero alloc_delta really has a snowball effect, as it
creates new memory block which will produce a non-zero alloc_delta on the
next run, etc.
&lt;/pre&gt;
&lt;p&gt;I implemented Antoine's idea to fix the bug, &lt;a class="reference external" href="https://github.com/python/cpython/commit/6c2feabc5dac2f3049b15134669e9ad5af573193"&gt;commit&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Use a pool of integer objects to prevent false alarm when checking for
memory block leaks. Fill the pool with values in -1000..1000 which
are the most common (reference, memory block, file descriptor)
differences.

Co-Authored-By: Antoine Pitrou &amp;lt;pitrou&amp;#64;free.fr&amp;gt;
&lt;/pre&gt;
&lt;p&gt;The bug is probably as old as the code hunting memory leaks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bugfixes"&gt;
&lt;h2&gt;Bugfixes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30891"&gt;bpo-30891&lt;/a&gt;: Second fix for
importlib &lt;tt class="docutils literal"&gt;_find_and_load()&lt;/tt&gt; to handle correctly parallelism with threads.
Call &lt;tt class="docutils literal"&gt;sys.modules.get()&lt;/tt&gt; in the &lt;tt class="docutils literal"&gt;with _ModuleLockManager(name):&lt;/tt&gt; block to
protect the dictionary key with the module lock and use an atomic get to
prevent race conditions.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31019"&gt;bpo-31019&lt;/a&gt;:
&lt;tt class="docutils literal"&gt;multiprocessing.Process.is_alive()&lt;/tt&gt; now removes the process from the
&lt;tt class="docutils literal"&gt;_children set&lt;/tt&gt; if the process completed. The change prevents leaking
&amp;quot;dangling&amp;quot; processes.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31326"&gt;bpo-31326&lt;/a&gt;, &lt;tt class="docutils literal"&gt;concurrent.futures&lt;/tt&gt;:
&lt;tt class="docutils literal"&gt;ProcessPoolExecutor.shutdown()&lt;/tt&gt; now explicitly closes the call queue.
Moreover, &lt;tt class="docutils literal"&gt;shutdown(wait=True)&lt;/tt&gt; now also joins the call queue thread, to
prevent leaking a dangling thread.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31170"&gt;bpo-31170&lt;/a&gt;: Update libexpat from
2.2.3 to 2.2.4: fix copying of partial characters for UTF-8 input (&lt;a class="reference external" href="https://github.com/libexpat/libexpat/issues/115"&gt;libexpat
bug 115&lt;/a&gt;). Later, I also
wrote non-regression tests for this bug (libexpat doesn't have any test
for this bug).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31499"&gt;bpo-31499&lt;/a&gt;, &lt;tt class="docutils literal"&gt;xml.etree&lt;/tt&gt;:
&lt;tt class="docutils literal"&gt;xmlparser_gc_clear()&lt;/tt&gt; now sets self.parser to &lt;tt class="docutils literal"&gt;NULL&lt;/tt&gt; to prevent a crash
in &lt;tt class="docutils literal"&gt;xmlparser_dealloc()&lt;/tt&gt; if &lt;tt class="docutils literal"&gt;xmlparser_gc_clear()&lt;/tt&gt; was called previously
by the garbage collector, because the parser was part of a reference cycle.
Fix co-written with &lt;strong&gt;Serhiy Storchaka&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30892"&gt;bpo-30892&lt;/a&gt;: Fix &lt;tt class="docutils literal"&gt;_elementtree&lt;/tt&gt;
module initialization (accelerator of &lt;tt class="docutils literal"&gt;xml.etree&lt;/tt&gt;), handle correctly
&lt;tt class="docutils literal"&gt;getattr(copy, 'deepcopy')&lt;/tt&gt; failure to not fail with an assertion error.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other Changes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30866"&gt;bpo-30866&lt;/a&gt;: Add _testcapi.stack_pointer(). I used it to write the &amp;quot;Stack
consumption&amp;quot; section of a previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q1.html"&gt;My contributions to CPython
during 2017 Q1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;_ssl_: Fix compiler warning. Cast Py_buffer.len (Py_ssize_t, signed) to
size_t (unsigned) to prevent the &amp;quot;comparison between signed and unsigned
integer expressions&amp;quot; warning.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30486"&gt;bpo-30486&lt;/a&gt;: Make cell_set_contents() symbol private. Don't export the
&lt;tt class="docutils literal"&gt;cell_set_contents()&lt;/tt&gt; symbol in the C API.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2017 Q3: Part 2 (dangling threads)</title><link href="https://haypo.github.io/contrib-cpython-2017q3-part2.html" rel="alternate"></link><published>2017-10-19T15:00:00+02:00</published><updated>2017-10-19T15:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-10-19:/contrib-cpython-2017q3-part2.html</id><summary type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q3
(july, august, september), Part 2: &amp;quot;Dangling threads&amp;quot;.&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part1.html"&gt;My contributions to CPython during 2017 Q3: Part 1&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part3.html"&gt;My contributions
to CPython during 2017 Q3: Part 3 (funny bugs)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Bugfixes: Reference cycles&lt;/li&gt;
&lt;li&gt;socketserver leaking threads and processes&lt;ul&gt;
&lt;li&gt;test_logging random bug …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q3
(july, august, september), Part 2: &amp;quot;Dangling threads&amp;quot;.&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part1.html"&gt;My contributions to CPython during 2017 Q3: Part 1&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part3.html"&gt;My contributions
to CPython during 2017 Q3: Part 3 (funny bugs)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Bugfixes: Reference cycles&lt;/li&gt;
&lt;li&gt;socketserver leaking threads and processes&lt;ul&gt;
&lt;li&gt;test_logging random bug&lt;/li&gt;
&lt;li&gt;Skip failing tests&lt;/li&gt;
&lt;li&gt;Fix socketserver for processes&lt;/li&gt;
&lt;li&gt;Fix socketserver for threads&lt;/li&gt;
&lt;li&gt;Issue not done yet&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Environment altered and dangling threads&lt;ul&gt;
&lt;li&gt;Environment changed&lt;/li&gt;
&lt;li&gt;test.support and regrtest enhancements&lt;/li&gt;
&lt;li&gt;multiprocessing bug fixes&lt;/li&gt;
&lt;li&gt;concurrent.futures bug fixes&lt;/li&gt;
&lt;li&gt;test_threading and test_thread&lt;/li&gt;
&lt;li&gt;Other fixes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="bugfixes-reference-cycles"&gt;
&lt;h2&gt;Bugfixes: Reference cycles&lt;/h2&gt;
&lt;p&gt;While fixing &amp;quot;dangling threads&amp;quot; (see below), I found and fixed 4 reference
cycles which caused memory leaks and objects to live longer than expected. I
was surprised that the bug in the common &lt;tt class="docutils literal"&gt;socket.create_connection()&lt;/tt&gt;
function was not noticed before! So my work on dangling threads was useful!&lt;/p&gt;
&lt;p&gt;The typical pattern of such reference cycle is:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def func():
    err = None
    try:
        do_something()
    except Exception as exc:
        err = exc
    if err is not None:
        handle_error(exc)
    # the exception is stored in the 'err' variable

func()
# surprise, surprise, the exception is still alive at this point!
&lt;/pre&gt;
&lt;p&gt;Or the variant:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def func():
    try:
        do_something()
    except Exception as exc:
        exc_info = sys.exc_info()
        handle_error(exc_info)
    # the exception is stored in the 'exc_info' variable

func()
# surprise, surprise, the exception is still alive at this point!
&lt;/pre&gt;
&lt;p&gt;It's not easy to spot the bug, the bug is subtle. An exception object in Python
3 has a &lt;tt class="docutils literal"&gt;__traceback__&lt;/tt&gt; attribute which contains frames. If a frame stores
the exception in a variable, like &lt;tt class="docutils literal"&gt;err&lt;/tt&gt; in the first example, or &lt;tt class="docutils literal"&gt;exc_info&lt;/tt&gt;
in the second example, a cycle exists between the exception and frames. In this
case, the exception, the traceback, the frames, &lt;strong&gt;and all variables of all
frames are kept alive&lt;/strong&gt; by the reference cycle, &lt;strong&gt;until the cycle is break by
the garbage collector&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The problem is that the garbage collector is only called infrequently, so the
cycle may stay alive for a long time.&lt;/p&gt;
&lt;p&gt;Sometimes, the reference cycle is even more subtle than the simple examples
above.&lt;/p&gt;
&lt;p&gt;Fixed reference cycles:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;,
&lt;tt class="docutils literal"&gt;socket.create_connection()&lt;/tt&gt;: Fix reference cycle.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31247"&gt;bpo-31247&lt;/a&gt;: &lt;tt class="docutils literal"&gt;xmlrpc.server&lt;/tt&gt; now explicitly breaks reference cycles when using
&lt;tt class="docutils literal"&gt;sys.exc_info()&lt;/tt&gt; in code handling exceptions.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31249"&gt;bpo-31249&lt;/a&gt;, &lt;tt class="docutils literal"&gt;concurrent.futures&lt;/tt&gt;:
&lt;tt class="docutils literal"&gt;WorkItem.run()&lt;/tt&gt; used by ThreadPoolExecutor now explicitly breaks a
reference cycle between an exception object and the &lt;tt class="docutils literal"&gt;WorkItem&lt;/tt&gt; object.
&lt;tt class="docutils literal"&gt;ThreadPoolExecutor.shutdown()&lt;/tt&gt; now also clears its threads set.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31238"&gt;bpo-31238&lt;/a&gt;: &lt;tt class="docutils literal"&gt;pydoc&lt;/tt&gt;:
&lt;tt class="docutils literal"&gt;ServerThread.stop()&lt;/tt&gt; now joins itself to wait until
&lt;tt class="docutils literal"&gt;DocServer.serve_until_quit()&lt;/tt&gt; completes and then explicitly sets its
docserver attribute to None to break a reference cycle. This change was made
to fix &lt;tt class="docutils literal"&gt;test_doc&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31323"&gt;bpo-31323&lt;/a&gt;: Fix reference leak in
test_ssl. Store exceptions as string rather than object to prevent reference
cycles which cause leaking dangling threads.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also started a discussion on reference cycles caused by exceptions:
&lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-September/149586.html"&gt;[Python-Dev] Evil reference cycles caused Exception.__traceback__&lt;/a&gt;.
Sadly, no action was taken, no obvious solution was found.&lt;/p&gt;
&lt;p&gt;I found the &lt;tt class="docutils literal"&gt;socket.create_connection()&lt;/tt&gt; reference cycle because of an
unrelated change in test.support:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
bpo-29639: change test.support.HOST to &amp;quot;localhost&amp;quot;
&lt;/pre&gt;
&lt;p&gt;Read &lt;a class="reference external" href="https://bugs.python.org/issue29639#msg302087"&gt;my message&lt;/a&gt; on bpo-29639
for the full story. Extract:&lt;/p&gt;
&lt;blockquote&gt;
Modifying support.HOST to &amp;quot;localhost&amp;quot; triggered a reference cycle!?&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="socketserver-leaking-threads-and-processes"&gt;
&lt;h2&gt;socketserver leaking threads and processes&lt;/h2&gt;
&lt;div class="section" id="test-logging-random-bug"&gt;
&lt;h3&gt;test_logging random bug&lt;/h3&gt;
&lt;p&gt;This story starts at July, 3, with test_logging failing randomly on FreeBSD,
&lt;a class="reference external" href="https://bugs.python.org/issue30830"&gt;bpo-30830&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
test_output (test.test_logging.HTTPHandlerTest) ... ok
Warning -- threading_cleanup() failed to cleanup -1 threads after 3 sec (count: 0, dangling: 1)
&lt;/pre&gt;
&lt;p&gt;I failed to reproduce the bug on my FreeBSD VM, nor on Linux. The bug only
occurred on one specific FreeBSD buildbot. I even got access to the buildbot...
and I still failed to reproduce the bug! I tried to run test_logging multiple
times in parallel, increase the system load, etc. I felt disappointed. I used
my &lt;tt class="docutils literal"&gt;system_load.py&lt;/tt&gt; script which spawns Python processes running &lt;tt class="docutils literal"&gt;while 1:
pass&lt;/tt&gt; to stress the CPU.&lt;/p&gt;
&lt;p&gt;After one month, I succeeded to reproduce the bug by running two commands in
parallel.&lt;/p&gt;
&lt;p&gt;Command 1 to trigger the bug:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
./python -m test -v test_logging \
    --fail-env-changed \
    --forever \
    -m test.test_logging.DatagramHandlerTest.test_output \
    -m test.test_logging.ConfigDictTest.test_listen_config_10_ok \
    -m test.test_logging.SocketHandlerTest.test_output
&lt;/pre&gt;
&lt;p&gt;Command 2 to stress the system:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
./python -m test -j4
&lt;/pre&gt;
&lt;p&gt;It seems like the Python test suite is a very good tool to stress a system to
trigger a race condition!&lt;/p&gt;
&lt;p&gt;Finally, I was able to identify the bug:&lt;/p&gt;
&lt;blockquote&gt;
The problem is that &lt;tt class="docutils literal"&gt;socketserver.ThreadingMixIn&lt;/tt&gt; spawns threads without
waiting for their completion in server_close().&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="skip-failing-tests"&gt;
&lt;h3&gt;Skip failing tests&lt;/h3&gt;
&lt;p&gt;To stabilize the buildbots and to be able to work on other bugs, I decided to
first skip all tests using &lt;tt class="docutils literal"&gt;socketserver.ThreadingMixIn&lt;/tt&gt; until this class was
fixed to prevent &amp;quot;dangling threads&amp;quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fix-socketserver-for-processes"&gt;
&lt;h3&gt;Fix socketserver for processes&lt;/h3&gt;
&lt;p&gt;While trying to see how to fix &lt;tt class="docutils literal"&gt;socketserver.ThreadingMixIn&lt;/tt&gt;, I understood
that &lt;a class="reference external" href="https://bugs.python.org/issue31151"&gt;bpo-31151&lt;/a&gt; was a similar bug in
the &lt;tt class="docutils literal"&gt;socketserver&lt;/tt&gt; module but for processes:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
test_ForkingUDPServer (test.test_socketserver.SocketServerTest) ... creating server
(...)
Warning -- reap_children() reaped child process 18281
&lt;/pre&gt;
&lt;p&gt;My analysis:&lt;/p&gt;
&lt;blockquote&gt;
The problem is that &lt;tt class="docutils literal"&gt;socketserver.ForkinMixin&lt;/tt&gt; doesn't wait until all
children completes. It only calls &lt;tt class="docutils literal"&gt;os.waitpid()&lt;/tt&gt; in non-blocking module
(using &lt;tt class="docutils literal"&gt;os.WNOHANG&lt;/tt&gt;) after each loop iteration. If a child process
completes after the last call to &lt;tt class="docutils literal"&gt;ForkingMixIn.collect_children()&lt;/tt&gt;, the
server leaks zombie processes.&lt;/blockquote&gt;
&lt;p&gt;I fixed &lt;tt class="docutils literal"&gt;socketserver.ForkingMixIn&lt;/tt&gt; by modifying the &lt;tt class="docutils literal"&gt;server_close()&lt;/tt&gt;
method to &lt;strong&gt;block&lt;/strong&gt; until all child processes complete: &lt;a class="reference external" href="https://github.com/python/cpython/commit/aa8ec34ad52bb3b274ce91169e1bc4a598655049"&gt;commit&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Just after pushing my fix, I understood that my fix changed the
&lt;tt class="docutils literal"&gt;ForkingMixIn&lt;/tt&gt; behaviour. I wrote an email to ask if it's the good behaviour
or if a change was needed: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-August/148826.html"&gt;[Python-Dev] socketserver ForkingMixin waiting for
child processes&lt;/a&gt;.
The answer is that not everybody wants this behaviour. Sadly, I didn't have
time yet to let the user chooses the behaviour.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fix-socketserver-for-threads"&gt;
&lt;h3&gt;Fix socketserver for threads&lt;/h3&gt;
&lt;p&gt;Fixing &lt;tt class="docutils literal"&gt;socketserver.ForkinMixin&lt;/tt&gt; was simple because the code already tracked
the (identifier of) child processes and already had code to wait for child
completion.&lt;/p&gt;
&lt;p&gt;Fixing &lt;tt class="docutils literal"&gt;socketserver.ThreadingMixIn&lt;/tt&gt; (&lt;a class="reference external" href="https://bugs.python.org/issue31233"&gt;bpo-31233&lt;/a&gt;) was more complicated since it didn't
keep track of spawned threads.&lt;/p&gt;
&lt;p&gt;I chose to keep a list of &lt;tt class="docutils literal"&gt;threading.Thread&lt;/tt&gt; objects, but only for
non-daemonic threads. &lt;tt class="docutils literal"&gt;socketserver.ThreadingMixIn.server_close()&lt;/tt&gt; now joins
all threads: &lt;a class="reference external" href="https://github.com/python/cpython/commit/b8f4163da30e16c7cd58fe04f4b17e38d53cd57e"&gt;commit&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="issue-not-done-yet"&gt;
&lt;h3&gt;Issue not done yet&lt;/h3&gt;
&lt;p&gt;As I wrote above, the &lt;tt class="docutils literal"&gt;socketserver&lt;/tt&gt; still needs to be reworked to let the
user decides if the server must gracefully wait for child completion or not.
Maybe expose also a method to explicitly wait for children, maybe with a
timeout?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="environment-altered-and-dangling-threads"&gt;
&lt;h2&gt;Environment altered and dangling threads&lt;/h2&gt;
&lt;p&gt;This part kept me busy for the whole quarter. While trying to fix &amp;quot;all bugs&amp;quot;, I
looked at two specific &amp;quot;environment changes&amp;quot;: &amp;quot;dangling threads&amp;quot; and &amp;quot;zombie
processes&amp;quot;. A dangling thread comes from a test spawning a thread but doesn't
proper &amp;quot;clean&amp;quot; the thread.&lt;/p&gt;
&lt;p&gt;Leaking threads or processes is a very bad side effect since it is likely to
cause random bugs in following tests.&lt;/p&gt;
&lt;p&gt;At the beginning, I expected that only 2 or 3 bugs should be fixed. At the end,
it was closer to 100 bugs. I don't regret, I'm now sure that I made the Python
test suite more reliable, and this work allowed me to catch &lt;strong&gt;and fix&lt;/strong&gt; old
reference cycles bugs (see above).&lt;/p&gt;
&lt;div class="section" id="environment-changed"&gt;
&lt;h3&gt;Environment changed&lt;/h3&gt;
&lt;p&gt;To detect bugs, I modified Travis CI jobs, AppVeyor and buildbots to run tests
with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--fail-env-changed&lt;/span&gt;&lt;/tt&gt;. With this option, if a test alters the
environment, the full test suite is marked as failed with &amp;quot;ENV_CHANGED&amp;quot;.&lt;/p&gt;
&lt;p&gt;I also fixed &lt;tt class="docutils literal"&gt;python3 &lt;span class="pre"&gt;-m&lt;/span&gt; test &lt;span class="pre"&gt;--fail-env-changed&lt;/span&gt; &lt;span class="pre"&gt;--forever&lt;/span&gt;&lt;/tt&gt; in &lt;a class="reference external" href="https://bugs.python.org/issue30764"&gt;bpo-30764&lt;/a&gt;: --forever now stops if a test alters
the environment.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="test-support-and-regrtest-enhancements"&gt;
&lt;h3&gt;test.support and regrtest enhancements&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30845"&gt;bpo-30845&lt;/a&gt;: reap_children() now logs
warnings.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;support.reap_children()&lt;/tt&gt; now sets environment_altered to &lt;tt class="docutils literal"&gt;True&lt;/tt&gt; if a
test leaked a zombie process, to detect bugs using &lt;tt class="docutils literal"&gt;python3 &lt;span class="pre"&gt;-m&lt;/span&gt; test
&lt;span class="pre"&gt;--fail-env-changed&lt;/span&gt;&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;regrtest: count also &amp;quot;env changed&amp;quot; tests as failed tests in the test
progress.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;:
&lt;tt class="docutils literal"&gt;support.threading_cleanup()&lt;/tt&gt; now emits a warning immediately if there are
threads running in the background, to be able to catch bugs more easily.
Previously, the warning was only emitted if the function failed to cleanup
these threads after 1 second.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: Add
&lt;tt class="docutils literal"&gt;test.support.wait_threads_exit()&lt;/tt&gt;. Use &lt;tt class="docutils literal"&gt;_thread.count()&lt;/tt&gt; to wait until
threads exit. The new context manager prevents the &amp;quot;dangling thread&amp;quot; warning.
Add also &lt;tt class="docutils literal"&gt;support.join_thread()&lt;/tt&gt; helper: joins a thread but raises an
AssertionError if the thread is still alive after &lt;em&gt;timeout&lt;/em&gt; seconds.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="multiprocessing-bug-fixes"&gt;
&lt;h3&gt;multiprocessing bug fixes&lt;/h3&gt;
&lt;p&gt;The multiprocessing module is very complex. multiprocessing tests are failing
randomly for years, but nobody seems able to fix them. I can only hope that my
following fixes will help to make these tests more reliable.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;multiprocessing.Queue.join_thread() now waits until the thread
completes, even if the thread was started by the same process which
created the queue.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue26762"&gt;bpo-26762&lt;/a&gt;: Avoid daemon processes in _test_multiprocessing. test_level() of
_test_multiprocessing._TestLogging now uses regular processes rather than
daemon processes to prevent zombi processes (to not &amp;quot;leak&amp;quot; processes).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue26762"&gt;bpo-26762&lt;/a&gt;: Fix more dangling processes and threads in test_multiprocessing.
Queue: call close() followed by join_thread(). Process: call join() or
self.addCleanup(p.join).&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue26762"&gt;bpo-26762&lt;/a&gt;: test_multiprocessing now detects dangling processes and threads
per test case classes.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue26762"&gt;bpo-26762&lt;/a&gt;: test_multiprocessing close more queues. Close explicitly queues to
make sure that we don't leave dangling threads. test_queue_in_process():
remove unused queue. test_access() joins also the process to fix a random
warning.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue26762"&gt;bpo-26762&lt;/a&gt;: _test_multiprocessing now marks the test as ENV_CHANGED on
dangling process or thread.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31069"&gt;bpo-31069&lt;/a&gt;, Fix a warning about dangling processes in test_rapid_restart() of
_test_multiprocessing: join the process.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;, test_multiprocessing:
Give 30 seconds to join_process(), instead of 5 or 10 seconds, to wait until
the process completes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="concurrent-futures-bug-fixes"&gt;
&lt;h3&gt;concurrent.futures bug fixes&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30845"&gt;bpo-30845&lt;/a&gt;: Enhance test_concurrent_futures cleanup. Make sure that tests
don't leak threads nor processes. Clear explicitly the reference to the
executor to make sure that it's destroyed.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31249"&gt;bpo-31249&lt;/a&gt;: test_concurrent_futures checks dangling threads. Add a
BaseTestCase class to test_concurrent_futures to check for dangling threads
and processes on all tests, not only tests using ExecutorMixin.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31249"&gt;bpo-31249&lt;/a&gt;: Fix test_concurrent_futures dangling thread.
ProcessPoolShutdownTest.test_del_shutdown() now closes the call queue and
joins its thread, to prevent leaking a dangling thread.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="test-threading-and-test-thread"&gt;
&lt;h3&gt;test_threading and test_thread&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: test_threaded_import: fix
test_side_effect_import().  Don't leak the module into sys.modules. Avoid
also dangling threads.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;:
test_thread.test_forkinthread() now waits until the thread completes.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: Try to fix the
threading_cleanup() warning in test.lock_tests: wait a little bit longer to
give time to the threads to complete. Warning seen on test_thread and
test_importlib.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: Join threads in test_threading. Call thread.join() to prevent the
&amp;quot;dangling thread&amp;quot; warning.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: Join timers in
test_threading. Call the .join() method of threading.Timer timers to prevent
the threading_cleanup() warning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="other-fixes"&gt;
&lt;h3&gt;Other fixes&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;test_urllib2_localnet: clear server variable. Set the server attribute to
None in cleanup to avoid dangling threads.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30818"&gt;bpo-30818&lt;/a&gt;: test_ftplib calls asyncore.close_all(). Always clear asyncore
socket map using asyncore.close_all(ignore_all=True) in tearDown() method.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30908"&gt;bpo-30908&lt;/a&gt;: Fix dangling thread in test_os.TestSendfile. tearDown() now clears
explicitly the self.server variable to make sure that the thread is
completely cleared when tearDownClass() checks if all threads have been
cleaned up.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31067"&gt;bpo-31067&lt;/a&gt;: test_subprocess now also calls reap_children() in tearDown(), not
only on setUp().&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31160"&gt;bpo-31160&lt;/a&gt;: Fix test_builtin for zombie process. PtyTests.run_child() now calls
os.waitpid() to read the exit status of the child process to avoid creating
zombie process and leaking processes in the background.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31160"&gt;bpo-31160&lt;/a&gt;: Fix test_random for zombie process. TestModule.test_after_fork()
now calls os.waitpid() to read the exit status of the child process to avoid
creating a zombie process.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31160"&gt;bpo-31160&lt;/a&gt;: test_tempfile: TestRandomNameSequence.test_process_awareness() now
calls os.waitpid() to avoid leaking a zombie process.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: fork_wait.py tests now joins threads, to not leak running threads
in the background.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30830"&gt;bpo-30830&lt;/a&gt;: test_logging uses threading_setup/cleanup. Replace
&amp;#64;support.reap_threads on some methods with support.threading_setup() in
setUp() and support.threading_cleanup() in tearDown() in BaseTest.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: test_httpservers joins the server thread.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31250"&gt;bpo-31250&lt;/a&gt;, test_asyncio: fix dangling threads. Explicitly call
shutdown(wait=True) on executors to wait until all threads complete to
prevent side effects between tests. Fix test_loop_self_reading_exception():
don't mock loop.close().  Previously, the original close() method was called
rather than the mock, because how set_event_loop() registered loop.close().&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: Explicitly clear the server attribute in test_ftplib and
test_poplib to prevent dangling thread. Clear also self.server_thread
attribute in TestTimeouts.tearDown().&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: Join threads in tests. Call thread.join() on threads to prevent
the &amp;quot;dangling threads&amp;quot; warning.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: Join threads in test_hashlib: use thread.join() to wait until the
parallel hash tasks complete rather than using events. Calling thread.join()
prevent &amp;quot;dangling thread&amp;quot; warnings.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31234"&gt;bpo-31234&lt;/a&gt;: Join threads in test_queue. Call thread.join() to prevent the
&amp;quot;dangling thread&amp;quot; warning.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Next report:&lt;/strong&gt; &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part3.html"&gt;My contributions to CPython during 2017 Q3: Part 3 (funny
bugs)&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2017 Q3: Part 1</title><link href="https://haypo.github.io/contrib-cpython-2017q3-part1.html" rel="alternate"></link><published>2017-10-18T15:00:00+02:00</published><updated>2017-10-18T15:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-10-18:/contrib-cpython-2017q3-part1.html</id><summary type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q3
(july, august, september), Part 1.&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part1.html"&gt;My contributions to CPython during 2017 Q2 (part1)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next reports:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part2.html"&gt;My contributions to CPython during 2017 Q3: Part 2 (dangling
threads)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part3.html"&gt;My contributions to CPython during 2017 Q3: Part 3 (funny bugs)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Security fixes …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q3
(july, august, september), Part 1.&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part1.html"&gt;My contributions to CPython during 2017 Q2 (part1)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next reports:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part2.html"&gt;My contributions to CPython during 2017 Q3: Part 2 (dangling
threads)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part3.html"&gt;My contributions to CPython during 2017 Q3: Part 3 (funny bugs)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Security fixes&lt;/li&gt;
&lt;li&gt;Enhancement: socket.close() now ignores ECONNRESET&lt;/li&gt;
&lt;li&gt;Removal of the macOS job of Travis CI&lt;/li&gt;
&lt;li&gt;New test.pythoninfo utility&lt;/li&gt;
&lt;li&gt;Revert commits if buildbots are broken&lt;/li&gt;
&lt;li&gt;Fix the Python test suite&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="statistics"&gt;
&lt;h2&gt;Statistics&lt;/h2&gt;
&lt;pre class="literal-block"&gt;
# All branches
$ git log --after=2017-06-30 --before=2017-10-01 --reverse --branches='*' --author=Stinner|grep '^commit ' -c
209

# Master branch only
$ git log --after=2017-06-30 --before=2017-10-01 --reverse --author=Stinner origin/master|grep '^commit ' -c
97
&lt;/pre&gt;
&lt;p&gt;Statistics: I pushed &lt;strong&gt;97&lt;/strong&gt; commits in the master branch on a &lt;strong&gt;total of 209
commits&lt;/strong&gt;, remaining: 112 commits in the other branches (backports, fixes
specific to Python 2.7, security fixes in Python 3.3 and 3.4, etc.)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="security-fixes"&gt;
&lt;h2&gt;Security fixes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30947"&gt;bpo-30947&lt;/a&gt;: Update libexpat from 2.2.1 to 2.2.3. Fix applied to master, 3.6,
3.5, 3.4, 3.3 and 2.7 branches! Expat 2.2.2 and 2.2.3 fixed multiple security
vulnerabilities.
&lt;a class="reference external" href="http://python-security.readthedocs.io/vuln/expat_2.2.3.html"&gt;http://python-security.readthedocs.io/vuln/expat_2.2.3.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fix whichmodule() of _pickle: : _PyUnicode_FromId() can return NULL, replace
Py_INCREF() with Py_XINCREF(). Fix coverity report: CID 1417269.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30860"&gt;bpo-30860&lt;/a&gt;: &lt;tt class="docutils literal"&gt;_PyMem_Initialize()&lt;/tt&gt; contains code which is never executed.
Replace the runtime check with a build assertion. Fix Coverity CID 1417587.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See also my &lt;a class="reference external" href="http://python-security.readthedocs.io/"&gt;python-security website&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="enhancement-socket-close-now-ignores-econnreset"&gt;
&lt;h2&gt;Enhancement: socket.close() now ignores ECONNRESET&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30319"&gt;bpo-30319&lt;/a&gt;: socket.close() now ignores ECONNRESET. Previously, many network
tests failed randomly with ConnectionResetError on socket.close().&lt;/p&gt;
&lt;p&gt;Patching all functions calling socket.close() would require a lot of work, and
it was surprising to get a &amp;quot;connection reset&amp;quot; when closing a socket.&lt;/p&gt;
&lt;p&gt;Who cares that the peer closed the connection, since we are already closing
it!?&lt;/p&gt;
&lt;p&gt;Note: socket.close() was modified in Python 3.6 to raise OSError on failure
(&lt;a class="reference external" href="https://bugs.python.org/issue26685"&gt;bpo-26685&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="removal-of-the-macos-job-of-travis-ci"&gt;
&lt;h2&gt;Removal of the macOS job of Travis CI&lt;/h2&gt;
&lt;a class="reference external image-reference" href="https://travis-ci.org/"&gt;&lt;img alt="call_method microbenchmark" class="align-right" src="https://haypo.github.io/images/travis-ci.png" /&gt;&lt;/a&gt;
&lt;p&gt;While the Linux jobs of Travis CI usually takes 15 minutes, up to 30 minutes in
the worst case, the macOS job of Travis CI regulary took longer than 30
minutes, sometimes longer than 1 hour.&lt;/p&gt;
&lt;p&gt;While the macOS job was optional, sometimes it gone mad and prevented a PR to
be merged. Cancelling the job marked Travis CI as failed on a PR, so it was
still not possible to merge the PR, whereas, again, the job is marked as
optional (&amp;quot;Allowed Failure&amp;quot;).&lt;/p&gt;
&lt;p&gt;Moreover, when the macOS job failed, the failure was not reported on the PR,
since the job was marked as optional. The only way to notify a failure was to
go to Travis CI and wait at least 30 minutes (whereas the Linux jobs already
completed and it was already possible merge a PR...).&lt;/p&gt;
&lt;p&gt;I sent a first mail in June: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-committers/2017-June/004661.html"&gt;[python-committers] macOS Travis CI job became
mandatory?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In september, we decided to remove the macOS job during the CPython sprint at
Instagram (see my previous &lt;a class="reference external" href="https://haypo.github.io/new-python-c-api.html"&gt;New C API&lt;/a&gt;
article), to not slowdown our development speed (&lt;a class="reference external" href="https://bugs.python.org/issue31355"&gt;bpo-31355&lt;/a&gt;). I sent another
email to announce the change: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-committers/2017-September/004824.html"&gt;[python-committers] Travis CI: macOS is now
blocking -- remove macOS from Travis CI?&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;After the sprint, it was decided to not add again the macOS job, since we have
3 macOS buildbots. It's enough to detect regressions specific to macOS.&lt;/p&gt;
&lt;p&gt;After the removal of the macOS end, at the end of september, Travis CI
published an article about the bad performances of their macOS fleet: &lt;a class="reference external" href="https://blog.travis-ci.com/2017-09-22-macos-update"&gt;Updating
Our macOS Open Source Offering&lt;/a&gt;. Sadly, the article
confirms that the situation is not going to evolve quickly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="new-test-pythoninfo-utility"&gt;
&lt;h2&gt;New test.pythoninfo utility&lt;/h2&gt;
&lt;p&gt;To understand the &amp;quot;Segfault when readline history is more then 2 * history
size&amp;quot; crash of &lt;a class="reference external" href="https://bugs.python.org/issue29854"&gt;bpo-29854&lt;/a&gt;, I modified
&lt;tt class="docutils literal"&gt;test_readline&lt;/tt&gt; to log libreadline  versions.  I also added
&lt;tt class="docutils literal"&gt;readline._READLINE_LIBRARY_VERSION&lt;/tt&gt;. My colleague &lt;strong&gt;Nir Soffer&lt;/strong&gt; wrote the
final readline fix: skip the test on old readline versions.&lt;/p&gt;
&lt;p&gt;As a follow-up of this issue, I added a new &lt;tt class="docutils literal"&gt;test.pythoninfo&lt;/tt&gt; program to log
many information to debug Python tests (&lt;a class="reference external" href="https://bugs.python.org/issue30871"&gt;bpo-30871&lt;/a&gt;). pythoninfo is now run on
Travis CI, AppVeyor and buildbots.&lt;/p&gt;
&lt;p&gt;Example of output:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ ./python -m test.pythoninfo
(...)
_decimal.__libmpdec_version__: 2.4.2
expat.EXPAT_VERSION: expat_2.2.4
gdb_version: GNU gdb (GDB) Fedora 8.0.1-26.fc26
locale.encoding: UTF-8
os.cpu_count: 4
(...)
time.timezone: -3600
time.tzname: ('CET', 'CEST')
tkinter.TCL_VERSION: 8.6
tkinter.TK_VERSION: 8.6
tkinter.info_patchlevel: 8.6.6
zlib.ZLIB_RUNTIME_VERSION: 1.2.11
zlib.ZLIB_VERSION: 1.2.11
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;test.pythoninfo&lt;/tt&gt; can be easily extended to log more information, without
polluting the output of the Python test suite which is already too verbose and
very long.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="revert-commits-if-buildbots-are-broken"&gt;
&lt;h2&gt;Revert commits if buildbots are broken&lt;/h2&gt;
&lt;p&gt;Thanks to my work done last months on the Python test suite, the buildbots are
now very reliable. When a buildbot fails, it becomes very likely that it's a
real regression, and not a random failure caused by a bug in the Python test
suite.&lt;/p&gt;
&lt;p&gt;I proposed a new rule: &lt;strong&gt;revert a change if it breaks builbots and the but
cannot be fixed easily&lt;/strong&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;So I would like to set a new rule: if I'm unable to fix buildbots
failures caused by a recent change quickly (say, in less than 2
hours), I propose to revert the change.&lt;/p&gt;
&lt;p&gt;It doesn't mean that the commit is bad and must not be merged ever.
No. It would just mean that we need time to work on fixing the issue,
and it shouldn't impact other pending changes, to keep a sane master
branch.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a class="reference external" href="https://mail.python.org/pipermail/python-committers/2017-June/004588.html"&gt;[python-committers] Revert changes which break too many buildbots&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="test-datetime"&gt;
&lt;h3&gt;test_datetime&lt;/h3&gt;
&lt;p&gt;The first revert was an enhancement of test_datetime, &lt;a class="reference external" href="https://bugs.python.org/issue30822"&gt;bpo-30822&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
commit 98b6bc3bf72532b784a1c1fa76eaa6026a663e44
Author: Utkarsh Upadhyay &amp;lt;mail&amp;#64;musicallyut.in&amp;gt;
Date:   Sun Jul 2 14:46:04 2017 +0200

    bpo-30822: Fix testing of datetime module. (#2530)

    Only C implementation was tested.
&lt;/pre&gt;
&lt;p&gt;I wrote an email to announce the revert: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-committers/2017-July/004673.html"&gt;[python-committers] Revert changes
which break too many buildbots&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It took 15 days to decide how to fix properly the issue (exclude &lt;tt class="docutils literal"&gt;tzdata&lt;/tt&gt;
from test resources). I don't regret my revert, since having broken buildbots
for 15 days would be very annoying.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-gdb-py-fix"&gt;
&lt;h3&gt;python-gdb.py fix&lt;/h3&gt;
&lt;p&gt;I also reverted this commit of &lt;a class="reference external" href="https://bugs.python.org/issue30983"&gt;bpo-30983&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
commit 2e0f4db114424a00354eab889ba8f7334a2ab8f0
Author: Bruno &amp;quot;Polaco&amp;quot; Penteado &amp;lt;polaco&amp;#64;gmail.com&amp;gt;
Date:   Mon Aug 14 23:14:17 2017 +0100

    bpo-30983: eval frame rename in pep 0523 broke gdb's python extension (#2803)

    pep 0523 renames PyEval_EvalFrameEx to _PyEval_EvalFrameDefault while the gdb python extension only looks for PyEval_EvalFrameEx to understand if it is dealing with a frame.

    Final effect is that attaching gdb to a python3.6 process doesnt resolve python objects. Eg. py-list and py-bt dont work properly.

    This patch fixes that. Tested locally on python3.6
&lt;/pre&gt;
&lt;p&gt;My comment on the issue:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I chose to revert the change because I don't have the bandwidth right now
to investigate why the change broke test_gdb.&lt;/p&gt;
&lt;p&gt;I'm surprised that a change affecting python-gdb.py wasn't properly tested
manually using test_gdb.py :-( I understand that Travis CI doesn't have gdb
and/or that the test pass in some cases?&lt;/p&gt;
&lt;p&gt;The revert only gives us more time to design the proper solution.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hopefully, a new fixed commit was pushed 4 days later and this one didn't break
buildbots!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="fix-the-python-test-suite"&gt;
&lt;h2&gt;Fix the Python test suite&lt;/h2&gt;
&lt;p&gt;As usual, I spent a significant part of my time to fix bugs in the Python test
suite to make it more reliable and more &amp;quot;usable&amp;quot;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30822"&gt;bpo-30822&lt;/a&gt;: Exclude &lt;tt class="docutils literal"&gt;tzdata&lt;/tt&gt; from &lt;tt class="docutils literal"&gt;regrtest &lt;span class="pre"&gt;--all&lt;/span&gt;&lt;/tt&gt;. When running the test suite
using &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--use=all&lt;/span&gt;&lt;/tt&gt; / &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-u&lt;/span&gt; all&lt;/tt&gt;, exclude &lt;tt class="docutils literal"&gt;tzdata&lt;/tt&gt; since it makes
test_datetime too slow (15-20 min on some buildbots, just this single test
file) which then times out on some buildbots. &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-u&lt;/span&gt; tzdata&lt;/tt&gt; must now be
enabled explicitly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30188"&gt;bpo-30188&lt;/a&gt;, test_nntplib: Catch also
ssl.SSLEOFError in NetworkedNNTPTests.setUpClass(), not only EOFError.
(&lt;em&gt;Sadly, test_nntplib still fails randomly with EOFError or SSLEOFError...&lt;/em&gt;)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31009"&gt;bpo-31009&lt;/a&gt;: Fix
&lt;tt class="docutils literal"&gt;support.fd_count()&lt;/tt&gt; on Windows. Call &lt;tt class="docutils literal"&gt;msvcrt.CrtSetReportMode()&lt;/tt&gt; to not
kill the process nor log any error on stderr on os.dup(fd) if the file
descriptor is invalid.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31034"&gt;bpo-31034&lt;/a&gt;: Reliable signal handler for test_asyncio. Don't rely on the
current SIGHUP signal handler, make sure that it's set to the &amp;quot;default&amp;quot;
signal handler: SIG_DFL. A colleague reported me that the Python test suite
hangs on running test_subprocess_send_signal() of test_asyncio. After
analysing the issue, it seems like the test hangs because the RPM package
builder ignores SIGHUP.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31028"&gt;bpo-31028&lt;/a&gt;: Fix test_pydoc when run
directly. Fix &lt;tt class="docutils literal"&gt;get_pydoc_link()&lt;/tt&gt;: get the absolute path to &lt;tt class="docutils literal"&gt;__file__&lt;/tt&gt; to
prevent relative directories.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31066"&gt;bpo-31066&lt;/a&gt;: Fix
&lt;tt class="docutils literal"&gt;test_httpservers.test_last_modified()&lt;/tt&gt;. Write the temporary file on disk
and then get its modification time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31173"&gt;bpo-31173&lt;/a&gt;: Rewrite WSTOPSIG test of test_subprocess.&lt;/p&gt;
&lt;p&gt;The current &lt;tt class="docutils literal"&gt;test_child_terminated_in_stopped_state()&lt;/tt&gt; function test creates a
child process which calls &lt;tt class="docutils literal"&gt;ptrace(PTRACE_TRACEME, 0, 0)&lt;/tt&gt; and then crash
(SIGSEGV). The problem is that calling &lt;tt class="docutils literal"&gt;os.waitpid()&lt;/tt&gt; in the parent process is
not enough to close the process: the child process remains alive and so the
unit test leaks a child process in a strange state. Closing the child process
requires non-trivial code, maybe platform specific.&lt;/p&gt;
&lt;p&gt;Remove the functional test and replaces it with an unit test which mocks
&lt;tt class="docutils literal"&gt;os.waitpid()&lt;/tt&gt; using a new &lt;tt class="docutils literal"&gt;_testcapi.W_STOPCODE()&lt;/tt&gt; function to test the
&lt;tt class="docutils literal"&gt;WIFSTOPPED()&lt;/tt&gt; path.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31008"&gt;bpo-31008&lt;/a&gt;: Fix asyncio
test_wait_for_handle on Windows, tolerate a difference of 50 ms.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31235"&gt;bpo-31235&lt;/a&gt;: Fix ResourceWarning in
test_logging: always close all asyncore dispatchers (ignoring errors if any).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue30121"&gt;bpo-30121&lt;/a&gt;: Add test_subprocess.test_nonexisting_with_pipes(). Test the Popen
failure when Popen was created with pipes. Create also NONEXISTING_CMD
variable in test_subprocess.py.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31250"&gt;bpo-31250&lt;/a&gt;, test_asyncio: fix EventLoopTestsMixin.tearDown(). Call
doCleanups() to close the loop after calling executor.shutdown(wait=True).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;test_ssl: Implement timeout in ssl_io_loop(). The timeout parameter was not
used.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31448"&gt;bpo-31448&lt;/a&gt;, test_poplib: Call POP3.close(), don't close close directly the
sock attribute to fix a ResourceWarning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;os.test_utime_current(): tolerate 50 ms delta.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31135"&gt;bpo-31135&lt;/a&gt;: ttk: fix LabeledScale and OptionMenu destroy() method. Call the
parent destroy() method even if the used attribute doesn't exist. The
LabeledScale.destroy() method now also explicitly clears label and scale
attributes to help the garbage collector to destroy all widgets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;&lt;a class="reference external" href="https://bugs.python.org/issue31479"&gt;bpo-31479&lt;/a&gt;: Always reset the signal alarm in tests. Use
the &lt;tt class="docutils literal"&gt;try: ... finally: signal.signal(0)&lt;/tt&gt; pattern to make sure that tests
don't &amp;quot;leak&amp;quot; a pending fatal signal alarm. Move some signal.alarm() calls
into the try block.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Next report:&lt;/strong&gt; &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part2.html"&gt;My contributions to CPython during 2017 Q3: Part 2 (dangling
threads)&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>Python Security</title><link href="https://haypo.github.io/python-security.html" rel="alternate"></link><published>2017-09-15T22:00:00+02:00</published><updated>2017-09-15T22:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-09-15:/python-security.html</id><summary type="html">&lt;p&gt;I am working on the Python security for years, but I never wrote anything about
that. Let's fix this!&lt;/p&gt;
&lt;div class="section" id="psrt"&gt;
&lt;h2&gt;PSRT&lt;/h2&gt;
&lt;p&gt;I am part of the Python Security Response Team (PSRT): I get emails sent to
&lt;a class="reference external" href="mailto:security&amp;#64;python.org"&gt;security&amp;#64;python.org&lt;/a&gt;. I try to analyze each report to validate that the bug
is …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I am working on the Python security for years, but I never wrote anything about
that. Let's fix this!&lt;/p&gt;
&lt;div class="section" id="psrt"&gt;
&lt;h2&gt;PSRT&lt;/h2&gt;
&lt;p&gt;I am part of the Python Security Response Team (PSRT): I get emails sent to
&lt;a class="reference external" href="mailto:security&amp;#64;python.org"&gt;security&amp;#64;python.org&lt;/a&gt;. I try to analyze each report to validate that the bug
is reproductible, find impacted Python versions and start to discuss how to fix
the vulnerability. In some cases, the reported issue is not a security
vulnerability, is not related to CPython, or sometimes is already fixed.  We
also get reports about CPython, but also the web sites and other projects
related to Python.&lt;/p&gt;
&lt;p&gt;Warning: I don't represent the PSRT, I speak for my own!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="vulnerabilities-sent-to-psrt"&gt;
&lt;h2&gt;Vulnerabilities sent to PSRT&lt;/h2&gt;
&lt;p&gt;In this article, I will focus on vulnerabilities impacting CPython: the C and
Python code of CPython core and the standard library.&lt;/p&gt;
&lt;p&gt;When vulnerabilities are obvious bugs, they are quickly fixed. Done.&lt;/p&gt;
&lt;p&gt;But it's not uncommon that fixing a vulnerability impacts the backward
compatibility which is a major concern of CPython core developers. There is
also a risk of rejecting legit input data because the added checks are too
strict. We have to be very careful and so fixing vulnerabilities can take
weeks, if not months in the worst case.&lt;/p&gt;
&lt;p&gt;While CPython has few active core developers, the PSRT has even lesser active
members to handle incoming reports. We are volunteers, so please be kind and
patient...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-of-a-complex-fix"&gt;
&lt;h2&gt;Example of a complex fix&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://python-security.readthedocs.io/vuln/urllib_ftp_protocol_stream_injection.html"&gt;urllib FTP protocol stream injection&lt;/a&gt;
vulnerability was reported to the PSRT at 2016-01-15. The fix was only merged
at 2017-07-26.&lt;/p&gt;
&lt;p&gt;First, it was not obvious how the vulnerability can be exploited, nor if it
should be fixed.&lt;/p&gt;
&lt;p&gt;Then it was not obvious if the vulnerability should be fixed in the urllib
module or in the ftplib module.&lt;/p&gt;
&lt;p&gt;Even if the bug was public, it didn't get much attention. Since I don't know
well how the urllib module, I wrote an email to the python-dev mailing
list: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-July/148699.html"&gt;Need help to fix urllib(.parse) vulnerabilities&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I proposed a fix for the urllib module: &lt;a class="reference external" href="https://bugs.python.org/issue30713"&gt;Reject newline character (U+000A) in
URLs in urllib.parse&lt;/a&gt;. But it was
rejected, since it was the wrong approach and my checks were too strict in many
cases (rejected legit requests).&lt;/p&gt;
&lt;p&gt;The final fix rejects &lt;tt class="docutils literal"&gt;\b&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;\r&lt;/tt&gt; newline characters in the putline()
method of the ftplib module.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="track-known-and-fixed-cpython-vulnerabilities"&gt;
&lt;h2&gt;Track known and fixed CPython vulnerabilities&lt;/h2&gt;
&lt;p&gt;Currently, not least that six branches still get security fixes!&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Python 2.7&lt;/li&gt;
&lt;li&gt;Python 3.3&lt;/li&gt;
&lt;li&gt;Python 3.4&lt;/li&gt;
&lt;li&gt;Python 3.5&lt;/li&gt;
&lt;li&gt;Python 3.6&lt;/li&gt;
&lt;li&gt;master: the development branch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Last year, I added a table to the Python developer guide to help me to track
the status of each branch: see the &lt;a class="reference external" href="https://devguide.python.org/#status-of-python-branches"&gt;Status of Python branches&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This year, I created a tool to help me to track known CPython vulnerabilities:
&lt;a class="reference external" href="https://github.com/vstinner/python-security"&gt;python-security project&lt;/a&gt; (hosted
at GitHub). The &lt;a class="reference external" href="https://github.com/vstinner/python-security/blob/master/vulnerabilities.yaml"&gt;vulnerabilities.yaml file&lt;/a&gt;
is a YAML file with one section per vulnerability. Each vulnerability has
a title, link to the Python bug, disclosure date, reported date, commits, etc.&lt;/p&gt;
&lt;p&gt;The tool gets the date of commits and the Git tags which contains the commit
to infer the first Python versions of each branch which contain the fix. It
also build a timeline to help to understand how the vulnerability was handled.&lt;/p&gt;
&lt;p&gt;I also wanted to be more transparent on how we handle vulnerabilities and our
velocity to fix them.&lt;/p&gt;
&lt;p&gt;Honestly, I was disappointed that it took so long to fix some vulnerabilities
in the past. Hopefully, it seems like we are more reactive nowadays!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-of-a-fixed-vulnerability"&gt;
&lt;h2&gt;Example of a fixed vulnerability&lt;/h2&gt;
&lt;p&gt;Example: &lt;a class="reference external" href="https://python-security.readthedocs.io/vuln/cve-2016-5699_http_header_injection.html"&gt;CVE-2016-5699: HTTP header injection&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Right now, Python 3.3 is still vulnerable (my fix was commited, I am now
waiting Python 3.3.7 which is coming at the end of september).&lt;/p&gt;
&lt;p&gt;Since the vulnerability was reported, it took 108 days to merge the fix, 72
more days (total 180 days) for the first release including the fix (Python
2.7.10).&lt;/p&gt;
&lt;p&gt;Sadly, the PSRT doesn't compute a severity of vulnerabilities yet.&lt;/p&gt;
&lt;p&gt;Hopefully, for this vulnerability, web frameworks were able to workaround the
vulnerability by input sanitization.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="backport-all-fixes"&gt;
&lt;h2&gt;Backport all fixes&lt;/h2&gt;
&lt;p&gt;Last months, I backported fixes to the six branches which still accept security
fixes, to respect the contract with our users: we are doing our best to protect
you!&lt;/p&gt;
&lt;p&gt;The good news is that with Python 2.7.14 and Python 3.3.7 releases scheduled
this month, all major security vulnerabilities will be fixed in all maintained
Python branches!&lt;/p&gt;
&lt;p&gt;Some fixes were not backported on purpose. For example, the &lt;a class="reference external" href="https://python-security.readthedocs.io/vuln/cve-2013-7040_hash_not_properly_randomized.html#cve-2013-7040-hash-not-properly-randomized"&gt;CVE-2013-7040:
Hash not properly randomized&lt;/a&gt;
vulnerability requires to change the hash algorithm and we decided to not touch
Python 2.7 and 3.3 for backward compatibility reasons (don't break code relying
on the exact hash function). The issue was fixed in Python 3.4 by using the
SipHash hash algorithm which uses a hash secret (generated randomly by Python
at startup).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-security-documentation"&gt;
&lt;h2&gt;Python security documentation&lt;/h2&gt;
&lt;p&gt;Last months, I also started to collect random notes about the Python security.&lt;/p&gt;
&lt;p&gt;Explore my &lt;a class="reference external" href="https://python-security.readthedocs.io/"&gt;python-security.readthedocs.io&lt;/a&gt; documentation and send me feedback!&lt;/p&gt;
&lt;/div&gt;
</content><category term="security"></category><category term="python"></category></entry><entry><title>A New C API for CPython</title><link href="https://haypo.github.io/new-python-c-api.html" rel="alternate"></link><published>2017-09-07T18:00:00+02:00</published><updated>2017-09-07T18:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-09-07:/new-python-c-api.html</id><summary type="html">&lt;p&gt;I am currently at a CPython sprint 2017 at Facebook. We are discussing my idea
of writing a new C API for CPython hiding implementation details and replacing
macros with function calls.&lt;/p&gt;
&lt;img alt="CPython sprint at Facebook, september 2017" src="https://haypo.github.io/images/cpython_sprint_sept2017.jpg" /&gt;
&lt;p&gt;This article tries to explain why the CPython C API needs to &lt;strong&gt;evolve&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="section" id="c-api-prevents-further-optimizations"&gt;
&lt;h2&gt;C API prevents further optimizations …&lt;/h2&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;I am currently at a CPython sprint 2017 at Facebook. We are discussing my idea
of writing a new C API for CPython hiding implementation details and replacing
macros with function calls.&lt;/p&gt;
&lt;img alt="CPython sprint at Facebook, september 2017" src="https://haypo.github.io/images/cpython_sprint_sept2017.jpg" /&gt;
&lt;p&gt;This article tries to explain why the CPython C API needs to &lt;strong&gt;evolve&lt;/strong&gt;.&lt;/p&gt;
&lt;div class="section" id="c-api-prevents-further-optimizations"&gt;
&lt;h2&gt;C API prevents further optimizations&lt;/h2&gt;
&lt;p&gt;The CPython &lt;tt class="docutils literal"&gt;PyListObject&lt;/tt&gt; type uses an array of &lt;tt class="docutils literal"&gt;PyObject*&lt;/tt&gt; objects. PyPy
is able to use a C array of integers if the list only contains small integers.
CPython cannot because PyList_GET_ITEM(list, index) is implemented as a macro:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
#define PyList_GET_ITEM(op, i) ((PyListObject *)op)-&amp;gt;ob_item[i]
&lt;/pre&gt;
&lt;p&gt;The macro relies on the &lt;tt class="docutils literal"&gt;PyListObject&lt;/tt&gt; structure:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
typedef struct {
    PyVarObject ob_base;
    PyObject **ob_item;   // &amp;lt;-- pointer to real data
    Py_ssize_t allocated;
} PyListObject;

typedef struct {
    PyObject ob_base;
    Py_ssize_t ob_size; /* Number of items in variable part */
} PyVarObject;

typedef struct _object {
    Py_ssize_t ob_refcnt;
    struct _typeobject *ob_type;
} PyObject;
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="api-and-abi"&gt;
&lt;h2&gt;API and ABI&lt;/h2&gt;
&lt;p&gt;Compiling C extension code using &lt;tt class="docutils literal"&gt;PyList_GET_ITEM()&lt;/tt&gt; produces machine code
accessing &lt;tt class="docutils literal"&gt;PyListObject&lt;/tt&gt; members. Something like (C pseudo code):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PyObject **items;
PyObject *item;
items = (PyObject **)(((char*)list) + 24);
item = items[i];
&lt;/pre&gt;
&lt;p&gt;The offset 24 is hardcoded in the C extension object file: the &lt;strong&gt;API&lt;/strong&gt;
(&lt;strong&gt;programming&lt;/strong&gt; interface) becomes the &lt;strong&gt;ABI&lt;/strong&gt; (&lt;strong&gt;binary&lt;/strong&gt; interface).&lt;/p&gt;
&lt;p&gt;But debug builds use a different memory layout:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
typedef struct _object {
    struct _object *_ob_next;   // &amp;lt;--- two new fields are added
    struct _object *_ob_prev;   // &amp;lt;--- for debug purpose
    Py_ssize_t ob_refcnt;
    struct _typeobject *ob_type;
} PyObject;
&lt;/pre&gt;
&lt;p&gt;The machine code becomes something like:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
items = (PyObject **)(((char*)op) + 40);
item = items[i];
&lt;/pre&gt;
&lt;p&gt;The offset changes from 24 to 40 (+16, two pointers of 8 bytes).&lt;/p&gt;
&lt;p&gt;C extensions have to be recompiled to work on Python compiled in debug mode.&lt;/p&gt;
&lt;p&gt;Another example is Python 2.7 which uses a different ABI for UTF-16 and UCS-4
Unicode string: the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--with-wide-unicode&lt;/span&gt;&lt;/tt&gt; configure option.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="stable-abi"&gt;
&lt;h2&gt;Stable ABI&lt;/h2&gt;
&lt;p&gt;If the machine code doesn't use the offset, it would be able to only compile C
extensions once.&lt;/p&gt;
&lt;p&gt;A solution is to replace PyList_GET_ITEM() &lt;strong&gt;macro&lt;/strong&gt; with a &lt;strong&gt;function&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PyObject* PyList_GET_ITEM(PyObject *list, Py_ssize_t index);
&lt;/pre&gt;
&lt;p&gt;defined as:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PyObject* PyList_GET_ITEM(PyObject *list, Py_ssize_t index)
{
    return ((PyListObject *)list)-&amp;gt;ob_item[i];
}
&lt;/pre&gt;
&lt;p&gt;The machine code becomes a &lt;strong&gt;function call&lt;/strong&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PyObject *item;
item = PyList_GET_ITEM(list, index);
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="specialized-list-for-small-integers"&gt;
&lt;h2&gt;Specialized list for small integers&lt;/h2&gt;
&lt;p&gt;If C extension objects don't access structure members anymore, it becomes
possible to modify the memory layout.&lt;/p&gt;
&lt;p&gt;For example, it's possible to design a specialized implementation of
&lt;tt class="docutils literal"&gt;PyListObject&lt;/tt&gt; for small integers:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
typedef struct {
    PyVarObject ob_base;
    int use_small_int;
    PyObject **pyobject_array;
    int32_t *small_int_array;   // &amp;lt;-- new compact C array for integers
    Py_ssize_t allocated;
} PyListObject;

PyObject* PyList_GET_ITEM(PyObject *op, Py_ssize_t index)
{
    PyListObject *list = (PyListObject *)op;
    if (list-&amp;gt;use_small_int) {
        int32_t item = list-&amp;gt;small_int_array[index];
        /* create a new object at each call */
        return PyLong_FromLong(item);
    }
    else {
        return list-&amp;gt;pyobject_array[index];
    }
}
&lt;/pre&gt;
&lt;p&gt;It's just an example to show that it becomes possible to modify PyObject
structures. I'm not sure that it's useful in practice.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="multiple-python-runtimes"&gt;
&lt;h2&gt;Multiple Python &amp;quot;runtimes&amp;quot;&lt;/h2&gt;
&lt;p&gt;Assuming that all used C extensions use the new stable ABI, we can now imagine
multiple specialized Python runtimes installed in parallel, instead of a single
runtime:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;python3.7: regular/legacy CPython, backward compatible&lt;/li&gt;
&lt;li&gt;python3.7-dbg: runtime checks to ease debug&lt;/li&gt;
&lt;li&gt;fasterpython3.7: use specialized list&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;python3&lt;/tt&gt; runtime would remain &lt;strong&gt;fully&lt;/strong&gt; compatible since it would use
the old C API with macros and full structures. So by default, everything will
continue to work.&lt;/p&gt;
&lt;p&gt;But the other runtimes require that all imported C extensions were compiled
with the new C API.&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;python3.7-dbg&lt;/span&gt;&lt;/tt&gt; adds more checks tested at runtime. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PyObject* PyList_GET_ITEM(PyObject *list, Py_ssize_t index)
{
    assert(PyList_Check(list));
    assert(0 &amp;lt;= index &amp;amp;&amp;amp; index &amp;lt; Py_SIZE(list));
    return ((PyListObject *)list)-&amp;gt;ob_item[i];
}
&lt;/pre&gt;
&lt;p&gt;Currently, some Linux distributions provide a &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;python3-dbg&lt;/span&gt;&lt;/tt&gt; binary, but may
not provide &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-dbg&lt;/span&gt;&lt;/tt&gt; binary packages of all C extensions. So all C extensions
have to be recompiled manually which is quite painful (need to install build
dependencies, wait until everthing is recompiled, etc.).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="experiment-optimizations"&gt;
&lt;h2&gt;Experiment optimizations&lt;/h2&gt;
&lt;p&gt;With the new C API, it becomes possible to implement a new class of
optimizations.&lt;/p&gt;
&lt;div class="section" id="tagged-pointer"&gt;
&lt;h3&gt;Tagged pointer&lt;/h3&gt;
&lt;p&gt;Store small integers directly into the pointer value. Reduce the memory usage,
avoid expensive unboxing-boxing.&lt;/p&gt;
&lt;p&gt;See &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tagged_pointer"&gt;Wikipedia: Tagged pointer&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="no-garbage-collector-gc-at-all"&gt;
&lt;h3&gt;No garbage collector (GC) at all&lt;/h3&gt;
&lt;p&gt;Python runtime without GC at all. Remove the following header from objects
tracked by the GC:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
struct {
    union _gc_head *gc_next;
    union _gc_head *gc_prev;
    Py_ssize_t gc_refs;
} PyGC_Head;
&lt;/pre&gt;
&lt;p&gt;It would remove 24 bytes per object tracked by the GC.&lt;/p&gt;
&lt;p&gt;For comparison, the smallest Python object is &amp;quot;object()&amp;quot; which only takes 16
bytes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="tracing-garbage-collector-without-reference-counting"&gt;
&lt;h3&gt;Tracing garbage collector without reference counting&lt;/h3&gt;
&lt;p&gt;This idea is really the most complex and most experimental idea, but IMHO it's
required to &amp;quot;unlock&amp;quot; Python performances.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Write a new API to keep track of pointers:&lt;ul&gt;
&lt;li&gt;Declare a variable storing a &lt;tt class="docutils literal"&gt;PyObject*&lt;/tt&gt; object&lt;/li&gt;
&lt;li&gt;Set a pointer&lt;/li&gt;
&lt;li&gt;Maybe also read a pointer?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Modify C extensions to use this new API&lt;/li&gt;
&lt;li&gt;Implement a tracing garbage collector which can move objects in memory
to compact memory&lt;/li&gt;
&lt;li&gt;Remove reference counting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It even seems possible to implement a tracing garbage collector &lt;strong&gt;and&lt;/strong&gt; use
reference counting. But I'm not an expert in this area, need to dig the topic.&lt;/p&gt;
&lt;p&gt;Questions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Is it possible to fix all C extensions to use the new API? Should be an
opt-in option in a first stage.&lt;/li&gt;
&lt;li&gt;Is it possible to emulate Py_INCREF/DECREF API, for backward compatibility,
using an hash table which maintains a reference counter outside &lt;tt class="docutils literal"&gt;PyObject&lt;/tt&gt;?&lt;/li&gt;
&lt;li&gt;Do we need to fix all C extensions?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Read also &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Tracing_garbage_collection"&gt;Wikipedia: Tracing garbage collection&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="gilectomy"&gt;
&lt;h3&gt;Gilectomy&lt;/h3&gt;
&lt;p&gt;Abstracting the ABI allows to customize the runtime for Gilectomy needs, to be
able to reemove the GIL.&lt;/p&gt;
&lt;p&gt;Removing reference counting would make Gilectomy much simpler.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="optimization"></category><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2017 Q2 (part 3)</title><link href="https://haypo.github.io/contrib-cpython-2017q2-part3.html" rel="alternate"></link><published>2017-07-13T17:00:00+02:00</published><updated>2017-07-13T17:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-07-13:/contrib-cpython-2017q2-part3.html</id><summary type="html">&lt;p&gt;This is the third part of my contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q2 (april, may, june):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;Trick bug: Clang 4.0, dtoa and strict aliasing&lt;/li&gt;
&lt;li&gt;sigwaitinfo() race condition in test_eintr&lt;/li&gt;
&lt;li&gt;FreeBSD test_subprocess core dump&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous reports:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part1.html"&gt;My contributions to CPython during 2017 Q2 (part 1)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part2.html"&gt;My contributions to CPython …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;This is the third part of my contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q2 (april, may, june):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;Trick bug: Clang 4.0, dtoa and strict aliasing&lt;/li&gt;
&lt;li&gt;sigwaitinfo() race condition in test_eintr&lt;/li&gt;
&lt;li&gt;FreeBSD test_subprocess core dump&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous reports:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part1.html"&gt;My contributions to CPython during 2017 Q2 (part 1)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part2.html"&gt;My contributions to CPython during 2017 Q2 (part 2)&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next report:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part1.html"&gt;My contributions to CPython during 2017 Q3: Part 1&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="security"&gt;
&lt;h2&gt;Security&lt;/h2&gt;
&lt;div class="section" id="backport-fixes"&gt;
&lt;h3&gt;Backport fixes&lt;/h3&gt;
&lt;p&gt;I am trying to fix all known security fixes in the 6 maintained Python
branches: 2.7, 3.3, 3.4, 3.5, 3.6 and master.&lt;/p&gt;
&lt;p&gt;I created the &lt;a class="reference external" href="http://python-security.readthedocs.io/"&gt;python-security.readthedocs.io&lt;/a&gt; website to track these
vulnerabilities, especially which Python versions are fixed, to identifiy
missing backports.&lt;/p&gt;
&lt;p&gt;Python 2.7, 3.5, 3.6 and master are quite good, I am still working on
backporting fixes into 3.4 and 3.3. Larry Hastings merged my 3.4 backports and
other security fixes, and scheduled a new 3.4.7 release next weeks. Later, I
will try to fix Python 3.3 as well, before its end-of-life, scheduled for the
end of september.&lt;/p&gt;
&lt;p&gt;See the &lt;a class="reference external" href="https://docs.python.org/devguide/#status-of-python-branches"&gt;Status of Python branches&lt;/a&gt; in the
devguide.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="libexpat-2-2"&gt;
&lt;h3&gt;libexpat 2.2&lt;/h3&gt;
&lt;p&gt;Python embeds a copy of libexpat to ease Python compilation on Windows and
macOS. It means that we have to remind to upgrade it at each libexpat release.
It is especially important when security vulerabilities are fixed in libexpat.&lt;/p&gt;
&lt;p&gt;libexpat 2.2 was released at 2016-06-21 and it contains such fixes for
vulnerabilities, see: &lt;a class="reference external" href="http://python-security.readthedocs.io/vuln/cve-2016-0718_expat_2.2_bug_537.html"&gt;CVE-2016-0718: expat 2.2, bug #537&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Sadly, it took us a few months to upgrade libexpat. I wrote a short shell
script to easily upgrade libexpat: recreate the &lt;tt class="docutils literal"&gt;Modules/expat/&lt;/tt&gt; directory
from a libexpat tarball.&lt;/p&gt;
&lt;p&gt;My commit:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;bpo-29591: Upgrade Modules/expat to libexpat 2.2 (#2164)&lt;/p&gt;
&lt;p&gt;Remove the configuration (&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;Modules/expat/*config.h&lt;/span&gt;&lt;/tt&gt;) of unsupported
platforms: Amiga, MacOS Classic on PPC32, Open Watcom.&lt;/p&gt;
&lt;p&gt;Remove XML_HAS_SET_HASH_SALT define: it became useless since our local
expat copy was upgrade to expat 2.1 (it's now expat 2.2.0).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I upgraded libexpat to 2.2 in Pytohn 2.7, 3.4, 3.5, 3.6 and master branches.
I still have a pending pull request for 3.3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="libexpat-2-2-1"&gt;
&lt;h3&gt;libexpat 2.2.1&lt;/h3&gt;
&lt;p&gt;Just after I finally upgraded our libexpat copy to 2.2.0... libexpat 2.2.1 was
released with new security fixes!  See &lt;a class="reference external" href="http://python-security.readthedocs.io/vuln/cve-2017-9233_expat_2.2.1.html"&gt;CVE-2017-9233: Expat 2.2.1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Again, I upgraded libexpat to 2.2.1 in all branches (pending: 3.3), see
bpo-30694. My commit:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Upgrade expat copy from 2.2.0 to 2.2.1 to get fixes
of multiple security vulnerabilities including:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;CVE-2017-9233 (External entity infinite loop DoS),&lt;/li&gt;
&lt;li&gt;CVE-2016-9063 (Integer overflow, re-fix),&lt;/li&gt;
&lt;li&gt;CVE-2016-0718 (Fix regression bugs from 2.2.0's fix to CVE-2016-0718)&lt;/li&gt;
&lt;li&gt;CVE-2012-0876 (Counter hash flooding with SipHash).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: the CVE-2016-5300 (Use os-specific entropy sources like getrandom)
doesn't impact Python, since Python already gets entropy from the OS to set
the expat secret using &lt;tt class="docutils literal"&gt;XML_SetHashSalt()&lt;/tt&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="urllib-splithost-vulnerability"&gt;
&lt;h3&gt;urllib splithost() vulnerability&lt;/h3&gt;
&lt;p&gt;Vulnerability: &lt;a class="reference external" href="http://python-security.readthedocs.io/vuln/bpo-30500_urllib_connects_to_a_wrong_host.html"&gt;bpo-30500: urllib connects to a wrong host&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While it was quick to confirm the vulnerability, it was tricky to decide how to
properly &lt;strong&gt;fix it without breaking backward compatibility&lt;/strong&gt;. We had too few
unit tests, and no obvious definition of the &lt;em&gt;expected&lt;/em&gt; behaviour. I
contributed to the discussed and to polish the fix:&lt;/p&gt;
&lt;p&gt;bpo-30500 commit:&lt;/p&gt;
&lt;blockquote&gt;
Fix urllib.parse.splithost() to correctly parse fragments. For example,
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;splithost('//127.0.0.1#&amp;#64;evil.com/')&lt;/span&gt;&lt;/tt&gt; now correctly returns the
&lt;tt class="docutils literal"&gt;127.0.0.1&lt;/tt&gt; host, instead of treating &lt;tt class="docutils literal"&gt;&amp;#64;evil.com&lt;/tt&gt; as the host in an
authentification (&lt;tt class="docutils literal"&gt;login&amp;#64;host&lt;/tt&gt;).&lt;/blockquote&gt;
&lt;p&gt;Fix applied to master, 3.6, 3.5, 3.4 and 2.7; pending pull request for 3.3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="travis-ci"&gt;
&lt;h3&gt;Travis CI&lt;/h3&gt;
&lt;p&gt;I also wrote a pull request to enable Travis CI and AppVeyor CI on Python 3.3
and 3.4 branches, to test security on CI. These changes are complex and not
merged yet, but I am now confident that the CI will be enabled on 3.4!&lt;/p&gt;
&lt;p&gt;My PR for Python 3.4: &lt;a class="reference external" href="https://github.com/python/cpython/pull/2475"&gt;[3.4] Backport CI config from master&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="tricky-bug-clang-4-0-dtoa-and-strict-aliasing"&gt;
&lt;h2&gt;Tricky bug: Clang 4.0, dtoa and strict aliasing&lt;/h2&gt;
&lt;p&gt;Aha, another funny story about compilers: bpo-30104.&lt;/p&gt;
&lt;p&gt;I noticed that the following tests started to fail on the &amp;quot;AMD64 FreeBSD
CURRENT Debug 3.x&amp;quot; buildbot:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;test_cmath&lt;/li&gt;
&lt;li&gt;test_float&lt;/li&gt;
&lt;li&gt;test_json&lt;/li&gt;
&lt;li&gt;test_marshal&lt;/li&gt;
&lt;li&gt;test_math&lt;/li&gt;
&lt;li&gt;test_statistics&lt;/li&gt;
&lt;li&gt;test_strtod&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;First, I bet on a libc change on FreeBSD. Then, I found that test_strtod fails
on FreeBSD using clang 4.0, but pass on FreeBSD using clang 3.8.&lt;/p&gt;
&lt;p&gt;I started to bisect the code on Linux using a subset of &lt;tt class="docutils literal"&gt;Python/dtoa.c&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Start (integrated in CPython code base): 2,876 lines&lt;/li&gt;
&lt;li&gt;dtoa2.c (standalone): 2,865 lines&lt;/li&gt;
&lt;li&gt;dtoa5.c: 50 lines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Extract of dtoa5.c:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
typedef union { double d; uint32_t L[2]; } U;

struct Bigint { int wds; };

static double
ratio(struct Bigint *a)
{
    U da, db;
    int k, ka, kb;
    double r;

    da.d = 1.682;
    ka = 6;
    db.d = 1.0;
    kb = 5;
    k = ka - kb + 32 * (a-&amp;gt;wds - 12);
    printf(&amp;quot;k=%i\n&amp;quot;, k);

    if (k &amp;gt; 0)
        da.L[1] += k * 0x100000;
    else {
        k = -k;
        db.L[1] += k * 0x100000;
    }
    r = da.d / db.d;
    /* r == 3.364 */
    return r;
}
&lt;/pre&gt;
&lt;p&gt;Even if I had a very short C code (50 lines) reproducing the bug, I was still
unable to understand the bug. I read many articles about aliasing, and I still
don't understand fully the bug... I suggest you these two good articles:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://cellperformance.beyond3d.com/articles/2006/06/understanding-strict-aliasing.html"&gt;Understanding Strict Aliasing&lt;/a&gt;
(Mike Acton, June 1, 2006)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://cellperformance.beyond3d.com/articles/2006/05/demystifying-the-restrict-keyword.html"&gt;Demystifying The Restrict Keyword&lt;/a&gt;
(Mike Acton, May 29, 2006)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Anyway, I wanted to report the bug to clang (LLVM), but the LLVM bug tracker was
migrating and I was unable to subscribe to get an account!&lt;/p&gt;
&lt;p&gt;In the meanwhile, &lt;strong&gt;Dimitry Andric&lt;/strong&gt;, a FreeBSD developer, told me that he got
&lt;em&gt;exactly&lt;/em&gt; the same clang 4.0 issue with &amp;quot;dtoa.c&amp;quot; in the &lt;em&gt;julia&lt;/em&gt; programming
language. Two months before I saw the same bug, he already reported the bug to
FreeBSD: &lt;a class="reference external" href="https://bugs.freebsd.org/216770"&gt;lang/julia: fails to build with clang 4.0&lt;/a&gt;, and to clang: &lt;a class="reference external" href="https://bugs.llvm.org//show_bug.cgi?id=31928"&gt;After r280351: if/else
blocks incorrectly optimized away?&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &amp;quot;problem&amp;quot; is that clang
developers disagree that it's a bug. In short, the discussion was around the C
standard: does clang respect C aliasing rules or not? At the end, clang
developers consider that they are right to optimize. To summarize:&lt;/p&gt;
&lt;blockquote&gt;
It's a bug in the code, not in the compiler&lt;/blockquote&gt;
&lt;p&gt;So I made a first change to use the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fno-strict-aliasing&lt;/span&gt;&lt;/tt&gt; flag when Python
is compiled with clang:&lt;/p&gt;
&lt;blockquote&gt;
Python/dtoa.c is not compiled correctly with clang 4.0 and
optimization level -O2 or higher, because of an aliasing issue on
the double/ULong[2] union.&lt;/blockquote&gt;
&lt;p&gt;But this change can make Python slower when compiled on clang, so I was asked
to only compile &lt;tt class="docutils literal"&gt;Python/dtoa.c&lt;/tt&gt; with this flag:&lt;/p&gt;
&lt;blockquote&gt;
On clang, only compile dtoa.c with -fno-strict-aliasing, use strict
aliasing to compile all other C files.&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="sigwaitinfo-race-condition-in-test-eintr"&gt;
&lt;h2&gt;sigwaitinfo() race condition in test_eintr&lt;/h2&gt;
&lt;div class="section" id="the-tricky-test-eintr"&gt;
&lt;h3&gt;The tricky test_eintr&lt;/h3&gt;
&lt;p&gt;When I wrote and implemented the &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0475/"&gt;PEP 475, Retry system calls failing with
EINTR&lt;/a&gt;, I didn't expect so many
annoying bugs of the newly written &lt;tt class="docutils literal"&gt;test_eintr&lt;/tt&gt; unit test. This test calls
system calls while sending signals every 100 ms. Usually the test tries to
block on a system call during at least 200 ms, to make sure that the syscall
was interrupted at least once by a signal, to check that Python correctly
retries the interrupted system call.&lt;/p&gt;
&lt;p&gt;Since the PEP was implemented, I already fixed many race conditions in
&lt;tt class="docutils literal"&gt;test_eintr&lt;/tt&gt;, but there was still a race condition on the &lt;tt class="docutils literal"&gt;sigwaitinfo()&lt;/tt&gt;
unit test. &lt;em&gt;Sometimes&lt;/em&gt; on a &lt;em&gt;few specific buildbots&lt;/em&gt; (FreeBSD), the test fails
randomly.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="first-attempt"&gt;
&lt;h3&gt;First attempt&lt;/h3&gt;
&lt;p&gt;My first attempt was the &lt;a class="reference external" href="http://bugs.python.org/issue25277"&gt;bpo-25277&lt;/a&gt;,
opened at 2015-09-30. I added faulthandler to dump tracebacks if a test hangs
longer than 10 minutes. Then I changed the sleep from 200 ms to 2 seconds in
the &lt;tt class="docutils literal"&gt;sigwaitinfo()&lt;/tt&gt; test... just to make the bug less likely, but using a
longer sleep doesn't fix the root issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="second-attempt"&gt;
&lt;h3&gt;Second attempt&lt;/h3&gt;
&lt;p&gt;My second attempt was the &lt;a class="reference external" href="http://bugs.python.org/issue25868"&gt;bpo-25868&lt;/a&gt;,
opened at 2015-12-15. I added a pipe to &amp;quot;synchronize the parent and the child
processes&amp;quot;, to try to make the sigwaitinfo() test a little bit more reliable. I
also reduced the sleep from 2 seconds to 100 ms.&lt;/p&gt;
&lt;p&gt;7 minutes after my fix, &lt;strong&gt;Martin Panter&lt;/strong&gt; wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;With the pipe, there is still a potential race after the parent writes to
the pipe and before sigwaitinfo() is invoked, versus the child sleep()
call.&lt;/p&gt;
&lt;p&gt;What do you think of my suggestion to block the signal? Then (in theory) it
should be robust, rather than relying on timing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I replied that I wasn't sure that sigwaitinfo() EINTR error was still tested if
we make his proposed change.&lt;/p&gt;
&lt;p&gt;One month later, Martin wrote a patch but I was unable to take a decision on
his change. In september 2016, Martin noticed a new test failure on the FreeBSD
9 buildbot.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="third-attempt"&gt;
&lt;h3&gt;Third attempt&lt;/h3&gt;
&lt;p&gt;My third attempt is the bpo-30320, opened at 2017-05-09. This time, I really
wanted to fix &lt;em&gt;all&lt;/em&gt; buildbot random failures. Since I was now able to reproduce
the bug on my FreeBSD VM, I was able to write a fix but also to check that:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;sigwaitinfo() and sigtimedwait() fail with EINTR and Python automatically
restarts the interrupted syscall&lt;/li&gt;
&lt;li&gt;I hacked the test file to only run the sigwaitinfo() and sigtimedwait() unit
tests. Running the test in a loop doesn't fail: I ran the test during 5
minutes in 10 shells (tests running 10 times in parallel) =&amp;gt; no failure, the
race condition seems to be gone.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So I &lt;a class="reference external" href="https://github.com/python/cpython/commit/211a392cc15f9a7b1b8ce65d8f6c9f8237d1b77f"&gt;pushed my fix&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;bpo-30320: test_eintr now uses pthread_sigmask()&lt;/p&gt;
&lt;p&gt;Rewrite sigwaitinfo() and sigtimedwait() unit tests for EINTR using
pthread_sigmask() to fix a race condition between the child and the
parent process.&lt;/p&gt;
&lt;p&gt;Remove the pipe which was used as a weak workaround against the race
condition.&lt;/p&gt;
&lt;p&gt;sigtimedwait() is now tested with a child process sending a signal
instead of testing the timeout feature which is more unstable
(especially regarding to clock resolution depending on the platform).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To be honest, I wasn't really confident, when I pushed my fix, that blocking
the waited signal is the proper fix.&lt;/p&gt;
&lt;p&gt;So it took &lt;strong&gt;1 year and 8 months&lt;/strong&gt; to really find and fix the root bug.&lt;/p&gt;
&lt;p&gt;Sadly, while I was working on dozens of other bugs, I completely lost track of
Martin's patch, even if I opened the bpo-25868. Sorry Martin for forgotting to
review your patch! But when you wrote it, I was unable to test that
sigwaitinfo() was still failing with EINTR.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="freebsd-test-subprocess-core-dump"&gt;
&lt;h2&gt;FreeBSD test_subprocess core dump&lt;/h2&gt;
&lt;p&gt;bpo-30448: During one month, some FreeBSD buildbots was emitting this warning
which started to annoy me, since I was trying to fix &lt;em&gt;all&lt;/em&gt; buildbots warnings:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Warning -- files was modified by test_subprocess
  Before: []
  After:  ['python.core']
&lt;/pre&gt;
&lt;p&gt;I tried and failed to reproduce the warning on my FreeBSD 11 VM. I also asked a
friend to reproduce the bug, but he also failed. I was developping my
&lt;tt class="docutils literal"&gt;test.bisect&lt;/tt&gt; tool and I wanted to get access to a machine to reproduce the
bug!&lt;/p&gt;
&lt;p&gt;Later, &lt;strong&gt;Kubilay Kocak&lt;/strong&gt; aka &lt;em&gt;koobs&lt;/em&gt; gave me access to his FreeBSD buildbots
and in a few seconds with my new test.bisect tool, I identified that the
&lt;tt class="docutils literal"&gt;test_child_terminated_in_stopped_state()&lt;/tt&gt; test triggers a deliberate crash,
but doesn't disable core dump creation. The fix is simple, use
&lt;tt class="docutils literal"&gt;test.support.SuppressCrashReport&lt;/tt&gt; context manager. Thanks &lt;em&gt;koobs&lt;/em&gt; for the
access!&lt;/p&gt;
&lt;p&gt;Maybe only FreeBSD 10 and older dump a core on this specific test, not FreeBSD
11. I don't know why. The test is special, it tests a process which crashs
while being traced with &lt;tt class="docutils literal"&gt;ptrace()&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2017 Q2 (part 2)</title><link href="https://haypo.github.io/contrib-cpython-2017q2-part2.html" rel="alternate"></link><published>2017-07-13T16:30:00+02:00</published><updated>2017-07-13T16:30:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-07-13:/contrib-cpython-2017q2-part2.html</id><summary type="html">&lt;p&gt;This is the second part of my contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q2 (april, may, june):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Mentoring&lt;/li&gt;
&lt;li&gt;Reference and memory leaks&lt;/li&gt;
&lt;li&gt;Contributions&lt;/li&gt;
&lt;li&gt;Enhancements&lt;/li&gt;
&lt;li&gt;Bugfixes&lt;/li&gt;
&lt;li&gt;Stars of the CPython GitHub project&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part1.html"&gt;My contributions to CPython during 2017 Q2 (part 1)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part3.html"&gt;My contributions to CPython during 2017 Q2 …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is the second part of my contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q2 (april, may, june):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Mentoring&lt;/li&gt;
&lt;li&gt;Reference and memory leaks&lt;/li&gt;
&lt;li&gt;Contributions&lt;/li&gt;
&lt;li&gt;Enhancements&lt;/li&gt;
&lt;li&gt;Bugfixes&lt;/li&gt;
&lt;li&gt;Stars of the CPython GitHub project&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part1.html"&gt;My contributions to CPython during 2017 Q2 (part 1)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part3.html"&gt;My contributions to CPython during 2017 Q2 (part 3)&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="mentoring"&gt;
&lt;h2&gt;Mentoring&lt;/h2&gt;
&lt;p&gt;During this quarter, I tried to mark &amp;quot;easy&amp;quot; issues using a &amp;quot;[EASY]&amp;quot; tag in
their title and the &amp;quot;easy&amp;quot; or &amp;quot;easy C&amp;quot; keyword. I announced these issues on the
&lt;a class="reference external" href="https://www.python.org/dev/core-mentorship/"&gt;core-mentorship mailing list&lt;/a&gt;.
I asked core developers to not fix these easy issues, but rather explain how to
fix them. In each issue, I described how fix these issues.&lt;/p&gt;
&lt;p&gt;It was a success since all easy issues were fixed quickly, usually the PR was
merged in less than 24 hours after I created the issue!&lt;/p&gt;
&lt;p&gt;I mentored &lt;strong&gt;Stéphane Wirtel&lt;/strong&gt; and &lt;strong&gt;Louie Lu&lt;/strong&gt; to fix issues (easy or not).
During this quarter, Stéphane Wirtel got &lt;strong&gt;5 commits&lt;/strong&gt; merged into master (on a
&lt;strong&gt;total of 11 commits&lt;/strong&gt;), and Louie lu got &lt;strong&gt;6 commits&lt;/strong&gt; merged into master (on
a &lt;strong&gt;total of 10 commits&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;They helped me to fix reference leaks spotted by the new Refleaks buildbots.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="reference-and-memory-leaks"&gt;
&lt;h2&gt;Reference and memory leaks&lt;/h2&gt;
&lt;p&gt;Zachary Ware installed a Gentoo and a Windows buildbots running the Python test
suite with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--huntrleaks&lt;/span&gt;&lt;/tt&gt; to detect reference and memory leaks.&lt;/p&gt;
&lt;p&gt;I worked hard with others, especially Stéphane Wirtel and Louie Lu, to fix
&lt;em&gt;all&lt;/em&gt; reference leaks and memory leaks in Python 2.7, 3.5, 3.6 and master.
Right now, there is no more leaks on Windows! For Gentoo, the buildbot is
currently offline, but I am confident that all leaks also fixed.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;bpo-30598: _PySys_EndInit() now duplicates warnoptions. Fix a reference leak
in subinterpreters, like test_callbacks_leak() of test_atexit. warnoptions is
a list used to pass options from the command line to the sys module
constructor. Before this change, the list was shared by multiple interpreter
which is not the expected behaviour. Each interpreter should have their own
independent mutable world. This change duplicates the list in each
interpreter. So each interpreter owns its own list, so each interpreter can
clear its own list.&lt;/li&gt;
&lt;li&gt;bpo-30601: Fix a refleak in WindowsConsoleIO. Fix a reference leak in
_io._WindowsConsoleIO: PyUnicode_FSDecoder() always initialize decodedname
when it succeed and it doesn't clear input decodedname object.&lt;/li&gt;
&lt;li&gt;bpo-30599: Fix test_threaded_import reference leak. Mock
os.register_at_fork() when importing the random module, since this function
doesn't allow to unregister callbacks and so leaked memory.&lt;/li&gt;
&lt;li&gt;2.7: _tkinter: Fix refleak in getint(). PyNumber_Int() creates a new reference:
need to decrement result reference counter.&lt;/li&gt;
&lt;li&gt;bpo-30635: Fix refleak in test_c_locale_coercion. When checking for reference
leaks, test_c_locale_coercion is run multiple times and so
_LocaleCoercionTargetsTestCase.setUpClass() is called multiple times.
setUpClass() appends new value at each call, so it looks like a reference
leak. Moving the setup from setUpClass() to setUpModule() avoids this,
eliminating the false alarm.&lt;/li&gt;
&lt;li&gt;bpo-30602: Fix refleak in os.spawnve(). When os.spawnve() fails while
handling arguments, free correctly argvlist: pass lastarg+1 rather than
lastarg to free_string_array() to also free the first item.&lt;/li&gt;
&lt;li&gt;bpo-30602: Fix refleak in os.spawnv(). When os.spawnv() fails while handling
arguments, free correctly argvlist: pass lastarg+1 rather than lastarg to
free_string_array() to also free the first item.&lt;/li&gt;
&lt;li&gt;Fix ref cycles in TestCase.assertRaises(). bpo-23890:
unittest.TestCase.assertRaises() now manually breaks a reference cycle to not
keep objects alive longer than expected.&lt;/li&gt;
&lt;li&gt;Python 2.7: bpo-30675: Fix refleak hunting in regrtest. regrtest now warms up
caches: create explicitly all internal singletons which are created on demand
to prevent false positives when checking for reference leaks.&lt;/li&gt;
&lt;li&gt;_winconsoleio: Fix memory leak. Fix memory leak when _winconsoleio tries to
open a non-console file: free the name buffer.&lt;/li&gt;
&lt;li&gt;bpo-30813: Fix unittest when hunting refleaks. bpo-11798, bpo-16662,
bpo-16935, bpo-30813: Skip
test_discover_with_module_that_raises_SkipTest_on_import() and
test_discover_with_init_module_that_raises_SkipTest_on_import() of
test_unittest when hunting reference leaks using regrtest.&lt;/li&gt;
&lt;li&gt;bpo-30704, bpo-30604: Fix memleak in code_dealloc(): Free also
co_extra-&amp;gt;ce_extras, not only co_extra. XXX Serhiy rewrote the structure in
master to use a single memory block, implemented my idea.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="python-3-5-regrtest-fix"&gt;
&lt;h3&gt;Python 3.5 regrtest fix&lt;/h3&gt;
&lt;p&gt;bpo-30675, Fix the multiprocessing code in regrtest:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Rewrite code to pass &lt;tt class="docutils literal"&gt;slaveargs&lt;/tt&gt; from the master process to worker
processes: reuse the same code of the Python master branch.&lt;/li&gt;
&lt;li&gt;Move code to initialize tests in a new &lt;tt class="docutils literal"&gt;setup_tests()&lt;/tt&gt; function,
similar change was done in the master branch.&lt;/li&gt;
&lt;li&gt;In a worker process, call &lt;tt class="docutils literal"&gt;setup_tests()&lt;/tt&gt; with the namespace built
from &lt;tt class="docutils literal"&gt;slaveargs&lt;/tt&gt; to initialize correctly tests.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before this change, &lt;tt class="docutils literal"&gt;warm_caches()&lt;/tt&gt; was not called in worker processes
because the setup was done before rebuilding the namespace from &lt;tt class="docutils literal"&gt;slaveargs&lt;/tt&gt;.
As a consequence, the &lt;tt class="docutils literal"&gt;huntrleaks&lt;/tt&gt; feature was unstable. For example,
&lt;tt class="docutils literal"&gt;test_zipfile&lt;/tt&gt; reported randomly false positive on reference leaks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="false-positives"&gt;
&lt;h3&gt;False positives&lt;/h3&gt;
&lt;p&gt;bpo-30776: reduce regrtest -R false positives (#2422)&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Change the regrtest --huntrleaks checker to decide if a test file
leaks or not. Require that each run leaks at least 1 reference.&lt;/li&gt;
&lt;li&gt;Warmup runs are now completely ignored: ignored in the checker test
and not used anymore to compute the sum.&lt;/li&gt;
&lt;li&gt;Add an unit test for a reference leak.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example of reference differences previously considered a failure
(leak) and now considered as success (success, no leak):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[3, 0, 0]
[0, 1, 0]
[8, -8, 1]
&lt;/pre&gt;
&lt;p&gt;The same change was done to check for memory leaks.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="contributions"&gt;
&lt;h2&gt;Contributions&lt;/h2&gt;
&lt;p&gt;This quarter, I helped to merge two contributions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;bpo-9850: Deprecate the macpath module. Co-Authored-By: &lt;strong&gt;Chi Hsuan Yen&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;bpo-30595: Fix multiprocessing.Queue.get(timeout).
multiprocessing.Queue.get() with a timeout now polls its reader in
non-blocking mode if it succeeded to aquire the lock but the acquire took
longer than the timeout. Co-Authored-By: &lt;strong&gt;Grzegorz Grzywacz&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="enhancements"&gt;
&lt;h2&gt;Enhancements&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;bpo-30265: support.unlink() now only ignores ENOENT and ENOTDIR, instead of
ignoring all OSError exception.&lt;/li&gt;
&lt;li&gt;bpo-30054: Expose tracemalloc C API: make PyTraceMalloc_Track() and
PyTraceMalloc_Untrack() functions public. numpy is able to use
tracemalloc since numpy 1.13.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="bugfixes"&gt;
&lt;h2&gt;Bugfixes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;bpo-30125: On Windows, faulthandler.disable() now removes the exception
handler installed by faulthandler.enable().&lt;/li&gt;
&lt;li&gt;bpo-30284: Fix regrtest for out of tree build. Use a build/ directory in the
build directory, not in the source directory, since the source directory may
be read-only and must not be modified. Fallback on the source directory if
the build directory is not available (missing &amp;quot;abs_builddir&amp;quot; sysconfig
variable).&lt;/li&gt;
&lt;li&gt;test_locale now ignores the DeprecationWarning, don't fail anymore if test
run with &lt;tt class="docutils literal"&gt;python3 &lt;span class="pre"&gt;-Werror&lt;/span&gt;&lt;/tt&gt;. Fix also deprecation message: add a space.&lt;/li&gt;
&lt;li&gt;Fix a compiler warnings on AIX: only define get_zone() and get_gmtoff() if
needed.&lt;/li&gt;
&lt;li&gt;Fix a compiler warning in tmtotuple(): use the &lt;tt class="docutils literal"&gt;time_t&lt;/tt&gt; type for the
&lt;tt class="docutils literal"&gt;gmtoff&lt;/tt&gt; parameter.&lt;/li&gt;
&lt;li&gt;bpo-30264: ExpatParser closes the source on error. ExpatParser.parse() of
xml.sax.xmlreader now always closes the source: close the file object or the
urllib object if source is a string (not an open file-like object). The
change fixes a ResourceWarning on parsing error. Add
test_parse_close_source() unit test.&lt;/li&gt;
&lt;li&gt;Fix SyntaxWarning on importing test_inspect. Fix the following warning when
test_inspect.py is compiled to test_inspect.pyc:
&lt;tt class="docutils literal"&gt;SyntaxWarning: tuple parameter unpacking has been removed in 3.x&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;bpo-30418: On Windows, subprocess.Popen.communicate() now also ignore EINVAL
on stdin.write(): ignore also EINVAL if the child process is still running
but closed the pipe.&lt;/li&gt;
&lt;li&gt;bpo-30257: _bsddb: Fix newDBObject(). Don't set cursorSetReturnsNone to
DEFAULT_CURSOR_SET_RETURNS_NONE anymore if self-&amp;gt;myenvobj is set.
Fix a GCC warning on the strange indentation.&lt;/li&gt;
&lt;li&gt;bpo-30231: Remove skipped test_imaplib tests. The public cyrus.andrew.cmu.edu
IMAP server (port 993) doesn't accept TLS connection using our self-signed
x509 certificate. Remove the two tests which are already skipped. Write a new
test_certfile_arg_warn() unit test for the certfile deprecation warning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="stars-of-the-cpython-github-project"&gt;
&lt;h2&gt;Stars of the CPython GitHub project&lt;/h2&gt;
&lt;p&gt;At June 30, I wrote &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-June/148523.html"&gt;an email to python-dev&lt;/a&gt; about
&lt;a class="reference external" href="https://github.com/showcases/programming-languages"&gt;GitHub showcase of hosted programming languages&lt;/a&gt;: Python is only #11 with
8,539 stars, behind PHP and Ruby! I suggested to &amp;quot;like&amp;quot; (&amp;quot;star&amp;quot;?) the &lt;a class="reference external" href="https://github.com/python/cpython/"&gt;CPython
project on GitHub&lt;/a&gt; if you like the Python
programming language!&lt;/p&gt;
&lt;p&gt;Four days later, &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-July/148548.html"&gt;we got +2,389 new stars (8,539 =&amp;gt; 10,928)&lt;/a&gt;, thank
you! Python moved from the 11th place to the 9th, before Elixir and Julia.&lt;/p&gt;
&lt;p&gt;Ben Hoyt &lt;a class="reference external" href="https://www.reddit.com/r/Python/comments/6kg4w0/cpython_recently_moved_to_github_star_the_project/"&gt;posted it on reddit.com/r/Python&lt;/a&gt;,
where it got a bit of traction. Terry Jan Reedy also &lt;a class="reference external" href="https://mail.python.org/pipermail/python-list/2017-July/723476.html"&gt;posted it on python-list&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Screenshot at 2017-07-13 showing Ruby, PHP and CPython:&lt;/p&gt;
&lt;a class="reference external image-reference" href="https://github.com/showcases/programming-languages"&gt;&lt;img alt="GitHub showcase: Programming languages" src="https://haypo.github.io/images/github_cpython_stars.png" /&gt;&lt;/a&gt;
&lt;p&gt;CPython now has 11,512 stars, only 861 stars behind PHP ;-)&lt;/p&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2017 Q2 (part 1)</title><link href="https://haypo.github.io/contrib-cpython-2017q2-part1.html" rel="alternate"></link><published>2017-07-13T16:00:00+02:00</published><updated>2017-07-13T16:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-07-13:/contrib-cpython-2017q2-part1.html</id><summary type="html">&lt;p&gt;This is the first part of my contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q2 (april, may, june):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Buidbots and test.bisect&lt;/li&gt;
&lt;li&gt;Python 3.6.0 regression&lt;/li&gt;
&lt;li&gt;struct.Struct.format type&lt;/li&gt;
&lt;li&gt;Optimization: one less syscall per open() call&lt;/li&gt;
&lt;li&gt;make regen-all&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q1.html"&gt;My contributions to CPython during 2017 Q1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next reports …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is the first part of my contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q2 (april, may, june):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Buidbots and test.bisect&lt;/li&gt;
&lt;li&gt;Python 3.6.0 regression&lt;/li&gt;
&lt;li&gt;struct.Struct.format type&lt;/li&gt;
&lt;li&gt;Optimization: one less syscall per open() call&lt;/li&gt;
&lt;li&gt;make regen-all&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q1.html"&gt;My contributions to CPython during 2017 Q1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next reports:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part2.html"&gt;My contributions to CPython during 2017 Q2 (part 2)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part3.html"&gt;My contributions to CPython during 2017 Q2 (part 3)&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q3-part1.html"&gt;My contributions to CPython during 2017 Q3: Part 1&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next parts&lt;/p&gt;
&lt;div class="section" id="statistics"&gt;
&lt;h2&gt;Statistics&lt;/h2&gt;
&lt;pre class="literal-block"&gt;
# All branches
$ git log --after=2017-03-31 --before=2017-06-30 --reverse --branches='*' --author=Stinner &amp;gt; 2017Q2
$ grep '^commit ' 2017Q2|wc -l
222

# Master branch only
$ git log --after=2017-03-31 --before=2017-06-30 --reverse --author=Stinner origin/master|grep '^commit '|wc -l
85
&lt;/pre&gt;
&lt;p&gt;Statistics: &lt;strong&gt;85&lt;/strong&gt; commits in the master branch, a &lt;strong&gt;total of 222 commits&lt;/strong&gt;:
most (but not all) of the remaining 137 commits are cherry-picked backports to
2.7, 3.5 and 3.6 branches.&lt;/p&gt;
&lt;p&gt;Note: I didn't use &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--no-merges&lt;/span&gt;&lt;/tt&gt; since we don't use merge anymore, but &lt;tt class="docutils literal"&gt;git
&lt;span class="pre"&gt;cherry-pick&lt;/span&gt; &lt;span class="pre"&gt;-x&lt;/span&gt;&lt;/tt&gt;, to &lt;em&gt;backport&lt;/em&gt; fixes. Before GitHub, we used &lt;strong&gt;forwardport&lt;/strong&gt;
with Mercurial merges (ex: commit into 3.6, then merge into master).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="buildbots-and-test-bisect"&gt;
&lt;h2&gt;Buildbots and test.bisect&lt;/h2&gt;
&lt;p&gt;Since this article became way too long, I splitted it into sub-articles:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/python-test-bisect.html"&gt;New Python test.bisect tool&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/python-buildbots-2017q2.html"&gt;Work on Python buildbots, 2017 Q2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-6-0-regression"&gt;
&lt;h2&gt;Python 3.6.0 regression&lt;/h2&gt;
&lt;p&gt;I am ashamed, I introduced a tricky regression in Pyton 3.6.0 with my work on
FASTCALL optimizations :-( A special way to call C builtin functions was broken:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
from datetime import datetime
next(iter(datetime.now, None))
&lt;/pre&gt;
&lt;p&gt;This code raises a &lt;tt class="docutils literal"&gt;StopIteration&lt;/tt&gt; exception instead of formatting the
current date and time.&lt;/p&gt;
&lt;p&gt;It's even worse. I was aware of the bug, it was already fixed it in master, but
I just forgot to backport my fix: bpo-30524, fix _PyStack_UnpackDict().&lt;/p&gt;
&lt;p&gt;To prevent regressions, I wrote exhaustive unit tests on the 3 FASTCALL
functions, commit: &lt;a class="reference external" href="https://github.com/python/cpython/commit/3b5cf85edc188345668f987c824a2acb338a7816"&gt;bpo-30524: Write unit tests for FASTCALL&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="struct-struct-format-type"&gt;
&lt;h2&gt;struct.Struct.format type&lt;/h2&gt;
&lt;p&gt;Sometimes, fixing a bug can take longer than expected. In March 2014, &lt;strong&gt;Zbyszek
Jędrzejewski-Szmek&lt;/strong&gt; reported a bug on the &lt;tt class="docutils literal"&gt;format&lt;/tt&gt; attribute of the
&lt;tt class="docutils literal"&gt;struct.Struct&lt;/tt&gt; class: this attribute type is bytes, whereas a Unicode string
(str) was expected.&lt;/p&gt;
&lt;p&gt;I proposed to &amp;quot;just&amp;quot; change the attribute type in December 2014, but it was an
incompatible change which would break the backward compatibility. &lt;strong&gt;Martin
Panter&lt;/strong&gt; agreed and wrote a patch. &lt;strong&gt;Serhiy Storchaka&lt;/strong&gt; asked to discuss such
incompatible change on python-dev, but then nothing happened during longer
than...  2 years!&lt;/p&gt;
&lt;p&gt;In March 2017, I converted the old Martin's patch into a new GitHub pull
request. &lt;strong&gt;Serhiy&lt;/strong&gt; asked again to write to python-dev, so I wrote:
&lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-March/147688.html"&gt;Issue #21071: change struct.Struct.format type from bytes to str&lt;/a&gt;. And...
I got zero answer.&lt;/p&gt;
&lt;p&gt;Well, I didn't expect any, since it's a trivial change, and I don't expect that
anyone rely on the exact &lt;tt class="docutils literal"&gt;format&lt;/tt&gt; attribute type.  Moreover, the
&lt;tt class="docutils literal"&gt;struct.Struct&lt;/tt&gt; constructor already accepts bytes and str types. If the
attribute is passed to the constructor: it just works.&lt;/p&gt;
&lt;p&gt;In June 2017, Serhiy Storchaka replied to my email: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-June/148360.html"&gt;If nobody opposed to this
change it will be made in short time.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Since nobody replied, again, I just merged my pull request. So it took &lt;strong&gt;3
years and 3 months&lt;/strong&gt; to change the type of an uncommon attribute :-)&lt;/p&gt;
&lt;p&gt;Note: I never used this attribute... Before reading this issue, I didn't even
know that the &lt;tt class="docutils literal"&gt;struct&lt;/tt&gt; module has a &lt;tt class="docutils literal"&gt;struct.Struct&lt;/tt&gt; type...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="optimization-one-less-syscall-per-open-call"&gt;
&lt;h2&gt;Optimization: one less syscall per open() call&lt;/h2&gt;
&lt;p&gt;In bpo-30228, I modified FileIO.seek() and FileIO.tell() methods to now set the
internal seekable attribute to avoid one &lt;tt class="docutils literal"&gt;fstat()&lt;/tt&gt; syscall per Python open()
call in buffered or text mode.&lt;/p&gt;
&lt;p&gt;The seekable property is now also more reliable since its value is
set correctly on memory allocation failure.&lt;/p&gt;
&lt;p&gt;I still have a second pending pull request to remove one more &lt;tt class="docutils literal"&gt;fstat()&lt;/tt&gt;
syscall: &lt;a class="reference external" href="https://github.com/python/cpython/pull/1385"&gt;bpo-30228: TextIOWrapper uses abs_pos, not tell()&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="make-regen-all"&gt;
&lt;h2&gt;make regen-all&lt;/h2&gt;
&lt;p&gt;I started to look at bpo-23404, because the Python compilation failed on the
&amp;quot;AMD64 FreeBSD 9.x 3.x&amp;quot; buildbot when trying to regenerate the
&lt;tt class="docutils literal"&gt;Include/opcode.h&lt;/tt&gt; file.&lt;/p&gt;
&lt;div class="section" id="old-broken-make-touch"&gt;
&lt;h3&gt;Old broken make touch&lt;/h3&gt;
&lt;p&gt;We had a &lt;tt class="docutils literal"&gt;make touch&lt;/tt&gt; command to workaround this file timestamp issue, but
the command uses Mercurial, whereas Python migrated to Git last february. The
buildobt &amp;quot;touch&amp;quot; step was removed because &lt;tt class="docutils literal"&gt;make touch&lt;/tt&gt; was broken.&lt;/p&gt;
&lt;p&gt;I was always annoyed by the Makefile which wants to regenerate generated files
because of wrong file modification time, whereas the generated files were
already up to date.&lt;/p&gt;
&lt;p&gt;The bug annoyed me on OpenIndiana where &amp;quot;make touch&amp;quot; didn't work beause the
operating system only provides Python 2.6 and Mercurial didn't work on this
version.&lt;/p&gt;
&lt;p&gt;The bug also annoyed me on FreeBSD which has no &amp;quot;python&amp;quot; command, only
&amp;quot;python2.7&amp;quot;, and so required manual steps.&lt;/p&gt;
&lt;p&gt;The bug was also a pain point when trying to cross-compile Python.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="new-shiny-make-regen-all"&gt;
&lt;h3&gt;New shiny make regen-all&lt;/h3&gt;
&lt;p&gt;I decided to rewrite the Makefile to not regenerate generated files based on
the file modification time anymore. Instead, I added a new &lt;tt class="docutils literal"&gt;make &lt;span class="pre"&gt;regen-all&lt;/span&gt;&lt;/tt&gt;
command to regenerate explicitly all generated files. Basically, I replaced
&lt;tt class="docutils literal"&gt;make touch&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;make &lt;span class="pre"&gt;regen-all&lt;/span&gt;&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Add a new &lt;tt class="docutils literal"&gt;make &lt;span class="pre"&gt;regen-all&lt;/span&gt;&lt;/tt&gt; command to rebuild all generated files&lt;/li&gt;
&lt;li&gt;Add subcommands to only generate specific files:&lt;ul&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;regen-ast&lt;/span&gt;&lt;/tt&gt;: Include/Python-ast.h and Python/Python-ast.c&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;regen-grammar&lt;/span&gt;&lt;/tt&gt;: Include/graminit.h and Python/graminit.c&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;regen-importlib&lt;/span&gt;&lt;/tt&gt;: Python/importlib_external.h and Python/importlib.h&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;regen-opcode&lt;/span&gt;&lt;/tt&gt;: Include/opcode.h&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;regen-opcode-targets&lt;/span&gt;&lt;/tt&gt;: Python/opcode_targets.h&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;regen-typeslots&lt;/span&gt;&lt;/tt&gt;: Objects/typeslots.inc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Rename &lt;tt class="docutils literal"&gt;PYTHON_FOR_GEN&lt;/tt&gt; to &lt;tt class="docutils literal"&gt;PYTHON_FOR_REGEN&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;pgen is now only built by &lt;tt class="docutils literal"&gt;make &lt;span class="pre"&gt;regen-grammar&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Add &lt;tt class="docutils literal"&gt;$(srcdir)/&lt;/tt&gt; prefix to paths to source files to handle correctly
compilation outside the source directory&lt;/li&gt;
&lt;li&gt;Remove &lt;tt class="docutils literal"&gt;make touch&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;Tools/hg/hgtouch.py&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;.hgtouch&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: By default, &lt;tt class="docutils literal"&gt;$(PYTHON_FOR_REGEN)&lt;/tt&gt; is no more used nor needed by &amp;quot;make&amp;quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>Work on Python buildbots, 2017 Q2</title><link href="https://haypo.github.io/python-buildbots-2017q2.html" rel="alternate"></link><published>2017-07-13T09:00:00+02:00</published><updated>2017-07-13T09:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-07-13:/python-buildbots-2017q2.html</id><summary type="html">&lt;p&gt;I spent the last 6 months on working on buildbots: reduce the failure rate,
send email notitication on failure, fix random bugs, detect more bugs using
warnings, backport fixes to older branches, etc. I decided to fix &lt;em&gt;all&lt;/em&gt;
buildbots issues: fix all warnings and all unstable tests!&lt;/p&gt;
&lt;p&gt;The good news …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I spent the last 6 months on working on buildbots: reduce the failure rate,
send email notitication on failure, fix random bugs, detect more bugs using
warnings, backport fixes to older branches, etc. I decided to fix &lt;em&gt;all&lt;/em&gt;
buildbots issues: fix all warnings and all unstable tests!&lt;/p&gt;
&lt;p&gt;The good news is that I made great progress, I fixed most random failures. A
random fail now became the exception rather than the norm. Some issues were not
bugs in tests, but real race conditions in the code. It's always good to fix
unlikely race conditions before users hit them on production!&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Introduction: Python Buildbots&lt;/li&gt;
&lt;li&gt;Orange Is The New Color&lt;/li&gt;
&lt;li&gt;New buildbot-status Mailing List&lt;/li&gt;
&lt;li&gt;Hardware issues&lt;ul&gt;
&lt;li&gt;The vacuum cleaner&lt;/li&gt;
&lt;li&gt;The memory stick&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Warnings&lt;/li&gt;
&lt;li&gt;regrtest&lt;/li&gt;
&lt;li&gt;Bug fixes&lt;/li&gt;
&lt;li&gt;Python 2.7&lt;/li&gt;
&lt;li&gt;Buildbot reports to python-dev&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="introduction-python-buildbots"&gt;
&lt;h2&gt;Introduction: Python Buildbots&lt;/h2&gt;
&lt;p&gt;CPython is running a &lt;a class="reference external" href="https://buildbot.net/"&gt;Buildbot&lt;/a&gt; server for continuous
integration, but tests are run as post-commit: see &lt;a class="reference external" href="https://www.python.org/dev/buildbot/"&gt;Python buildbots&lt;/a&gt;. CPython is tested by a wide range of
buildbot slaves:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;6 operating systems:&lt;ul&gt;
&lt;li&gt;Linux (Debian, Ubuntu, Gentoo, RHEL, SLES)&lt;/li&gt;
&lt;li&gt;Windows (7, 8, 8.1 and 10)&lt;/li&gt;
&lt;li&gt;macOS (Tiger, El Capitain, Sierra)&lt;/li&gt;
&lt;li&gt;FreeBSD (9, 10, CURRENT)&lt;/li&gt;
&lt;li&gt;AIX&lt;/li&gt;
&lt;li&gt;OpenIndiana (currently offline)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;5 CPU architectures:&lt;ul&gt;
&lt;li&gt;ARMv7&lt;/li&gt;
&lt;li&gt;x86 (Intel 32 bit)&lt;/li&gt;
&lt;li&gt;x86-64 aka &amp;quot;AMD64&amp;quot; (Intel 64-bit)&lt;/li&gt;
&lt;li&gt;PPC64, PPC64LE&lt;/li&gt;
&lt;li&gt;s390x&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;3 C compilers:&lt;ul&gt;
&lt;li&gt;GCC&lt;/li&gt;
&lt;li&gt;Clang (FreeBSD, macOS)&lt;/li&gt;
&lt;li&gt;Visual Studio (Windows)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are different kinds of tests:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Python test suite: the most common check&lt;/li&gt;
&lt;li&gt;Docs: check that the documentation can be build and doesn't contain warnings&lt;/li&gt;
&lt;li&gt;Refleaks: check for reference leaks and memory leaks, run the Python test
suite with the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--huntrleaks&lt;/span&gt;&lt;/tt&gt; option&lt;/li&gt;
&lt;li&gt;DMG: Build the macOS installer with the
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;Mac/BuildScript/build-installer.py&lt;/span&gt;&lt;/tt&gt; script&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Python is tested in different configurations:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Debug: &lt;tt class="docutils literal"&gt;./configure &lt;span class="pre"&gt;--with-pydebug&lt;/span&gt;&lt;/tt&gt;, the most common configuration&lt;/li&gt;
&lt;li&gt;Non-debug: release mode, with compiler optimizations&lt;/li&gt;
&lt;li&gt;PGO: Profiled Guided Optimization, &lt;tt class="docutils literal"&gt;./configure &lt;span class="pre"&gt;--enable-optimizations&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Installed: &lt;tt class="docutils literal"&gt;./configure &lt;span class="pre"&gt;--prefix=XXX&lt;/span&gt; &amp;amp;&amp;amp; make install&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Shared library (libpython): &lt;tt class="docutils literal"&gt;./configure &lt;span class="pre"&gt;--enable-shared&lt;/span&gt;&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Currently, 4 branches are tested:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;master&lt;/tt&gt;: called &amp;quot;3.x&amp;quot; on buildbots&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;3.6&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;3.5&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;2.7&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also &lt;tt class="docutils literal"&gt;custom&lt;/tt&gt;, a special branch used by core developers for testing
patches.&lt;/p&gt;
&lt;p&gt;The buildbot configuration can be found in the &lt;a class="reference external" href="https://github.com/python/buildmaster-config/"&gt;buildmaster-config project&lt;/a&gt; (start with the
&lt;tt class="docutils literal"&gt;master/master.cfg&lt;/tt&gt; file).&lt;/p&gt;
&lt;p&gt;Note: Thanks to the migration to GitHub, Pull Requests are now tested on Linux,
Windows and macOS by Travis CI and AppVeyor. It's the first time in the CPython
development history that we have automated pre-commit tests!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="orange-is-the-new-color"&gt;
&lt;h2&gt;Orange Is The New Color&lt;/h2&gt;
&lt;p&gt;A buildbot now becomes orange when tests contain warnings.&lt;/p&gt;
&lt;p&gt;My first change was to modify the buildbot configuration to extract warnings
from the raw test output to create a new &amp;quot;warnings&amp;quot; report, to more easily
detect warnings and tests failing randomly (test fail then pass when re-run).&lt;/p&gt;
&lt;p&gt;Example of orange build, x86-64 El Capitain 3.x:&lt;/p&gt;
&lt;img alt="Buildbot: orange build" src="https://haypo.github.io/images/buildbot_orange.png" /&gt;
&lt;p&gt;Extract of the current &lt;tt class="docutils literal"&gt;master/custom/steps.py&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
class Test(BaseTest):
    # Regular expression used to catch warnings, errors and bugs
    warningPattern = (
        # regrtest saved_test_environment warning:
        # Warning -- files was modified by test_distutils
        # test.support &amp;#64;reap_threads:
        # Warning -- threading_cleanup() failed to cleanup ...
        r&amp;quot;Warning -- &amp;quot;,
        # Py_FatalError() call
        r&amp;quot;Fatal Python error:&amp;quot;,
        # PyErr_WriteUnraisable() exception: usually, error in
        # garbage collector or destructor
        r&amp;quot;Exception ignored in:&amp;quot;,
        # faulthandler_exc_handler(): Windows exception handler installed with
        # AddVectoredExceptionHandler() by faulthandler.enable()
        r&amp;quot;Windows fatal exception:&amp;quot;,
        # Resource warning: unclosed file, socket, etc.
        # NOTE: match the &amp;quot;ResourceWarning&amp;quot; anywhere, not only at the start
        r&amp;quot;ResourceWarning&amp;quot;,
        # regrtest: At least one test failed. Log a warning even if the test
        # passed on the second try, to notify that a test is unstable.
        r'Re-running failed tests in verbose mode',
        # Re-running test 'test_multiprocessing_fork' in verbose mode
        r'Re-running test .* in verbose mode',
        # Thread last resort exception handler in t_bootstrap()
        r'Unhandled exception in thread started by ',
        # test_os leaked [6, 6, 6] memory blocks, sum=18,
        r'test_[^ ]+ leaked ',
    )
    # Use &amp;quot;.*&amp;quot; prefix to search the regex anywhere since stdout is mixed
    # with stderr, so warnings are not always written at the start
    # of a line. The log consumer calls warningPattern.match(line)
    warningPattern = r&amp;quot;.*(?:%s)&amp;quot; % &amp;quot;|&amp;quot;.join(warningPattern)
    warningPattern = re.compile(warningPattern)

    # if tests have warnings, mark the overall build as WARNINGS (orange)
    warnOnWarnings = True
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="new-buildbot-status-mailing-list"&gt;
&lt;h2&gt;New buildbot-status Mailing List&lt;/h2&gt;
&lt;p&gt;To check buildbots, previously I had to analyze manually the huge &amp;quot;waterfall&amp;quot;
view of four Python branches: 2.7, 3.5, 3.6 and master (&amp;quot;3.x&amp;quot;).&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://buildbot.python.org/all/waterfall?category=3.x.stable&amp;amp;category=3.x.unstable"&gt;Python master (&amp;quot;3.x&amp;quot;)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://buildbot.python.org/all/waterfall?category=3.6.stable&amp;amp;category=3.6.unstable"&gt;Python 3.6&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://buildbot.python.org/all/waterfall?category=3.5.stable&amp;amp;category=3.5.unstable"&gt;Python 3.5&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://buildbot.python.org/all/waterfall?category=2.7.stable&amp;amp;category=2.7.unstable"&gt;Python 2.7&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example of typical buildbot waterfall:&lt;/p&gt;
&lt;a class="reference external image-reference" href="http://buildbot.python.org/all/waterfall?category=3.x.stable&amp;amp;category=3.x.unstable"&gt;&lt;img alt="Buildbot waterfall" src="https://haypo.github.io/images/buildbot_waterfall.png" /&gt;&lt;/a&gt;
&lt;p&gt;The screenshot is obviously truncated since the webpage is giant: I have to
scroll in all directions... It's not convenient to check the status of all
builds, detect random failures, etc.&lt;/p&gt;
&lt;p&gt;We also have an IRC bot reporting buildbot failures: when a green (success) or
orange (warning) buildbot becomes red (failure). I wanted to have the same
thing, but by email. Technically, it's trivial to enable email notification,
but I never did it because buildbots were simply too unstable: most failures
were not related to the newly tested changes.&lt;/p&gt;
&lt;p&gt;But I decided to fix &lt;em&gt;all&lt;/em&gt; buildbots issues, so I enabled email notification
(&lt;a class="reference external" href="https://bugs.python.org/issue30325"&gt;bpo-30325&lt;/a&gt;). Since May 2017,
buildbots are now sending notifications to a new &lt;a class="reference external" href="https://mail.python.org/mm3/mailman3/lists/buildbot-status.python.org/"&gt;buildbot-status mailing list&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I use the mailing list to check if the failure is known or not: I try to answer
to all failure notification emails. If the failure is known, I copy the link to
the issue. Otherwise, I create a new issue and then copy the link to the new
issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="hardware-issues"&gt;
&lt;h2&gt;Hardware issues&lt;/h2&gt;
&lt;p&gt;Unit tests versus real life :-) (or &amp;quot;software versus hardware&amp;quot;)&lt;/p&gt;
&lt;div class="section" id="the-vacuum-cleaner"&gt;
&lt;h3&gt;The vacuum cleaner&lt;/h3&gt;
&lt;p&gt;Fixing buildbot issues can be boring sometimes, so let's start with a funny
bug. At June 25, Nick Coghlan wrote to the &lt;a class="reference external" href="https://mail.python.org/mailman/listinfo/python-buildbots"&gt;python-buildbots&lt;/a&gt; mailing list:&lt;/p&gt;
&lt;blockquote&gt;
It looks like the FreeBSD buildbots had an outage a little while ago,
and the FreeBSD 10 one may need a nudge to get back online (the
FreeBSD Current one looks like it came back automatically).&lt;/blockquote&gt;
&lt;p&gt;The reason is unexpected :-) &lt;a class="reference external" href="https://mail.python.org/pipermail/python-buildbots/2017-June/000122.html"&gt;Kubilay Kocak, owner of the buildbot, answered&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
Vacuum cleaner tripped RCD pulling too much current from the same circuit
as heater was running on. Buildbot worker host on same circuit.&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="the-memory-stick"&gt;
&lt;h3&gt;The memory stick&lt;/h3&gt;
&lt;p&gt;I opened at least 50 issues to report random buildbot failures. In the middle
of these issues, you can find &lt;a class="reference external" href="http://bugs.python.org/issue30371"&gt;bpo-30371&lt;/a&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
http://buildbot.python.org/all/builders/AMD64%20Windows7%20SP1%203.x/builds/436/steps/test/logs/stdio

======================================================================
FAIL: test_long_lines (test.test_email.test_email.TestFeedParsers)
----------------------------------------------------------------------
Traceback (most recent call last):
  File &amp;quot;C:\buildbot.python.org\3.x.kloth-win64\build\lib\test\test_email\test_email.py&amp;quot;, line 3526, in test_long_lines
    self.assertEqual(m.get_payload(), 'x'*M*N)
AssertionError: 'xxxx[17103482 chars]xxxxxzxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx[2896464 chars]xxxx' != 'xxxx[17103482 chars]xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx[2896464 chars]xxxx'

Notice the &amp;quot;z&amp;quot; in &amp;quot;...xxxxxz...&amp;quot;.
&lt;/pre&gt;
&lt;p&gt;and:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
New fail, same buildbot:

======================================================================
FAIL: test_long_lines (test.test_email.test_email.TestFeedParsers)
----------------------------------------------------------------------
Traceback (most recent call last):
  File &amp;quot;C:\buildbot.python.org\3.x.kloth-win64\build\lib\test\test_email\test_email.py&amp;quot;, line 3534, in test_long_lines
    self.assertEqual(m.items(), [('a', ''), ('b', 'x'*M*N)])
AssertionError: Lists differ: [('a'[1845894 chars]xxxxxzxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx[18154072 chars]xx')] != [('a'[1845894 chars]xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx[18154072 chars]xx')]

First differing element 1:
('b',[1845882 chars]xxxxxzxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx[18154071 chars]xxx')
('b',[1845882 chars]xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx[18154071 chars]xxx')

  [('a', ''),
   ('b',


Don't click on http://buildbot.python.org/all/builders/AMD64%20Windows7%20SP1%203.x/builds/439/steps/test/logs/stdio
: the log contains lines of 2 MB which make my Firefox super slow :-)
&lt;/pre&gt;
&lt;p&gt;Jeremy Kloth, owner the buildbot, answered:&lt;/p&gt;
&lt;blockquote&gt;
Watch this space, but I'm pretty sure that it is (was) bad memory.&lt;/blockquote&gt;
&lt;p&gt;He fixed the issue:&lt;/p&gt;
&lt;blockquote&gt;
That's the real problem, I'm not &lt;em&gt;sure&lt;/em&gt; it's the memory, but it does have
the symptoms. And that is why my buildbot was down earlier, I was
attempting to determine the bad stick and replace it.&lt;/blockquote&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="warnings"&gt;
&lt;h2&gt;Warnings&lt;/h2&gt;
&lt;p&gt;To fix test warnings, I enhanced the test suite to report more information when
a warning is emitted and to ease detection of failures.&lt;/p&gt;
&lt;p&gt;A major change is the new &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--fail-env-changed&lt;/span&gt;&lt;/tt&gt; option I added to regrtest
(bpo-30764): make tests fail if the &amp;quot;environment&amp;quot; is changed. This option is
now used on buildbots, Travis CI and AppVeyor, but only for the &lt;em&gt;master&lt;/em&gt; branch
yet.&lt;/p&gt;
&lt;p&gt;Other changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The &amp;#64;reap_threads decorator and the threading_cleanup() function of
test.support now log a warning if they fail to clenaup threads. The log may
help to debug such other warning seen on the AMD64 FreeBSD CURRENT Non-Debug
3.x buildbot: &amp;quot;Warning -- threading._dangling was modified by test_logging&amp;quot;.&lt;/li&gt;
&lt;li&gt;threading_cleanup() failure marks test as ENV_CHANGED. If threading_cleanup()
fails to cleanup threads, set a a new support.environment_altered flag to
true, flag uses by save_env which is used by regrtest to check if a test
altered the environment. At the end, the test file fails with ENV_CHANGED
instead of SUCCESS, to report that it altered the environment.&lt;/li&gt;
&lt;li&gt;regrtest: always show before/after values of modified environment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I backported all these changes to the 2.7, 3.5 and 3.6 branches to make sure
that warnings are fixed in all maintained branches.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="regrtest"&gt;
&lt;h2&gt;regrtest&lt;/h2&gt;
&lt;p&gt;As usual, I spent time our specialized test runner, regrtest:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;bpo-30263: regrtest: log system load and the number of CPUs. I tried to find
a relationship between race conditions and the system load. I failed to
find any obvious correlation yet, but I still consider that the system load
is useful.&lt;/li&gt;
&lt;li&gt;bpo-27103: regrtest disables -W if -R (reference hunting) is used. Workaround
for a regrtest bug.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But the most complex task was to backport &lt;em&gt;all&lt;/em&gt; regrtest features and
enhancements from master to regrtest of 3.6, 3.5 and then 2.7 branches.&lt;/p&gt;
&lt;p&gt;In Python 3.6, I rewrote regrtest.py file to split it into smaller files a in
new Lib/test/libregrtest/ library, so it was painful to backport changes to 3.5
(bpo-30383) which still uses the single regrtest.py file.&lt;/p&gt;
&lt;p&gt;In Python 2.7 (bpo-30283), it is even worse. Lib/test/regrtest.py uses the old
&lt;tt class="docutils literal"&gt;getopt&lt;/tt&gt; module to parse the command line instead of the new &lt;tt class="docutils literal"&gt;argparse&lt;/tt&gt;
used in 3.5 and newer. But I succeeded to backport all features and
enhancements from master!&lt;/p&gt;
&lt;p&gt;Python 2.7, 3.5, 3.6 and master now have almost the same CLI for &lt;tt class="docutils literal"&gt;python &lt;span class="pre"&gt;-m&lt;/span&gt;
test&lt;/tt&gt;, almost the same features (except of one or two missing feature), and
should provide the same level of information on failures and warnings.&lt;/p&gt;
&lt;p&gt;By the way, the new &lt;tt class="docutils literal"&gt;test.bisect&lt;/tt&gt; tool is now also available in all these
branches. See my &lt;a class="reference external" href="https://haypo.github.io/python-test-bisect.html"&gt;New Python test.bisect tool&lt;/a&gt; article.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bug-fixes"&gt;
&lt;h2&gt;Bug fixes&lt;/h2&gt;
&lt;p&gt;As expected, the longest section here is the list of changes I wrote to fix all
buildbot failures and warnings:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;bpo-29972: Skip tests known to fail on AIX. See &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-April/147748.html"&gt;[Python-Dev] Fix or drop AIX
buildbot?&lt;/a&gt;
email.&lt;/li&gt;
&lt;li&gt;bpo-29925: Skip test_uuid1_safe() on OS X Tiger&lt;/li&gt;
&lt;li&gt;Fix and optimize test_asyncore.test_quick_connect(). Don't use addCleanup() in
test_quick_connect() because it keeps the Thread object alive and so
&amp;#64;reap_threads times out after 1 second. &amp;quot;./python -m test -v
test_asyncore -m test_quick_connect&amp;quot; now takes 185 ms, instead of 11 seconds.&lt;/li&gt;
&lt;li&gt;bpo-30106: Fix test_asyncore.test_quick_connect(). test_quick_connect() runs
a thread up to 50 seconds, whereas the socket is connected in 0.2 second and
then the thread is expected to end in less than 3 second. On Linux, the
thread ends quickly because select() seems to always return quickly. On
FreeBSD, sometimes select() fails with timeout and so the thread runs much
longer than expected. Fix the thread timeout to fix a race condition in the
test.&lt;/li&gt;
&lt;li&gt;bpo-30106: Fix tearDown() of test_asyncore. Call asyncore.close_all() with
ignore_all=True in the tearDown() method of the test_asyncore base test case.
It prevents keeping alive sockets in asyncore.socket_map if close()
fails with an unexpected error.&lt;/li&gt;
&lt;li&gt;bpo-30108: Restore sys.path in test_site. Add setUpModule() and
tearDownModule() functions to test_site to save/restore sys.path at the
module level to prevent warning if the user site directory is created, since
site.addsitedir() modifies sys.path.&lt;/li&gt;
&lt;li&gt;bpo-30107: test_io doesn't dump a core file on an expected crash anymore.
test_io has two unit tests which trigger a deadlock:
test_daemon_threads_shutdown_stdout_deadlock() and
test_daemon_threads_shutdown_stderr_deadlock(). These tests call
Py_FatalError() if the expected bug is triggered which calls abort(). Use
test.support.SuppressCrashReport to prevent the creation on a core dump, to
fix the warning:
&lt;tt class="docutils literal"&gt;Warning &lt;span class="pre"&gt;--&lt;/span&gt; files was modified by test_io &lt;span class="pre"&gt;(...)&lt;/span&gt; After: ['python.core']&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;bpo-30125: Disable faulthandler to run test_SEH() of test_ctypes to prevent
the following log with a traceback:
&lt;tt class="docutils literal"&gt;Windows fatal exception: access violation&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;bpo-30131: test_logging cleans up threads using &amp;#64;support.reap_threads.&lt;/li&gt;
&lt;li&gt;bpo-30132: BuildExtTestCase of test_distutils now uses support.temp_cwd() in
setUp() to remove files created in the current working directory by
BuildExtTestCase unit tests.&lt;/li&gt;
&lt;li&gt;bpo-30107: On macOS, test.support.SuppressCrashReport now redirects
/usr/bin/defaults command stderr into a pipe to not pollute stderr. It fixes
a test_io.test_daemon_threads_shutdown_stderr_deadlock() failure when the
CrashReporter domain doesn't exists.&lt;/li&gt;
&lt;li&gt;bpo-30175: Skip client cert tests of test_imaplib. The IMAP server
cyrus.andrew.cmu.edu doesn't accept our randomly generated client x509
certificate anymore.&lt;/li&gt;
&lt;li&gt;bpo-30175: test_nntplib fails randomly with EOFError in NetworkedNNTPTests.setUpClass():
catch EOFError to skip tests in that case.&lt;/li&gt;
&lt;li&gt;bpo-30199: AsyncoreEchoServer of test_ssl now calls
asyncore.close_all(ignore_all=True) to ensure that asyncore.socket_map is
cleared once the test completes, even if ConnectionHandler was not correctly
unregistered. Fix the following warning:
&lt;tt class="docutils literal"&gt;Warning &lt;span class="pre"&gt;--&lt;/span&gt; asyncore.socket_map was modified by test_ssl&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Fix test_ftplib warning if IPv6 is not available. DummyFTPServer now calls
del_channel() on bind() error to prevent the following warning in
TestIPv6Environment.setUpClass():
&lt;tt class="docutils literal"&gt;Warning &lt;span class="pre"&gt;--&lt;/span&gt; asyncore.socket_map was modified by test_ftplib&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;bpo-30329: Catch Windows error 10022 on shutdown(). Catch the Windows socket
WSAEINVAL error (code 10022) in imaplib and poplib on shutdown(SHUT_RDWR): An
invalid operation was attempted. This error occurs sometimes on SSL
connections.&lt;/li&gt;
&lt;li&gt;bpo-30357: test_thread now uses threading_cleanup(). test_thread: setUp() now
uses support.threading_setup() and support.threading_cleanup() to wait until
threads complete to avoid random side effects on following tests.
Co-Authored-By: &lt;strong&gt;Grzegorz Grzywacz&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;bpo-30339: test_multiprocessing_main_handling timeout.
test_multiprocessing_main_handling: increase the test_source timeout from 10
seconds to 60 seconds, since the test fails randomly on busy buildbots.
Sadly, this change wasn't enough to fix buildbots.&lt;/li&gt;
&lt;li&gt;bpo-30387: Fix warning in test_threading. test_is_alive_after_fork() now
joins directly the thread to avoid the following warning added by bpo-30357:
&amp;quot;Warning -- threading_cleanup() failed to cleanup 0 threads after 2 sec
(count: 0, dangling: 21)&amp;quot;. Use also a different exit code to catch generic
exit code 1.&lt;/li&gt;
&lt;li&gt;bpo-30649: On Windows, test_os now tolerates a delta of 50 ms instead of 20
ms in test_utime_current() and test_utime_current_old(). On other platforms,
reduce the delta from 20 ms to 10 ms. PPC64 Fedora 3.x buildbot requires at
least a delta of 14 ms.&lt;/li&gt;
&lt;li&gt;bpo-30595: test_queue_feeder_donot_stop_onexc() of _test_multiprocessing now
uses a timeout of 1 second on Queue.get(), instead of 0.1 second, for slow
buildbots.&lt;/li&gt;
&lt;li&gt;bpo-30764, bpo-29335: test_child_terminated_in_stopped_state() of
test_subprocess now uses support.SuppressCrashReport() to prevent the
creation of a core dump on FreeBSD.&lt;/li&gt;
&lt;li&gt;bpo-30280: TestBaseSelectorEventLoop of
test.test_asyncio.test_selector_events now correctly closes the event loop:
cleanup its executor to not leak threads: don't override the close() method
of the event loop, only override the_close_self_pipe() method. asyncio base
TestCase now uses threading_setup() and threading_cleanup() of test.support
to cleanup threads.&lt;/li&gt;
&lt;li&gt;bpo-26568, bpo-30812: Fix test_showwarnmsg_missing(): restore the attribute
after removing it.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;Python 2.7&lt;/h2&gt;
&lt;p&gt;I wanted to fix &lt;em&gt;all&lt;/em&gt; buildbot issues of &lt;em&gt;all&lt;/em&gt; branches including 2.7, whereas
I didn't touch much the Python 2.7 code base last months (last years???). The
first six months of 2017, I backported dozens of commits from master to 2.7!&lt;/p&gt;
&lt;p&gt;For example, I added AppVeyor on 2.7: a Windows CI for GitHub!&lt;/p&gt;
&lt;p&gt;On Windows we support multiple versions of Visual Studio. I use Visual Studio
2008, whereas most 2.7 Windows buildbots use Visual Studio 2010 or newer.  I
fixed sysconfig.is_python_build() if Python is built with Visual Studio 2008
(VS 9.0) (bpo-30342).&lt;/p&gt;
&lt;p&gt;Other Python 2.7 changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Fix &amp;quot;make tags&amp;quot; command.&lt;/li&gt;
&lt;li&gt;bpo-30764: support.SuppressCrashReport backported to 2.7 and &amp;quot;ported&amp;quot; to
Windows.  Add Windows support to test.support.SuppressCrashReport: call
SetErrorMode() and CrtSetReportMode(). _testcapi: add CrtSetReportMode() and
CrtSetReportFile() functions and CRT_xxx and CRTDBG_xxx constants needed by
SuppressCrashReport.&lt;/li&gt;
&lt;li&gt;bpo-30705: Fix test_regrtest.test_crashed(). Add test.support._crash_python()
which triggers a crash but uses test.support.SuppressCrashReport() to prevent
a crash report from popping up. Modify
test_child_terminated_in_stopped_state() of test_subprocess and
test_crashed() of test_regrtest to use _crash_python().&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I also backported many fixes wrote by other developers, including old fixes up
to 8 years old!&lt;/p&gt;
&lt;p&gt;Usually, &lt;strong&gt;finding&lt;/strong&gt; the proper fix takes much more time than the cherry-pick
itself which is usually straighforward (no conflict, nothing to do). I am
always impressed that Git is able to detect that a file was renamed between
Python 2 and Python 3, and applies cleanly the change!&lt;/p&gt;
&lt;p&gt;Example of backports from master to 2.7:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;bpo-6393: Fix locale.getprerredencoding() on macOS. Python crashes on OSX
when &lt;tt class="docutils literal"&gt;$LANG&lt;/tt&gt; is set to some (but not all) invalid values due to an invalid
result from nl_langinfo(). Fix written in &lt;strong&gt;September 2009&lt;/strong&gt; (8 years ago)!&lt;/li&gt;
&lt;li&gt;bpo-15526: test_startfile changes the cwd. Try to fix test_startfile's
inability to clean up after itself in time. Patch by &lt;strong&gt;Jeremy Kloth&lt;/strong&gt;.
Fix the following support.rmtree() error while trying to remove the temporary
working directory used by Python tests:
&amp;quot;WindowsError: [Error 32] The process cannot access the file because it is
being used by another process: ...&amp;quot;.
Original commit written in &lt;strong&gt;September 2012&lt;/strong&gt;!&lt;/li&gt;
&lt;li&gt;bpo-11790: Fix sporadic failures in
test_multiprocessing.WithProcessesTestCondition.
Fixed written in &lt;strong&gt;April 2011&lt;/strong&gt;. This backported commit was tricky to
identify!&lt;/li&gt;
&lt;li&gt;bpo-8799, fix test_threading: Reduce timing sensitivity of condition test by
explicitly.  delaying the main thread so that it doesn't race ahead of the
workers.  Fix written in &lt;strong&gt;Nov 2013&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;test_distutils: Use EnvironGuard on InstallTestCase, UtilTestCase, and
BuildExtTestCase  to prevent the following warning:
&lt;tt class="docutils literal"&gt;Warning &lt;span class="pre"&gt;--&lt;/span&gt; os.environ was modified by test_distutils&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Fix test_multprocessing: Relax test timing (bpo-29861) to avoid sporadic
failures.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="buildbot-reports-to-python-dev"&gt;
&lt;h2&gt;Buildbot reports to python-dev&lt;/h2&gt;
&lt;p&gt;I also wrote 3 reports to the Python-Dev mailing list:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;May 3: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-May/147838.html"&gt;Status of Python buildbots&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;June 8: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-June/148271.html"&gt;Buildbot report, june 2017&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;June 29: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-June/148511.html"&gt;Buildbot report (almost July)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>New Python test.bisect tool</title><link href="https://haypo.github.io/python-test-bisect.html" rel="alternate"></link><published>2017-07-12T15:00:00+02:00</published><updated>2017-07-12T15:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-07-12:/python-test-bisect.html</id><summary type="html">&lt;p&gt;This article tells the story of the new CPython &lt;tt class="docutils literal"&gt;test.bisect&lt;/tt&gt; tool to
identify failing tests in the CPython test suite.&lt;/p&gt;
&lt;div class="section" id="modify-manually-a-test-file"&gt;
&lt;h2&gt;Modify manually a test file&lt;/h2&gt;
&lt;p&gt;I am fixing reference leaks since many years. When the test file contains more
than 200 tests and is longer than 5,000 lines …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;This article tells the story of the new CPython &lt;tt class="docutils literal"&gt;test.bisect&lt;/tt&gt; tool to
identify failing tests in the CPython test suite.&lt;/p&gt;
&lt;div class="section" id="modify-manually-a-test-file"&gt;
&lt;h2&gt;Modify manually a test file&lt;/h2&gt;
&lt;p&gt;I am fixing reference leaks since many years. When the test file contains more
than 200 tests and is longer than 5,000 lines, it's just not possible to spot a
reference leak. Each time, I modified the long test file and actually &lt;em&gt;removes&lt;/em&gt;
enough code until the file becomes short enough so I can read it.&lt;/p&gt;
&lt;p&gt;This method &lt;em&gt;works&lt;/em&gt;, but it usually took me 20 to 30 minutes, and so it was
common that I made mistakes... and usually had to restart from the start...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="first-failed-attempt"&gt;
&lt;h2&gt;First failed attempt&lt;/h2&gt;
&lt;p&gt;In october 2014, while fixing &lt;a class="reference external" href="http://bugs.python.org/issue22588#msg228905"&gt;yet another reference leak in test_capi&lt;/a&gt;, &lt;strong&gt;Xavier de Gaye&lt;/strong&gt; was
surprised that I identified quickly the leak and wanted to want how I
proceeded. I explained my method removing code, but I also asked for a tool.&lt;/p&gt;
&lt;p&gt;Xavier created bpo-22607 at 2014-10-11 and wrote a patch based on an integer
range to run a subset of tests and did something special on the &lt;tt class="docutils literal"&gt;subTest()&lt;/tt&gt;
context manager. But &lt;strong&gt;Georg Brandl&lt;/strong&gt; wasn't convinced by this approach and...
I forgot this issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="new-design-list-tests-run-a-subset"&gt;
&lt;h2&gt;New design: list tests, run a subset&lt;/h2&gt;
&lt;p&gt;During this quarter, I had to fix dozens of reference leaks but also tests
failing with &amp;quot;environment changed&amp;quot;: one test method modified &amp;quot;something&amp;quot;. It
was really painful to identify each time the failing test.&lt;/p&gt;
&lt;p&gt;So I created bpo-29512 at 2017-02-09 to ask again the same tool. Technically, I
just wanted to run a subset of tests.&lt;/p&gt;
&lt;p&gt;While working on OpenStack, I enjoyed the &lt;tt class="docutils literal"&gt;testr&lt;/tt&gt; tool, a test runner able to
list tests and to run a subset of tests. &lt;tt class="docutils literal"&gt;testr&lt;/tt&gt; also provides a bisection
tool to identify a subset of tests enough to reproduce a bug. The subset can
contain more than a single test. Sometimes you need to run two tests
sequentially to trigger a specific bug, and it's usually long and boring to
identify manually these two tests.&lt;/p&gt;
&lt;p&gt;I proposed a similar design for my bisection tool. Start by listing all tests,
and then:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;create a pure &lt;em&gt;random&lt;/em&gt; sample of tests: subset with half the size of the
current test set&lt;/li&gt;
&lt;li&gt;If tests still fail, use the subset as the new set. Otherwise, throw the
subset.&lt;/li&gt;
&lt;li&gt;Loop until the subset is small enough or the process run longer than 100
iterations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="regrtest-list-cases"&gt;
&lt;h2&gt;regrtest --list-cases&lt;/h2&gt;
&lt;p&gt;To list tests, I created bpo-30523 and wrote a patch for the unittest module.
Modifying unittest didn't work well with doctests and the command line
interface (CLI) didn't work as I wanted. I proposed to modify regrtest instead
of unittest.&lt;/p&gt;
&lt;p&gt;I proposed to &lt;strong&gt;Louie Lu&lt;/strong&gt; to implement my new idea. I was impressed that he
implemented it so quickly and that it worked so well! I just asked him to not
exclude doctest test cases, since these test cases were working as expected!  I
quickly merged his modified patch which adds the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--list-cases&lt;/span&gt;&lt;/tt&gt; option to
regrtest.&lt;/p&gt;
&lt;p&gt;Note: regrtest already had a &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--list-tests&lt;/span&gt;&lt;/tt&gt; which lists test &lt;em&gt;files&lt;/em&gt;, whereas
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--list-cases&lt;/span&gt;&lt;/tt&gt; lists test &lt;em&gt;methods&lt;/em&gt; and doctests.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="regrtest-matchfile"&gt;
&lt;h2&gt;regrtest --matchfile&lt;/h2&gt;
&lt;p&gt;I created bpo-30540 to add a --matchfile option to regrtest. regrtest already
had a --match option, but it was only possible to use the option once, and I
wanted to use a text files for my list of tests.&lt;/p&gt;
&lt;p&gt;Again, I was surprised that it was so simple to implement the feature. By the
way, I modified regrtest --match to allow to specific the option multiple
times, to run multiple tests instead of a single one.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="new-test-bisect-tool"&gt;
&lt;h2&gt;New test.bisect tool&lt;/h2&gt;
&lt;p&gt;Since I had the two key features: &lt;tt class="docutils literal"&gt;regrtest &lt;span class="pre"&gt;--list-cases&lt;/span&gt;&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;regrtest
&lt;span class="pre"&gt;--matchfile&lt;/span&gt;&lt;/tt&gt;, it became trivial to implement the bisection tool. I wrote a
first prototype. The &amp;quot;prototype&amp;quot; worked much better than expected.&lt;/p&gt;
&lt;p&gt;My first version required a text file listing test cases. I modified it to run
automatically the new &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--list-cases&lt;/span&gt;&lt;/tt&gt; command.&lt;/p&gt;
&lt;p&gt;I extended the tool to not only track reference leaks, but also &amp;quot;environment
changed&amp;quot; failures like finding a test which creates a file but doesn't remove
it.&lt;/p&gt;
&lt;p&gt;I was asked to add this tool in the Python stdlib, so I added it as
&lt;tt class="docutils literal"&gt;Lib/test/bisect.py&lt;/tt&gt; to use it with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python3 -m test.bisect ...
&lt;/pre&gt;
&lt;p&gt;The test.bisect CLI is similar to the test CLI on purpose.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="reference-leak-example"&gt;
&lt;h2&gt;Reference leak example&lt;/h2&gt;
&lt;p&gt;I modified &lt;tt class="docutils literal"&gt;test_access()&lt;/tt&gt; of test_os to add manually a reference leak:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ ./python -m test -R 3:3 test_os
(...)
test_os leaked [1, 1, 1] references, sum=3
test_os leaked [1, 1, 1] memory blocks, sum=3
test_os failed in 33 sec
(...)
&lt;/pre&gt;
&lt;p&gt;Just replace &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-m&lt;/span&gt; test&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-m&lt;/span&gt; test.bisect&lt;/tt&gt; in the command, and you get
the guilty method:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ ./python -m test.bisect -R 3:3 test_os
Start bisection with 257 tests
Test arguments: -R 3:3 test_os
Bisection will stop when getting 1 or less tests (-n/--max-tests option), or after 100 iterations (-N/--max-iter option)

[+] Iteration 1: run 128 tests/257

+ /home/haypo/prog/python/master/python -m test --matchfile /tmp/tmpvbraed7h -R 3:3 test_os
(...)
Tests succeeded: skip this subtest, try a new subbset

[+] Iteration 2: run 128 tests/257

+ /home/haypo/prog/python/master/python -m test --matchfile /tmp/tmpcjqtzgfe -R 3:3 test_os
(...)
Tests failed: use this new subtest

[+] Iteration 3: run 64 tests/128
(...)
[+] Iteration 15: run 1 tests/2
(...)

Tests (1):
* test.test_os.FileTests.test_access

Bisection completed in 16 iterations and 0:03:10
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;test.bisect&lt;/tt&gt; command found the bug I introduced:
&lt;tt class="docutils literal"&gt;test.test_os.FileTests.test_access&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;The command takes a few minutes, but I don't care of its performance as soon as
its fully automated! If you use the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-o&lt;/span&gt; file&lt;/tt&gt; option, each time the tool is
able to reduce the size of the test set, it writes the new list of tests on
disk. So even if the tool crashs or fails to find a single failure test, it
already helps!&lt;/p&gt;
&lt;p&gt;I am now very happy that &lt;tt class="docutils literal"&gt;test.bisect&lt;/tt&gt; works better than I expected. So I
backported it to 2.7, 3.5, 3.6 and master branches, since I want to fix &lt;em&gt;all&lt;/em&gt;
buildbot failures on &lt;em&gt;all&lt;/em&gt; maintained branches.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="environment-changed-example"&gt;
&lt;h2&gt;Environment changed example&lt;/h2&gt;
&lt;p&gt;While running the previous example, I noticed the following warning:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Warning -- threading_cleanup() failed to cleanup 0 threads after 3 sec (count: 0, dangling: 2)
&lt;/pre&gt;
&lt;p&gt;Using the new &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--fail-env-changed&lt;/span&gt;&lt;/tt&gt; option, it is now posible to check which
test of test_os emits such warning:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
haypo&amp;#64;selma$ ./python -m test.bisect --fail-env-changed -R 3:3 test_os
(...)

Tests (1):
* test.test_os.TestSendfile.test_keywords

Bisection completed in 14 iterations and 0:03:27
&lt;/pre&gt;
&lt;p&gt;I never trust anything, so let's confirm the bug:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
haypo&amp;#64;selma$ ./python -m test --fail-env-changed -R 3:3 test_os -m test.test_os.TestSendfile.test_keywords
Run tests sequentially
0:00:00 load avg: 0.33 [1/1] test_os
Warning -- threading_cleanup() failed to cleanup 0 threads after 3 sec (count: 0, dangling: 2)
beginning 6 repetitions
123456
Warning -- threading_cleanup() failed to cleanup 0 threads after 3 sec (count: 0, dangling: 2)
.
Warning -- threading_cleanup() failed to cleanup 0 threads after 3 sec (count: 0, dangling: 2)
.Warning -- threading_cleanup() failed to cleanup 0 threads after 3 sec (count: 0, dangling: 2)
.Warning -- threading_cleanup() failed to cleanup 0 threads after 3 sec (count: 0, dangling: 2)
.Warning -- threading_cleanup() failed to cleanup 0 threads after 3 sec (count: 0, dangling: 2)
.Warning -- threading_cleanup() failed to cleanup 0 threads after 3 sec (count: 0, dangling: 2)
.
test_os failed (env changed)

1 test altered the execution environment:
    test_os

Total duration: 21 sec
Tests result: ENV CHANGED
&lt;/pre&gt;
&lt;p&gt;Ok right, there is something wrong with test_keywords(). I just opened
the &lt;a class="reference external" href="http://bugs.python.org/issue30908"&gt;bpo-30908&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="cpython"></category><category term="tests"></category></entry><entry><title>My contributions to CPython during 2017 Q1</title><link href="https://haypo.github.io/contrib-cpython-2017q1.html" rel="alternate"></link><published>2017-07-05T12:00:00+02:00</published><updated>2017-07-05T12:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-07-05:/contrib-cpython-2017q1.html</id><summary type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q1
(january, februrary, march):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;li&gt;Tricky bug&lt;/li&gt;
&lt;li&gt;FASTCALL optimizations&lt;/li&gt;
&lt;li&gt;Stack consumption&lt;/li&gt;
&lt;li&gt;Contributions&lt;/li&gt;
&lt;li&gt;os.urandom() and getrandom()&lt;/li&gt;
&lt;li&gt;Migration to GitHub&lt;/li&gt;
&lt;li&gt;Enhancements&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;regrtest&lt;/li&gt;
&lt;li&gt;Bugfixes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q4.html"&gt;My contributions to CPython during 2016 Q4&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part1.html"&gt;My contributions to
CPython during 2017 Q2 (part 1 …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2017 Q1
(january, februrary, march):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Statistics&lt;/li&gt;
&lt;li&gt;Optimization&lt;/li&gt;
&lt;li&gt;Tricky bug&lt;/li&gt;
&lt;li&gt;FASTCALL optimizations&lt;/li&gt;
&lt;li&gt;Stack consumption&lt;/li&gt;
&lt;li&gt;Contributions&lt;/li&gt;
&lt;li&gt;os.urandom() and getrandom()&lt;/li&gt;
&lt;li&gt;Migration to GitHub&lt;/li&gt;
&lt;li&gt;Enhancements&lt;/li&gt;
&lt;li&gt;Security&lt;/li&gt;
&lt;li&gt;regrtest&lt;/li&gt;
&lt;li&gt;Bugfixes&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q4.html"&gt;My contributions to CPython during 2016 Q4&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q2-part1.html"&gt;My contributions to
CPython during 2017 Q2 (part 1)&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="statistics"&gt;
&lt;h2&gt;Statistics&lt;/h2&gt;
&lt;pre class="literal-block"&gt;
# All commits
$ git log --after=2016-12-31 --before=2017-04-01 --reverse --branches='*' --author=Stinner &amp;gt; 2017Q1
$ grep '^commit ' 2017Q1|wc -l
121

# Exclude merges
$ git log --no-merges --after=2016-12-31 --before=2017-04-01 --reverse --branches='*' --author=Stinner|grep '^commit '|wc -l
105

# master branch (excluding merges)
$ git log --no-merges --after=2016-12-31 --before=2017-04-01 --reverse --author=Stinner origin/master|grep '^commit '|wc -l
98

# Only merges
$ git log --merges --after=2016-12-31 --before=2017-04-01 --reverse --branches='*' --author=Stinner|grep '^commit '|wc -l
16
&lt;/pre&gt;
&lt;p&gt;Statistics: &lt;strong&gt;98&lt;/strong&gt; commits in the master branch, 16 merge commits (done using
Mercurial before the migration to GitHub, and then converted to Git), and 7
other commits (likely backports), total: &lt;strong&gt;121&lt;/strong&gt; commits.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="optimization"&gt;
&lt;h2&gt;Optimization&lt;/h2&gt;
&lt;p&gt;With the work done in 2016 on FASTCALL, it became much easier to optimize code
by using the new FASTCALL API.&lt;/p&gt;
&lt;div class="section" id="python-slots"&gt;
&lt;h3&gt;Python slots&lt;/h3&gt;
&lt;p&gt;Issue #29507: I worked with &lt;strong&gt;INADA Naoki&lt;/strong&gt; to continue the work he did with
&lt;strong&gt;Yury Selivanov&lt;/strong&gt; on optimizing method calls. We optimized &amp;quot;slots&amp;quot; implemented
in Python. Slots is an internal optimization to call &amp;quot;dunder&amp;quot; methods like
&lt;tt class="docutils literal"&gt;__getitem__()&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;For Python methods, get the unbound Python function and prepend arguments with
&lt;em&gt;self&lt;/em&gt;, rather than calling the descriptor which creates a temporary
PyMethodObject.&lt;/p&gt;
&lt;p&gt;Add a new _PyObject_FastCall_Prepend() function used to call the unbound Python
method with &lt;em&gt;self&lt;/em&gt;. It avoids the creation of a temporary tuple to pass
positional arguments.&lt;/p&gt;
&lt;p&gt;Avoiding a temporary PyMethodObject and a temporary tuple makes Python slots up
to &lt;strong&gt;1.46x faster&lt;/strong&gt;. Microbenchmark on a &lt;tt class="docutils literal"&gt;__getitem__()&lt;/tt&gt; method implemented
in Python:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Median +- std dev: 121 ns +- 5 ns -&amp;gt; 82.8 ns +- 1.0 ns: 1.46x faster (-31%)
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="struct-module"&gt;
&lt;h3&gt;struct module&lt;/h3&gt;
&lt;p&gt;In the issue #29300, &lt;strong&gt;Serhiy Storchaka&lt;/strong&gt; and me converted most methods in the
C &lt;tt class="docutils literal"&gt;_struct&lt;/tt&gt; module to Argument Clinic to make them use the FASTCALL calling
convention. Using METH_FASTCALL avoids the creation of temporary tuple to pass
positional arguments and so is faster. For example, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;struct.pack(&amp;quot;i&amp;quot;,&lt;/span&gt; 1)&lt;/tt&gt;
becomes &lt;strong&gt;1.56x faster&lt;/strong&gt; (-36%):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ ./python -m perf timeit \
    -s 'import struct; pack=struct.pack' 'pack(&amp;quot;i&amp;quot;, 1)' \
    --compare-to=../default-ref/python
Median +- std dev: 119 ns +- 1 ns -&amp;gt; 76.8 ns +- 0.4 ns: 1.56x faster (-36%)
Significant (t=295.91)
&lt;/pre&gt;
&lt;p&gt;The difference is only &lt;tt class="docutils literal"&gt;42.2 ns&lt;/tt&gt;, but since the function only takes &lt;tt class="docutils literal"&gt;76.8
ns&lt;/tt&gt;, the difference is significant. The speedup can also be explained by more
efficient functions used to parse arguments. The new functions now use a cache
on the format string.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="deque-module"&gt;
&lt;h3&gt;deque module&lt;/h3&gt;
&lt;p&gt;Similar change in the deque module, I modified the index(), insert() and
rotate() methods to use METH_FASTCALL. Speedup:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;d.index(): &lt;strong&gt;1.24x faster&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;d.rotate(1): 1.24x faster&lt;/li&gt;
&lt;li&gt;d.insert(): 1.18x faster&lt;/li&gt;
&lt;li&gt;d.rotate(): 1.10x faster&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="tricky-bug"&gt;
&lt;h2&gt;Tricky bug&lt;/h2&gt;
&lt;div class="section" id="test-exceptions-test-unraisable"&gt;
&lt;h3&gt;test_exceptions.test_unraisable()&lt;/h3&gt;
&lt;p&gt;The optimization on Python slots (issue #29507) caused a regression in the
test_unraisable() unit test of test_exceptions.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;test_unraisable()&lt;/tt&gt; method expects that &lt;tt class="docutils literal"&gt;PyErr_WriteUnraisable(method)&lt;/tt&gt;
fails on &lt;tt class="docutils literal"&gt;repr(method)&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Before the change, &lt;tt class="docutils literal"&gt;slot_tp_finalize()&lt;/tt&gt; called
&lt;tt class="docutils literal"&gt;PyErr_WriteUnraisable()&lt;/tt&gt; with a PyMethodObject. In this case,
&lt;tt class="docutils literal"&gt;repr(method)&lt;/tt&gt; calls &lt;tt class="docutils literal"&gt;repr(self)&lt;/tt&gt; which is &lt;tt class="docutils literal"&gt;BrokenRepr.__repr__()&lt;/tt&gt; and
the calls raises a new exception.&lt;/p&gt;
&lt;p&gt;After the change, &lt;tt class="docutils literal"&gt;slot_tp_finalize()&lt;/tt&gt; uses an unbound method:
&lt;tt class="docutils literal"&gt;repr()&lt;/tt&gt; is called on a regular &lt;tt class="docutils literal"&gt;__del__()&lt;/tt&gt; method which doesn't call
&lt;tt class="docutils literal"&gt;repr(self)&lt;/tt&gt; and so &lt;tt class="docutils literal"&gt;repr()&lt;/tt&gt; doesn't fail anymore.&lt;/p&gt;
&lt;p&gt;The fix is to remove the BrokenRepr unit test, since
&lt;tt class="docutils literal"&gt;PyErr_WriteUnraisable()&lt;/tt&gt; doesn't call &lt;tt class="docutils literal"&gt;__repr__()&lt;/tt&gt; anymore.&lt;/p&gt;
&lt;p&gt;The removed test was really implementation specific, and my optimization
&amp;quot;fixed&amp;quot; the bug or &amp;quot;broke&amp;quot; the test. It's hard to say :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="unittest-assertraises-reference-cycle"&gt;
&lt;h3&gt;unittest assertRaises() reference cycle&lt;/h3&gt;
&lt;p&gt;At April 2015, &lt;strong&gt;Vjacheslav Fyodorov&lt;/strong&gt; reported a reference cycle in the
assertRaises() method of the unittest module: bpo-23890.&lt;/p&gt;
&lt;p&gt;When the context manager API of the &lt;tt class="docutils literal"&gt;assertRaises()&lt;/tt&gt; method is used, the
context manager returns an object which contains the exception. So the
exception is kept alive longer than usual.&lt;/p&gt;
&lt;p&gt;Python 3 exceptions now store traceback objects which contain local variables.
If a function stores the current exception in a local variable and the frame of
this function is part of the traceback, we get a reference cycle:&lt;/p&gt;
&lt;blockquote&gt;
exception -&amp;gt; traceback &amp;gt; frame -&amp;gt; variable -&amp;gt; exception&lt;/blockquote&gt;
&lt;p&gt;I fixed the reference cycle by manually clearing local variables. Example of
change of my commit:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
try:
    return context.handle('assertRaises', args, kwargs)
finally:
    # bpo-23890: manually break a reference cycle
    context = None
&lt;/pre&gt;
&lt;p&gt;It's not the first time that I fixed such reference cycle in the unit test
module. My previous fix was the issue #19880. Fix a reference leak in
unittest.TestCase. Explicitly break reference cycles between frames and the
&lt;tt class="docutils literal"&gt;_Outcome&lt;/tt&gt; instance: commit &lt;a class="reference external" href="https://github.com/python/cpython/commit/031bd532c48cf20a9cbf438bdae75dde49e36c51"&gt;031bd532&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="fastcall-optimizations"&gt;
&lt;h2&gt;FASTCALL optimizations&lt;/h2&gt;
&lt;p&gt;FASTCALL is my project to avoid temporary tuple to pass positional arguments
and avoid temporary dictionary to pass keyword arguments when calling a
function. It optimizes function calls in general.&lt;/p&gt;
&lt;p&gt;I continued work on FASTCALL to optimize code further and use FASTCALL in more
cases.&lt;/p&gt;
&lt;div class="section" id="recursion-depth"&gt;
&lt;h3&gt;Recursion depth&lt;/h3&gt;
&lt;p&gt;In the issue #29306, I fixed the usage of Py_EnterRecursiveCall() to account
correctly the recursion depth, to fix the code responsible to prevent C stack
overflow:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;*PyCFunction_*Call*()&lt;/span&gt;&lt;/tt&gt; functions now call &lt;tt class="docutils literal"&gt;Py_EnterRecursiveCall()&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;PyObject_Call()&lt;/tt&gt; now calls directly &lt;tt class="docutils literal"&gt;_PyFunction_FastCallDict()&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;PyCFunction_Call()&lt;/tt&gt; to avoid calling &lt;tt class="docutils literal"&gt;Py_EnterRecursiveCall()&lt;/tt&gt; twice per
function call&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="support-position-arguments"&gt;
&lt;h3&gt;Support position arguments&lt;/h3&gt;
&lt;p&gt;The issue #29286 enhanced Argument Clinic to use FASTCALL for functions which
only accept positional arguments:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Rename _PyArg_ParseStack to _PyArg_ParseStackAndKeywords&lt;/li&gt;
&lt;li&gt;Add _PyArg_ParseStack() helper function&lt;/li&gt;
&lt;li&gt;Add _PyArg_NoStackKeywords() helper function.&lt;/li&gt;
&lt;li&gt;Add _PyArg_UnpackStack() function helper&lt;/li&gt;
&lt;li&gt;Argument Clinic: Use METH_FASTCALL calling convention instead of METH_VARARGS
to parse position arguments and to parse &amp;quot;boring&amp;quot; position arguments.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="functions-converted-to-fastcall"&gt;
&lt;h3&gt;Functions converted to FASTCALL&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;_hashopenssl module&lt;/li&gt;
&lt;li&gt;collections.OrderedDict methods (some of them, not all)&lt;/li&gt;
&lt;li&gt;__build_class__(), getattr(), next() and sorted() builtin functions&lt;/li&gt;
&lt;li&gt;type_prepare() C function, used in type constructor&lt;/li&gt;
&lt;li&gt;dict.get() and dict.setdefault() now use Argument Clinic. The signature of
docstrings is also enhanced. For example, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;get(...)&lt;/span&gt;&lt;/tt&gt; becomes
&lt;tt class="docutils literal"&gt;get(self, key, default=None, /)&lt;/tt&gt;. Add also a note explaining why
dict_update() doesn't use METH_FASTCALL.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizations"&gt;
&lt;h3&gt;Optimizations&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #28839: Optimize function_call(), now simply calls
_PyFunction_FastCallDict() which is more efficient (fast paths for the common
case, optimized code object and no keyword argument).&lt;/li&gt;
&lt;li&gt;Issue #28839: Optimize _PyFunction_FastCallDict() when kwargs is an empty
dictionary, avoid the creation of an useless empty tuple.&lt;/li&gt;
&lt;li&gt;Issue #29259: Write fast path in _PyCFunction_FastCallKeywords() for
METH_FASTCALL, avoid the creation of a temporary dictionary for keyword
arguments.&lt;/li&gt;
&lt;li&gt;Issue #29259, #29263. methoddescr_call() creates a PyCFunction object, call
it and the destroy it. Add a new _PyMethodDef_RawFastCallDict() method to
avoid the temporary PyCFunction object.&lt;/li&gt;
&lt;li&gt;PyCFunction_Call() now calls _PyCFunction_FastCallDict()&lt;/li&gt;
&lt;li&gt;bpo-29735: Optimize partial_call(): avoid tuple. Add _PyObject_HasFastCall().
Fix also a performance regression in partial_call() if the callable doesn't
support FASTCALL.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="bugfixes"&gt;
&lt;h3&gt;Bugfixes&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #29286: _PyStack_UnpackDict() now returns -1 on error. Change
_PyStack_UnpackDict() prototype to be able to notify of failure when args is
NULL.&lt;/li&gt;
&lt;li&gt;Fix PyCFunction_Call() performance issue. Issue #29259, #29465:
PyCFunction_Call() doesn't create anymore a redundant tuple to pass
positional arguments for METH_VARARGS. Add a new cfunction_call()
subfunction.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="objects-call-c-file"&gt;
&lt;h3&gt;Objects/call.c file&lt;/h3&gt;
&lt;p&gt;The issue #29465 moved all C functions &amp;quot;calling functions&amp;quot; to a new
Objects/call.c file. Moving all functions at the same place should help to keep
the code consistent. It might also help the compiler to inline code more
easily, or maybe help to cache more machine code in CPU instruction cache.&lt;/p&gt;
&lt;p&gt;This change was made during the GitHub migration. Since the change is big
(modify many &lt;tt class="docutils literal"&gt;.c&lt;/tt&gt; files), I got many conflicts and it was annoying to rebase
it. I am not happy to get this &lt;tt class="docutils literal"&gt;call.c&lt;/tt&gt; file, it already helped me :-)&lt;/p&gt;
&lt;p&gt;Having &lt;tt class="docutils literal"&gt;call.c&lt;/tt&gt; also helps to keep helper functions need their callers, and
prevent to expose them in the C API, even if they are exposed as private
functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="don-t-optimize-keywords"&gt;
&lt;h3&gt;Don't optimize keywords&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Document that _PyFunction_FastCallDict() must copy kwargs. Issue #29318:
Caller and callee functions must not share the dictionary: kwargs must be
copied.&lt;/li&gt;
&lt;li&gt;Document why functools.partial() must copy kwargs. Add a comment to prevent
further attempts to avoid a copy for optimization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="stack-consumption"&gt;
&lt;h2&gt;Stack consumption&lt;/h2&gt;
&lt;p&gt;A FASTCALL micro-optimization was blocked by Serhiy Storchaka because it
increased the C stack consumption. In the past, I never analyzed the C stack
consumption. Since I wanted to get this micro-optimization merged, I tried to
reduce the consumption.&lt;/p&gt;
&lt;p&gt;At the beginning, I wrote a function to &lt;strong&gt;measure&lt;/strong&gt; the C stack consumption in
a reliable way. It took me a few iterations.&lt;/p&gt;
&lt;p&gt;Table showing the C stack consumption in bytes, and the difference compared to
Python 3.5 (last release before I started working on FASTCALL):&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="27%" /&gt;
&lt;col width="22%" /&gt;
&lt;col width="7%" /&gt;
&lt;col width="22%" /&gt;
&lt;col width="22%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Function&lt;/th&gt;
&lt;th class="head"&gt;2.7&lt;/th&gt;
&lt;th class="head"&gt;3.5&lt;/th&gt;
&lt;th class="head"&gt;3.6&lt;/th&gt;
&lt;th class="head"&gt;3.7&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;test_python_call&lt;/td&gt;
&lt;td&gt;1,360 (&lt;strong&gt;+352&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;1,008&lt;/td&gt;
&lt;td&gt;1,120 (&lt;strong&gt;+112&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;960 (&lt;strong&gt;-48&lt;/strong&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;test_python_getitem&lt;/td&gt;
&lt;td&gt;1,408 (&lt;strong&gt;+288&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;1,120&lt;/td&gt;
&lt;td&gt;1,168 (&lt;strong&gt;+48&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;880 (&lt;strong&gt;-240&lt;/strong&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;test_python_iterator&lt;/td&gt;
&lt;td&gt;1,424 (&lt;strong&gt;+192&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;1,232&lt;/td&gt;
&lt;td&gt;1,200 (&lt;strong&gt;-32&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;1,024 (&lt;strong&gt;-208&lt;/strong&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Total&lt;/td&gt;
&lt;td&gt;4,192 (&lt;strong&gt;+832&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;3,360&lt;/td&gt;
&lt;td&gt;3,488 (&lt;strong&gt;+128&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;2,864 (&lt;strong&gt;-496&lt;/strong&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Table showing the number of function calls before a stack overflow,
and the difference compared to Python 3.5:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="24%" /&gt;
&lt;col width="23%" /&gt;
&lt;col width="7%" /&gt;
&lt;col width="23%" /&gt;
&lt;col width="23%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Function&lt;/th&gt;
&lt;th class="head"&gt;2.7&lt;/th&gt;
&lt;th class="head"&gt;3.5&lt;/th&gt;
&lt;th class="head"&gt;3.6&lt;/th&gt;
&lt;th class="head"&gt;3.7&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;test_python_call&lt;/td&gt;
&lt;td&gt;6,161 (&lt;strong&gt;-2,153&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;8,314&lt;/td&gt;
&lt;td&gt;7,482 (&lt;strong&gt;-832&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;8,729 (&lt;strong&gt;+415&lt;/strong&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;test_python_getitem&lt;/td&gt;
&lt;td&gt;5,951 (&lt;strong&gt;-1,531&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;7,482&lt;/td&gt;
&lt;td&gt;7,174 (&lt;strong&gt;-308&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;9,522 (&lt;strong&gt;+2,040&lt;/strong&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;test_python_iterator&lt;/td&gt;
&lt;td&gt;5,885 (&lt;strong&gt;-916&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;6,801&lt;/td&gt;
&lt;td&gt;6,983 (&lt;strong&gt;+182&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;8,184 (&lt;strong&gt;+1,383&lt;/strong&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Total&lt;/td&gt;
&lt;td&gt;17,997 (&lt;strong&gt;-4600&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;22,597&lt;/td&gt;
&lt;td&gt;21,639 (&lt;strong&gt;-958&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;26,435 (&lt;strong&gt;+3,838&lt;/strong&gt;)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Python 3.7 is the best of 2.7, 3.5, 3.6 and 3.7: lowest stack consumption and
maximum number of calls (before a stack overflow) ;-)&lt;/p&gt;
&lt;p&gt;Changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;call_method() now uses _PyObject_FastCall(). Issue #29233: Replace the
inefficient _PyObject_VaCallFunctionObjArgs() with _PyObject_FastCall() in
call_method() and call_maybe().&lt;/li&gt;
&lt;li&gt;Issue #29227: Inline call_function() into _PyEval_EvalFrameDefault() using
Py_LOCAL_INLINE to reduce the stack consumption.&lt;/li&gt;
&lt;li&gt;Issue #29234: Inlining _PyStack_AsTuple() into callers increases their stack
consumption, Disable inlining to optimize the stack consumption. Add
_Py_NO_INLINE: use __attribute__((noinline)) of GCC and Clang.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="contributions"&gt;
&lt;h2&gt;Contributions&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #28961: Fix unittest.mock._Call helper: don't ignore the name parameter
anymore. Patch written by &lt;strong&gt;Jiajun Huang&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Prohibit implicit C function declarations. Issue #27659: use
-Werror=implicit-function-declaration when possible (GCC and Clang, but it
depends on the compiler version). Patch written by &lt;strong&gt;Chi Hsuan Yen&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="os-urandom-and-getrandom"&gt;
&lt;h2&gt;os.urandom() and getrandom()&lt;/h2&gt;
&lt;p&gt;As usual, I had fun with os.urandom() in this quarter (see my previous article
on urandom: &lt;a class="reference external" href="https://haypo.github.io/pep-524-os-urandom-blocking.html"&gt;PEP 524: os.urandom() now blocks on Linux in Python 3.6&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The glibc developers succeeded to implement a function getrandom() in glibc
2.25 (February 2017) to expose the &amp;quot;new&amp;quot; Linux getrandom() syscall which was
introduced in Linux 3.17 (August 2014). Read the LWN article: &lt;a class="reference external" href="https://lwn.net/Articles/711013/"&gt;The long road to
getrandom() in glibc&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I created the issue #29157 because my os.urandom() implementation wasn't ready
for the addition of a getrandom() function on Linux. My implementation using
the getrandom() function didn't handle the ENOSYS error (syscall not
supported), when Python is compiled on a recent kernel and glibc, but run on an
older kernel and glibc.&lt;/p&gt;
&lt;p&gt;I rewrote the code to prefer getrandom() over getentropy():&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;dev_urandom() now calls py_getentropy(). Prepare the fallback to support
getentropy() failure and falls back on reading from /dev/urandom.&lt;/li&gt;
&lt;li&gt;Simplify dev_urandom(). pyurandom() is now responsible to call getentropy()
or getrandom(). Enhance also dev_urandom() and pyurandom() documentation.&lt;/li&gt;
&lt;li&gt;getrandom() is now preferred over getentropy(). The glibc 2.24 now implements
getentropy() on Linux using the getrandom() syscall.  But getentropy()
doesn't support non-blocking mode. Since getrandom() is tried first, it's not
more needed to explicitly exclude getentropy() on Solaris. Replace:
&amp;quot;if defined(HAVE_GETENTROPY) &amp;amp;&amp;amp; !defined(sun)&amp;quot;
with &amp;quot;if defined(HAVE_GETENTROPY)&amp;quot;&lt;/li&gt;
&lt;li&gt;Enhance py_getrandom() documentation. py_getentropy() now supports ENOSYS,
EPERM &amp;amp; EINTR&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;IMHO the main enhancement was the documentation (comments) of the code. The
main function pyrandom() now has this long comment:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Read random bytes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Return 0 on success&lt;/li&gt;
&lt;li&gt;Raise an exception (if raise is non-zero) and return -1 on error&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Used sources of entropy ordered by preference, preferred source first:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;CryptGenRandom() on Windows&lt;/li&gt;
&lt;li&gt;getrandom() function (ex: Linux and Solaris): call py_getrandom()&lt;/li&gt;
&lt;li&gt;getentropy() function (ex: OpenBSD): call py_getentropy()&lt;/li&gt;
&lt;li&gt;/dev/urandom device&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Read from the /dev/urandom device if getrandom() or getentropy() function
is not available or does not work.&lt;/p&gt;
&lt;p&gt;Prefer getrandom() over getentropy() because getrandom() supports blocking
and non-blocking mode: see the PEP 524. Python requires non-blocking RNG at
startup to initialize its hash secret, but os.urandom() must block until the
system urandom is initialized (at least on Linux 3.17 and newer).&lt;/p&gt;
&lt;p&gt;Prefer getrandom() and getentropy() over reading directly /dev/urandom
because these functions don't need file descriptors and so avoid ENFILE or
EMFILE errors (too many open files): see the issue #18756.&lt;/p&gt;
&lt;p&gt;Only the getrandom() function supports non-blocking mode.&lt;/p&gt;
&lt;p&gt;Only use RNG running in the kernel. They are more secure because it is
harder to get the internal state of a RNG running in the kernel land than a
RNG running in the user land. The kernel has a direct access to the hardware
and has access to hardware RNG, they are used as entropy sources.&lt;/p&gt;
&lt;p&gt;Note: the OpenSSL RAND_pseudo_bytes() function does not automatically reseed
its RNG on fork(), two child processes (with the same pid) generate the same
random numbers: see issue #18747. Kernel RNGs don't have this issue,
they have access to good quality entropy sources.&lt;/p&gt;
&lt;p&gt;If raise is zero:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Don't raise an exception on error&lt;/li&gt;
&lt;li&gt;Don't call the Python signal handler (don't call PyErr_CheckSignals()) if
a function fails with EINTR: retry directly the interrupted function&lt;/li&gt;
&lt;li&gt;Don't release the GIL to call functions.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="migration-to-github"&gt;
&lt;h2&gt;Migration to GitHub&lt;/h2&gt;
&lt;p&gt;In February 2017, the Mercurial repository was converted to Git and the
development of CPython moved to GitHub at &lt;a class="reference external" href="https://github.com/python/cpython/"&gt;https://github.com/python/cpython/&lt;/a&gt;. I
helped to polish the migration in early days:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Rename README to README.rst and enhance formatting&lt;/li&gt;
&lt;li&gt;bpo-29527: Don't treat warnings as error in Travis docs job&lt;/li&gt;
&lt;li&gt;Travis CI: run rstlint.py in the docs job. Currently,
&lt;a class="reference external" href="http://buildbot.python.org/all/buildslaves/ware-docs"&gt;http://buildbot.python.org/all/buildslaves/ware-docs&lt;/a&gt; buildbot is only run as
post-commit. For example, bpo-29521 (PR#41) introduced two warnings,
unnotified by the Travis CI docs job. Modify the docs job to run
toosl/rstlint.py. Fix also the two minor warnings which causes the buildbot
slave to fail. Doc/Makefile: set PYTHON to python3.&lt;/li&gt;
&lt;li&gt;Add Travis CI and Codecov badges to README.&lt;/li&gt;
&lt;li&gt;Exclude myself from mention-bot. I made changes in almost all CPython files
last 5 years, so mention-bot asks me to review basically all pull requests. I
simply don't have the bandwidth to review everything, sorry! I prefer to
select myself which PR I want to follow.&lt;/li&gt;
&lt;li&gt;bpo-27425: Add .gitattributes, fix Windows tests. Mark binary files as binay
in .gitattributes to not translate newline characters in Git repositories on
Windows.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="enhancements"&gt;
&lt;h2&gt;Enhancements&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #29259: python-gdb.py now also looks for PyCFunction in the current
frame, not only in the older frame. python-gdb.py now also supports
method-wrapper (wrapperobject) objects (Issue #29367).&lt;/li&gt;
&lt;li&gt;Issue #26273: Document the new TCP_USER_TIMEOUT and TCP_CONGESTION constants&lt;/li&gt;
&lt;li&gt;bpo-29919: Remove unused imports found by pyflakes. Make also minor PEP8
coding style fixes on modified imports.&lt;/li&gt;
&lt;li&gt;bpo-29887: Test normalization now fails if download fails; fix also a
ResourceWarning.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="security"&gt;
&lt;h2&gt;Security&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Backport for Python 3.4. Issues #27850 and #27766: Remove 3DES from ssl
default cipher list and add ChaCha20 Poly1305. See the &lt;a class="reference external" href="http://python-security.readthedocs.io/vuln/cve-2016-2183_sweet32_attack_des_3des.html"&gt;CVE-2016-2183:
Sweet32 attack (DES, 3DES)&lt;/a&gt;
vulnerability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="regrtest"&gt;
&lt;h2&gt;regrtest&lt;/h2&gt;
&lt;p&gt;regrtest is the runner of the Python test suite. Changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;regrtest: don't fail immediately if a child does crash. Issue #29362: Catch a
crash of a worker process as a normal failure and continue to run next tests.
It allows to get the usual test summary: single line result (OK/FAIL), total
duration, etc.&lt;/li&gt;
&lt;li&gt;Fix regrtest -j0 -R output: write also dots into stderr, instead of stdout.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;Bugfixes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #29140: Fix hash(datetime.time). Fix time_hash() function: replace
DATE_xxx() macros with TIME_xxx() macros. Before, the hash function used a
wrong value for microseconds if fold is set (equal to 1).&lt;/li&gt;
&lt;li&gt;Issue #29174, #26741: Fix subprocess.Popen.__del__() on Python shutdown.
subprocess.Popen.__del__() now keeps a strong reference to warnings.warn()
function. The change allows to log the warning late at Python finalization.
Before the warning was ignored or logged an error instead of the warning.&lt;/li&gt;
&lt;li&gt;Issue #25591: Fix test_imaplib if the module ssl is missing.&lt;/li&gt;
&lt;li&gt;Fix script_helper.run_python_until_end(): copy the &lt;tt class="docutils literal"&gt;SYSTEMROOT&lt;/tt&gt; environment
variable.  Windows requires at least the SYSTEMROOT environment variable to
start Python. If run_python_until_end() doesn't copy SYSTEMROOT, the
function always fail on Windows.&lt;/li&gt;
&lt;li&gt;Fix datetime.fromtimestamp(): check bounds. Issue #29100: Fix
datetime.fromtimestamp() regression introduced in Python 3.6.0: check minimum
and maximum years.&lt;/li&gt;
&lt;li&gt;Fix test_datetime on system with 32-bit time_t. Issue #29100: Catch
OverflowError in the new test_timestamp_limits() test.&lt;/li&gt;
&lt;li&gt;Fix test_datetime on Windows. Issue #29100: On Windows,
datetime.datetime.fromtimestamp(min_ts) fails with an OSError in
test_timestamp_limits().&lt;/li&gt;
&lt;li&gt;bpo-29176: Fix the name of the _curses.window class. Set name to
&lt;tt class="docutils literal"&gt;_curses.window&lt;/tt&gt; instead of &lt;tt class="docutils literal"&gt;_curses.curses window&lt;/tt&gt; with a space!?&lt;/li&gt;
&lt;li&gt;bpo-29619: os.stat() and os.DirEntry.inodeo() now convert inode (st_ino)
using unsigned integers to support very large inodes (larger than 2^31).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>speed.python.org results: March 2017</title><link href="https://haypo.github.io/speed-python-org-march-2017.html" rel="alternate"></link><published>2017-03-29T00:40:00+02:00</published><updated>2017-03-29T00:40:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-03-29:/speed-python-org-march-2017.html</id><summary type="html">&lt;p&gt;In feburary 2017, CPython from Bitbucket with Mercurial moved to GitHub with
Git: read &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-February/147381.html"&gt;[Python-Dev] CPython is now on GitHub&lt;/a&gt; by
Brett Cannon.&lt;/p&gt;
&lt;p&gt;In 2016, I worked on speed.python.org to automate running benchmarks and make
benchmarks more stable. At the end, I had a single command to:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;tune …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;In feburary 2017, CPython from Bitbucket with Mercurial moved to GitHub with
Git: read &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2017-February/147381.html"&gt;[Python-Dev] CPython is now on GitHub&lt;/a&gt; by
Brett Cannon.&lt;/p&gt;
&lt;p&gt;In 2016, I worked on speed.python.org to automate running benchmarks and make
benchmarks more stable. At the end, I had a single command to:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;tune the system for benchmarks&lt;/li&gt;
&lt;li&gt;compile CPython using LTO+PGO&lt;/li&gt;
&lt;li&gt;install CPython&lt;/li&gt;
&lt;li&gt;install performance&lt;/li&gt;
&lt;li&gt;run performance&lt;/li&gt;
&lt;li&gt;upload results&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But my tools were written for Mercurial and speed.python.org uses Mercurial
revisions as keys for changes. Since the CPython repository was converted to
Git, I have to remove all old results and run again old benchmarks. But before
removing everyhing, I took screenshots of the most interesting pages. It would
prefer to keep a copy of all data, but it would require to write new tools
and I am not motivated to do that.&lt;/p&gt;
&lt;div class="section" id="python-3-7-compared-to-python-2-7"&gt;
&lt;h2&gt;Python 3.7 compared to Python 2.7&lt;/h2&gt;
&lt;p&gt;Benchmarks where Python 3.7 is &lt;strong&gt;faster&lt;/strong&gt; than Python 2.7:&lt;/p&gt;
&lt;img alt="python37_faster_py27" src="https://haypo.github.io/images/speed2017/python37_faster_py27.png" /&gt;
&lt;p&gt;Benchmarks where Python 3.7 is &lt;strong&gt;slower&lt;/strong&gt; than Python 2.7:&lt;/p&gt;
&lt;img alt="python37_slower_py27" src="https://haypo.github.io/images/speed2017/python37_slower_py27.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="significant-optimizations"&gt;
&lt;h2&gt;Significant optimizations&lt;/h2&gt;
&lt;p&gt;CPython became regulary faster in 2016 on the following benchmarks.&lt;/p&gt;
&lt;p&gt;call_method, the main optimized was &lt;a class="reference external" href="https://bugs.python.org/issue26110"&gt;Speedup method calls 1.2x&lt;/a&gt;:&lt;/p&gt;
&lt;img alt="call_method" src="https://haypo.github.io/images/speed2017/call_method.png" /&gt;
&lt;p&gt;float:&lt;/p&gt;
&lt;img alt="float" src="https://haypo.github.io/images/speed2017/float.png" /&gt;
&lt;p&gt;hexiom:&lt;/p&gt;
&lt;img alt="hexiom" src="https://haypo.github.io/images/speed2017/hexiom.png" /&gt;
&lt;p&gt;nqueens:&lt;/p&gt;
&lt;img alt="nqueens" src="https://haypo.github.io/images/speed2017/nqueens.png" /&gt;
&lt;p&gt;pickle_list, something happened near September 2016:&lt;/p&gt;
&lt;img alt="pickle_list" src="https://haypo.github.io/images/speed2017/pickle_list.png" /&gt;
&lt;p&gt;richards:&lt;/p&gt;
&lt;img alt="richards" src="https://haypo.github.io/images/speed2017/richards.png" /&gt;
&lt;p&gt;scimark_lu, I like the latest dot!&lt;/p&gt;
&lt;img alt="scimark_lu" src="https://haypo.github.io/images/speed2017/scimark_lu.png" /&gt;
&lt;p&gt;scimark_sor:&lt;/p&gt;
&lt;img alt="scimark_sor" src="https://haypo.github.io/images/speed2017/scimark_sor.png" /&gt;
&lt;p&gt;sympy_sum:&lt;/p&gt;
&lt;img alt="sympy_sum" src="https://haypo.github.io/images/speed2017/sympy_sum.png" /&gt;
&lt;p&gt;telco is one of the most impressive, it became regulary faster:&lt;/p&gt;
&lt;img alt="telco" src="https://haypo.github.io/images/speed2017/telco.png" /&gt;
&lt;p&gt;unpickle_list, something happened between March and May 2016:&lt;/p&gt;
&lt;img alt="unpickle_list" src="https://haypo.github.io/images/speed2017/unpickle_list.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="the-enum-change"&gt;
&lt;h2&gt;The enum change&lt;/h2&gt;
&lt;p&gt;One change related to the &lt;tt class="docutils literal"&gt;enum&lt;/tt&gt; module had significant impact on the two
following benchmarks.&lt;/p&gt;
&lt;p&gt;python_startup:&lt;/p&gt;
&lt;img alt="python_startup" src="https://haypo.github.io/images/speed2017/python_startup.png" /&gt;
&lt;p&gt;See &amp;quot;Python startup performance regression&amp;quot; section of &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q4.html"&gt;My contributions to
CPython during 2016 Q4&lt;/a&gt; for the
explanation on changes around September 2016.&lt;/p&gt;
&lt;p&gt;regex_compile became 1.2x slower (312 ms =&amp;gt; 376 ms: +20%) because constants
of the &lt;tt class="docutils literal"&gt;re&lt;/tt&gt; module became &lt;tt class="docutils literal"&gt;enum&lt;/tt&gt; objects: see &lt;a class="reference external" href="http://bugs.python.org/issue28082"&gt;convert re flags to (much
friendlier) IntFlag constants (issue #28082)&lt;/a&gt;.&lt;/p&gt;
&lt;img alt="regex_compile" src="https://haypo.github.io/images/speed2017/regex_compile.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="benchmarks-became-stable"&gt;
&lt;h2&gt;Benchmarks became stable&lt;/h2&gt;
&lt;p&gt;The following benchmarks are microbenchmarks which are impacted by many
external factors. It's hard to get stable results. I'm happy to see that
results are stable. I would say very stable compared to results when I started
to work on the project!&lt;/p&gt;
&lt;p&gt;call_simple:&lt;/p&gt;
&lt;img alt="call_simple" src="https://haypo.github.io/images/speed2017/call_simple.png" /&gt;
&lt;p&gt;spectral_norm:&lt;/p&gt;
&lt;img alt="spectral_norm" src="https://haypo.github.io/images/speed2017/spectral_norm.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="straight-line"&gt;
&lt;h2&gt;Straight line&lt;/h2&gt;
&lt;p&gt;It seems like no optimization had a significant impact on the following
benchmarks. You can also see that benchmarks became stable, so it's easier to
detect performance regression or significant optimization.&lt;/p&gt;
&lt;p&gt;dulwich_log:&lt;/p&gt;
&lt;img alt="dulwich_log" src="https://haypo.github.io/images/speed2017/dulwich_log.png" /&gt;
&lt;p&gt;pidigits:&lt;/p&gt;
&lt;img alt="pidigits" src="https://haypo.github.io/images/speed2017/pidigits.png" /&gt;
&lt;p&gt;sqlite_synth:&lt;/p&gt;
&lt;img alt="sqlite_synth" src="https://haypo.github.io/images/speed2017/sqlite_synth.png" /&gt;
&lt;p&gt;Apart something around April 2016, tornado_http result is stable:&lt;/p&gt;
&lt;img alt="tornado_http" src="https://haypo.github.io/images/speed2017/tornado_http.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="unstable-benchmarks"&gt;
&lt;h2&gt;Unstable benchmarks&lt;/h2&gt;
&lt;p&gt;After months of efforts to make everything stable, some benchmarks are still
unstable, even if temporary spikes are lower than before. See &lt;a class="reference external" href="https://haypo.github.io/analysis-python-performance-issue.html"&gt;Analysis of a
Python performance issue&lt;/a&gt;
to see the size of previous tempoary performance spikes.&lt;/p&gt;
&lt;p&gt;regex_v8:&lt;/p&gt;
&lt;img alt="regex_v8" src="https://haypo.github.io/images/speed2017/regex_v8.png" /&gt;
&lt;p&gt;scimark_sparse_mat_mult:&lt;/p&gt;
&lt;img alt="scimark_sparse_mat_mult" src="https://haypo.github.io/images/speed2017/scimark_sparse_mat_mult.png" /&gt;
&lt;p&gt;unpickle_pure_python:&lt;/p&gt;
&lt;img alt="unpickle_pure_python" src="https://haypo.github.io/images/speed2017/unpickle_pure_python.png" /&gt;
&lt;/div&gt;
&lt;div class="section" id="boring-results"&gt;
&lt;h2&gt;Boring results&lt;/h2&gt;
&lt;p&gt;There is nothing interesting to say on the following benchmark results.&lt;/p&gt;
&lt;p&gt;2to3:&lt;/p&gt;
&lt;img alt="2to3" src="https://haypo.github.io/images/speed2017/2to3.png" /&gt;
&lt;p&gt;crypto_pyaes:&lt;/p&gt;
&lt;img alt="crypto_pyaes" src="https://haypo.github.io/images/speed2017/crypto_pyaes.png" /&gt;
&lt;p&gt;deltablue:&lt;/p&gt;
&lt;img alt="deltablue" src="https://haypo.github.io/images/speed2017/deltablue.png" /&gt;
&lt;p&gt;logging_silent:&lt;/p&gt;
&lt;img alt="logging_silent" src="https://haypo.github.io/images/speed2017/logging_silent.png" /&gt;
&lt;p&gt;mako:&lt;/p&gt;
&lt;img alt="mako" src="https://haypo.github.io/images/speed2017/mako.png" /&gt;
&lt;p&gt;xml_etree_process:&lt;/p&gt;
&lt;img alt="xml_etree_process" src="https://haypo.github.io/images/speed2017/xml_etree_process.png" /&gt;
&lt;p&gt;xml_etre_iterparse:&lt;/p&gt;
&lt;img alt="xml_etre_iterparse" src="https://haypo.github.io/images/speed2017/xml_etre_iterparse.png" /&gt;
&lt;/div&gt;
</content><category term="benchmark"></category></entry><entry><title>FASTCALL issues</title><link href="https://haypo.github.io/fastcall-issues.html" rel="alternate"></link><published>2017-02-25T00:00:00+01:00</published><updated>2017-02-25T00:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-02-25:/fastcall-issues.html</id><summary type="html">&lt;p&gt;Here is the raw list of the 46 CPython issues I opended between 2016-04-21 and
2017-02-10 to implement my FASTCALL optimization. Most issues created in 2016
are already part of Python 3.6.0, some are already merged into the future
Python 3.7, the few remaining issues are still …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here is the raw list of the 46 CPython issues I opended between 2016-04-21 and
2017-02-10 to implement my FASTCALL optimization. Most issues created in 2016
are already part of Python 3.6.0, some are already merged into the future
Python 3.7, the few remaining issues are still open.&lt;/p&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;27 FASTCALL issues&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;2016-04-21: &lt;a class="reference external" href="http://bugs.python.org/issue26814"&gt;[WIP] Add a new _PyObject_FastCall() function which avoids the creation of a tuple or dict for arguments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-05-26: &lt;a class="reference external" href="http://bugs.python.org/issue27128"&gt;Add _PyObject_FastCall()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-08-20: &lt;a class="reference external" href="http://bugs.python.org/issue27809"&gt;Add _PyFunction_FastCallDict(): fast call with keyword arguments as a dict&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-08-20: &lt;a class="reference external" href="http://bugs.python.org/issue27810"&gt;Add METH_FASTCALL: new calling convention for C functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-08-22: &lt;a class="reference external" href="http://bugs.python.org/issue27830"&gt;Add _PyObject_FastCallKeywords(): avoid the creation of a temporary dictionary for keyword arguments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-08-23: &lt;a class="reference external" href="http://bugs.python.org/issue27840"&gt;functools.partial: don't copy keywoard arguments in partial_call()?&lt;/a&gt; [&lt;strong&gt;REJECTED&lt;/strong&gt;]&lt;/li&gt;
&lt;li&gt;2016-08-23: &lt;a class="reference external" href="http://bugs.python.org/issue27841"&gt;Use fast call in method_call() and slot_tp_new()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-08-23: &lt;a class="reference external" href="http://bugs.python.org/issue27845"&gt;Optimize update_keyword_args() function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-11-22: &lt;a class="reference external" href="http://bugs.python.org/issue28770"&gt;Update python-gdb.py for fastcalls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-11-30: &lt;a class="reference external" href="http://bugs.python.org/issue28839"&gt;_PyFunction_FastCallDict(): replace PyTuple_New() with PyMem_Malloc()&lt;/a&gt; [&lt;strong&gt;REJECTED&lt;/strong&gt;]&lt;/li&gt;
&lt;li&gt;2016-12-02: &lt;a class="reference external" href="http://bugs.python.org/issue28855"&gt;Compiler warnings in _PyObject_CallArg1()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-12-02: &lt;a class="reference external" href="http://bugs.python.org/issue28858"&gt;Fastcall uses more C stack&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-12-09: &lt;a class="reference external" href="http://bugs.python.org/issue28915"&gt;Modify PyObject_CallFunction() to use fast call internally&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-10: &lt;a class="reference external" href="http://bugs.python.org/issue29227"&gt;Reduce C stack consumption in function calls&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-10: &lt;a class="reference external" href="http://bugs.python.org/issue29233"&gt;call_method(): call _PyObject_FastCall() rather than _PyObject_VaCallFunctionObjArgs()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-11: &lt;a class="reference external" href="http://bugs.python.org/issue29234"&gt;Disable inlining of _PyStack_AsTuple() to reduce the stack consumption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-13: &lt;a class="reference external" href="http://bugs.python.org/issue29259"&gt;Add tp_fastcall to PyTypeObject: support FASTCALL calling convention for all callable objects&lt;/a&gt; [&lt;strong&gt;REJECTED&lt;/strong&gt;]&lt;/li&gt;
&lt;li&gt;2017-01-13: &lt;a class="reference external" href="http://bugs.python.org/issue29263"&gt;Implement LOAD_METHOD/CALL_METHOD for C functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-18: &lt;a class="reference external" href="http://bugs.python.org/issue29306"&gt;Check usage of Py_EnterRecursiveCall() and Py_LeaveRecursiveCall() in new FASTCALL functions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-19: &lt;a class="reference external" href="http://bugs.python.org/issue29318"&gt;Optimize _PyFunction_FastCallDict() for **kwargs&lt;/a&gt; [&lt;strong&gt;REJECTED&lt;/strong&gt;]&lt;/li&gt;
&lt;li&gt;2017-01-24: &lt;a class="reference external" href="http://bugs.python.org/issue29358"&gt;Add tp_fastnew and tp_fastinit to PyTypeObject, 15-20% faster object instanciation&lt;/a&gt; [&lt;strong&gt;REJECTED&lt;/strong&gt;]&lt;/li&gt;
&lt;li&gt;2017-01-24: &lt;a class="reference external" href="http://bugs.python.org/issue29360"&gt;_PyStack_AsDict(): Don't check if all keys are strings nor if keys are unique&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-25: &lt;a class="reference external" href="http://bugs.python.org/issue29367"&gt;python-gdb: display wrapper_call()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-02-05: &lt;a class="reference external" href="http://bugs.python.org/issue29451"&gt;Use _PyArg_Parser for _PyArg_ParseStack(): support positional only arguments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-02-06: &lt;a class="reference external" href="http://bugs.python.org/issue29465"&gt;Modify _PyObject_FastCall() to reduce stack consumption&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-02-09: &lt;a class="reference external" href="http://bugs.python.org/issue29507"&gt;Use FASTCALL in call_method() to avoid temporary tuple&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-02-10: &lt;a class="reference external" href="http://bugs.python.org/issue29524"&gt;Move functions to call objects into a new Objects/call.c file&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="issues-converting-functions-to-fastcall"&gt;
&lt;h2&gt;3 issues converting functions to FASTCALL&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;2017-01-16: &lt;a class="reference external" href="http://bugs.python.org/issue29286"&gt;Use METH_FASTCALL in str methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-18: &lt;a class="reference external" href="http://bugs.python.org/issue29312"&gt;Use FASTCALL in dict.update()&lt;/a&gt; [&lt;strong&gt;REJECTED&lt;/strong&gt;]&lt;/li&gt;
&lt;li&gt;2017-02-05: &lt;a class="reference external" href="http://bugs.python.org/issue29452"&gt;Use FASTCALL for collections.deque methods: index, insert, rotate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="argument-clinic-issues"&gt;
&lt;h2&gt;6 Argument Clinic issues&lt;/h2&gt;
&lt;p&gt;Converting code to Argument Clinic converts METH_VARARGS methods to
METH_FASTCALL.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;2017-01-16: &lt;a class="reference external" href="http://bugs.python.org/issue29289"&gt;Convert OrderedDict methods to Argument Clinic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-17: &lt;a class="reference external" href="http://bugs.python.org/issue29299"&gt;Argument Clinic: Fix signature of optional positional-only arguments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-17: &lt;a class="reference external" href="http://bugs.python.org/issue29300"&gt;Modify the _struct module to use FASTCALL and Argument Clinic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-17: &lt;a class="reference external" href="http://bugs.python.org/issue29301"&gt;decimal: Use FASTCALL and/or Argument Clinic&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-01-18: &lt;a class="reference external" href="http://bugs.python.org/issue29311"&gt;Argument Clinic: convert dict methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-02-02: &lt;a class="reference external" href="http://bugs.python.org/issue29419"&gt;Argument Clinic: inline PyArg_UnpackTuple and PyArg_ParseStack(AndKeyword)?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="other-optimization-issues"&gt;
&lt;h2&gt;10 other optimization issues&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;2016-08-24: &lt;a class="reference external" href="http://bugs.python.org/issue27848"&gt;C function calls: use Py_ssize_t rather than C int for number of arguments&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-09-07: &lt;a class="reference external" href="http://bugs.python.org/issue28004"&gt;Optimize bytes.join(sequence)&lt;/a&gt; [&lt;strong&gt;REJECTED&lt;/strong&gt;]&lt;/li&gt;
&lt;li&gt;2016-11-05: &lt;a class="reference external" href="http://bugs.python.org/issue28618"&gt;Decorate hot functions using __attribute__((hot)) to optimize Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-11-07: &lt;a class="reference external" href="http://bugs.python.org/issue28637"&gt;Python startup performance regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-11-25: &lt;a class="reference external" href="http://bugs.python.org/issue28800"&gt;Add RETURN_NONE bytecode instruction&lt;/a&gt; [&lt;strong&gt;REJECTED&lt;/strong&gt;]&lt;/li&gt;
&lt;li&gt;2016-11-25: &lt;a class="reference external" href="http://bugs.python.org/issue28799"&gt;Drop CALL_PROFILE special build?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2016-12-09: &lt;a class="reference external" href="http://bugs.python.org/issue28924"&gt;Inline PyEval_EvalFrameEx() in callers&lt;/a&gt; [&lt;strong&gt;REJECTED&lt;/strong&gt;]&lt;/li&gt;
&lt;li&gt;2016-12-15: &lt;a class="reference external" href="http://bugs.python.org/issue28977"&gt;Document PyObject_CallFunction() special case more explicitly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-02-06: &lt;a class="reference external" href="http://bugs.python.org/issue29461"&gt;Experiment usage of likely/unlikely in CPython core&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;2017-02-08: &lt;a class="reference external" href="http://bugs.python.org/issue29502"&gt;Should PyObject_Call() call the profiler on C functions, use C_TRACE() macro?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="fastcall"></category><category term="optimization"></category><category term="cpython"></category></entry><entry><title>FASTCALL microbenchmarks</title><link href="https://haypo.github.io/fastcall-microbenchmarks.html" rel="alternate"></link><published>2017-02-24T22:00:00+01:00</published><updated>2017-02-24T22:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-02-24:/fastcall-microbenchmarks.html</id><summary type="html">&lt;p&gt;For my FASTCALL project (CPython optimization avoiding temporary tuples and
dictionaries to pass arguments), I wrote many short microbenchmarks. I grouped
them into a new Git repository: &lt;a class="reference external" href="https://github.com/vstinner/pymicrobench"&gt;pymicrobench&lt;/a&gt;.  Benchmark results are required by
CPython developers to prove that an optimization is worth it. It's not uncommon
that I abandon a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For my FASTCALL project (CPython optimization avoiding temporary tuples and
dictionaries to pass arguments), I wrote many short microbenchmarks. I grouped
them into a new Git repository: &lt;a class="reference external" href="https://github.com/vstinner/pymicrobench"&gt;pymicrobench&lt;/a&gt;.  Benchmark results are required by
CPython developers to prove that an optimization is worth it. It's not uncommon
that I abandon a change because the speedup is not significant, makes CPython
slower, or because the change is too complex. Last 12 months, I counted that I
abandonned 9 optimization issues, rejected for different reasons, on a total of
46 optimization issues.&lt;/p&gt;
&lt;p&gt;This article gives Python 3.7 results of these microbenchmarks compared to
Python 3.5 (before FASTCALL). I ignored 3 microbenchmarks which are between 2%
and 5% slower: the code was not optimized and the result is not signifiant
(less than 10% on a &lt;em&gt;microbenchmark&lt;/em&gt; is not significant).&lt;/p&gt;
&lt;p&gt;On results below, the speedup is between 1.11x faster (-10%) and 1.92x faster
(-48%). It's not easy to isolate the speedup of only FASTCALL. Since Python
3.5, Python 3.7 got many other optimizations.&lt;/p&gt;
&lt;p&gt;Using FASTCALL gives a speedup around 20 ns: measured on a patch to use
FASTCALL.  It's not a lot, but many builtin functions take less than 100 ns, so
20 ns is significant in practice! Avoiding a tuple to pass positional arguments
is interesting, but FASTCALL also allows further internal optimizations.&lt;/p&gt;
&lt;p&gt;Microbenchmark on calling builtin functions:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="53%" /&gt;
&lt;col width="11%" /&gt;
&lt;col width="36%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Benchmark&lt;/th&gt;
&lt;th class="head"&gt;3.5&lt;/th&gt;
&lt;th class="head"&gt;3.7&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;struct.pack(&amp;quot;i&amp;quot;, 1)&lt;/td&gt;
&lt;td&gt;105 ns&lt;/td&gt;
&lt;td&gt;77.6 ns: 1.36x faster (-26%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;getattr(1, &amp;quot;real&amp;quot;)&lt;/td&gt;
&lt;td&gt;79.4 ns&lt;/td&gt;
&lt;td&gt;64.4 ns: 1.23x faster (-19%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Microbenchmark on calling methods of builtin types:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="53%" /&gt;
&lt;col width="11%" /&gt;
&lt;col width="36%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Benchmark&lt;/th&gt;
&lt;th class="head"&gt;3.5&lt;/th&gt;
&lt;th class="head"&gt;3.7&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;{1: 2}.get(7, None)&lt;/td&gt;
&lt;td&gt;84.9 ns&lt;/td&gt;
&lt;td&gt;61.6 ns: 1.38x faster (-27%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;collections.deque([None]).index(None)&lt;/td&gt;
&lt;td&gt;116 ns&lt;/td&gt;
&lt;td&gt;87.0 ns: 1.33x faster (-25%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;{1: 2}.get(1)&lt;/td&gt;
&lt;td&gt;79.4 ns&lt;/td&gt;
&lt;td&gt;59.6 ns: 1.33x faster (-25%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;&amp;quot;a&amp;quot;.replace(&amp;quot;x&amp;quot;, &amp;quot;y&amp;quot;)&lt;/td&gt;
&lt;td&gt;134 ns&lt;/td&gt;
&lt;td&gt;101 ns: 1.33x faster (-25%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;b&amp;quot;&amp;quot;.decode()&lt;/td&gt;
&lt;td&gt;71.5 ns&lt;/td&gt;
&lt;td&gt;54.5 ns: 1.31x faster (-24%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;b&amp;quot;&amp;quot;.decode(&amp;quot;ascii&amp;quot;)&lt;/td&gt;
&lt;td&gt;99.1 ns&lt;/td&gt;
&lt;td&gt;75.7 ns: 1.31x faster (-24%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;collections.deque.rotate(1)&lt;/td&gt;
&lt;td&gt;106 ns&lt;/td&gt;
&lt;td&gt;82.8 ns: 1.28x faster (-22%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;collections.deque.insert()&lt;/td&gt;
&lt;td&gt;778 ns&lt;/td&gt;
&lt;td&gt;608 ns: 1.28x faster (-22%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;b&amp;quot;&amp;quot;.join((b&amp;quot;hello&amp;quot;, b&amp;quot;world&amp;quot;) * 100)&lt;/td&gt;
&lt;td&gt;4.02 us&lt;/td&gt;
&lt;td&gt;3.32 us: 1.21x faster (-17%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;[0].count(0)&lt;/td&gt;
&lt;td&gt;53.9 ns&lt;/td&gt;
&lt;td&gt;46.3 ns: 1.16x faster (-14%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;collections.deque.rotate()&lt;/td&gt;
&lt;td&gt;72.6 ns&lt;/td&gt;
&lt;td&gt;63.1 ns: 1.15x faster (-13%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;b&amp;quot;&amp;quot;.join((b&amp;quot;hello&amp;quot;, b&amp;quot;world&amp;quot;))&lt;/td&gt;
&lt;td&gt;102 ns&lt;/td&gt;
&lt;td&gt;89.8 ns: 1.13x faster (-12%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Microbenchmark on builtin functions calling Python functions (callbacks):&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="53%" /&gt;
&lt;col width="11%" /&gt;
&lt;col width="36%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Benchmark&lt;/th&gt;
&lt;th class="head"&gt;3.5&lt;/th&gt;
&lt;th class="head"&gt;3.7&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;map(lambda x: x, list(range(1000)))&lt;/td&gt;
&lt;td&gt;76.1 us&lt;/td&gt;
&lt;td&gt;61.1 us: 1.25x faster (-20%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;sorted(list(range(1000)), key=lambda x: x)&lt;/td&gt;
&lt;td&gt;90.2 us&lt;/td&gt;
&lt;td&gt;78.2 us: 1.15x faster (-13%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;filter(lambda x: x, list(range(1000)))&lt;/td&gt;
&lt;td&gt;81.8 us&lt;/td&gt;
&lt;td&gt;73.4 us: 1.11x faster (-10%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Microbenchmark on calling slots (&lt;tt class="docutils literal"&gt;__getitem__&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;__init__&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;__int__&lt;/tt&gt;)
implemented in Python:&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="53%" /&gt;
&lt;col width="11%" /&gt;
&lt;col width="36%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Benchmark&lt;/th&gt;
&lt;th class="head"&gt;3.5&lt;/th&gt;
&lt;th class="head"&gt;3.7&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;Python __getitem__: obj[0]&lt;/td&gt;
&lt;td&gt;167 ns&lt;/td&gt;
&lt;td&gt;87.0 ns: 1.92x faster (-48%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;call_pyinit_kw1&lt;/td&gt;
&lt;td&gt;348 ns&lt;/td&gt;
&lt;td&gt;240 ns: 1.45x faster (-31%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;call_pyinit_kw5&lt;/td&gt;
&lt;td&gt;564 ns&lt;/td&gt;
&lt;td&gt;401 ns: 1.41x faster (-29%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;call_pyinit_kw10&lt;/td&gt;
&lt;td&gt;960 ns&lt;/td&gt;
&lt;td&gt;734 ns: 1.31x faster (-24%)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;&lt;td&gt;Python __int__: int(obj)&lt;/td&gt;
&lt;td&gt;241 ns&lt;/td&gt;
&lt;td&gt;207 ns: 1.16x faster (-14%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Microbenchmark on calling a method descriptor (static method):&lt;/p&gt;
&lt;table border="1" class="docutils"&gt;
&lt;colgroup&gt;
&lt;col width="53%" /&gt;
&lt;col width="11%" /&gt;
&lt;col width="36%" /&gt;
&lt;/colgroup&gt;
&lt;thead valign="bottom"&gt;
&lt;tr&gt;&lt;th class="head"&gt;Benchmark&lt;/th&gt;
&lt;th class="head"&gt;3.5&lt;/th&gt;
&lt;th class="head"&gt;3.7&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td&gt;int.to_bytes(1, 4, &amp;quot;little&amp;quot;)&lt;/td&gt;
&lt;td&gt;177 ns&lt;/td&gt;
&lt;td&gt;103 ns: 1.72x faster (-42%)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Benchmarks were run on &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;speed-python&lt;/span&gt;&lt;/tt&gt;, server used to run CPython benchmarks.&lt;/p&gt;
</content><category term="fastcall"></category><category term="optimization"></category><category term="cpython"></category></entry><entry><title>The start of the FASTCALL project</title><link href="https://haypo.github.io/start-fastcall-project.html" rel="alternate"></link><published>2017-02-16T17:00:00+01:00</published><updated>2017-02-16T17:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-02-16:/start-fastcall-project.html</id><summary type="html">&lt;div class="section" id="false-start"&gt;
&lt;h2&gt;False start&lt;/h2&gt;
&lt;p&gt;In April 2016, I experimented a Python change to avoid temporary tuple to call
functions. Builtin functions were between 20 and 50% faster!&lt;/p&gt;
&lt;p&gt;Sadly, some benchmarks were randomy slower. It will take me four months to
understand why!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="work-on-benchmarks"&gt;
&lt;h2&gt;Work on benchmarks&lt;/h2&gt;
&lt;p&gt;During four months, I worked on making …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="false-start"&gt;
&lt;h2&gt;False start&lt;/h2&gt;
&lt;p&gt;In April 2016, I experimented a Python change to avoid temporary tuple to call
functions. Builtin functions were between 20 and 50% faster!&lt;/p&gt;
&lt;p&gt;Sadly, some benchmarks were randomy slower. It will take me four months to
understand why!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="work-on-benchmarks"&gt;
&lt;h2&gt;Work on benchmarks&lt;/h2&gt;
&lt;p&gt;During four months, I worked on making benchmarks more stable. See my previous
blog posts:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-system.html"&gt;My journey to stable benchmark, part 1 (system)&lt;/a&gt; (May 21, 2016)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-deadcode.html"&gt;My journey to stable benchmark, part 2 (deadcode)&lt;/a&gt; (May 22, 2016)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-average.html"&gt;My journey to stable benchmark, part 3 (average)&lt;/a&gt; (May 23, 2016)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/perf-visualize-system-noise-with-cpu-isolation.html"&gt;Visualize the system noise using perf and CPU isolation&lt;/a&gt; (June 16, 2016)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/intel-cpus.html"&gt;Intel CPUs: P-state, C-state, Turbo Boost, CPU frequency, etc.&lt;/a&gt; (July 15, 2015)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/intel-cpus-part2.html"&gt;Intel CPUs (part 2): Turbo Boost, temperature, frequency and Pstate C0 bug&lt;/a&gt;
(September 23, 2016)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://haypo.github.io/analysis-python-performance-issue.html"&gt;Analysis of a Python performance issue&lt;/a&gt;
(November 19, 2016)&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;See my talk &lt;a class="reference external" href="https://fosdem.org/2017/schedule/event/python_stable_benchmark/"&gt;How to run a stable benchmark&lt;/a&gt; that I gave
at FOSDEM 2017 (Brussels, Belgium): slides + video. I listed all the issues
that I had to get reliable benchmarks.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ask-for-permission"&gt;
&lt;h2&gt;Ask for permission&lt;/h2&gt;
&lt;p&gt;August 2016, I
confirmed that my change didn't introduce any slowndown. So I asked for the
permission on the python-dev mailing list to start pushing changes: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2016-August/145793.html"&gt;New
calling convention to avoid temporarily tuples when calling functions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Guido van Rossum asked me for benchmark results:&lt;/p&gt;
&lt;blockquote&gt;
But is there a performance improvement?&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="benchmark-results"&gt;
&lt;h2&gt;Benchmark results&lt;/h2&gt;
&lt;p&gt;On micro-benchmarks, FASTCALL is much faster:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;getattr(1, &amp;quot;real&amp;quot;)&lt;/tt&gt; becomes &lt;strong&gt;44%&lt;/strong&gt; faster&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;list(filter(lambda x: x, &lt;span class="pre"&gt;list(range(1000))))&lt;/span&gt;&lt;/tt&gt; becomes &lt;strong&gt;31%&lt;/strong&gt; faster&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;namedtuple.attr&lt;/tt&gt; (read the attribute) becomes &lt;strong&gt;23%&lt;/strong&gt; faster&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Full results:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue26814#msg263999"&gt;FASTCALL compared to Python 3.6 (default branch)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://bugs.python.org/issue26814#msg264003"&gt;2.7 / 3.4 / 3.5 / 3.6 / 3.6 FASTCALL comparison&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the &lt;a class="reference external" href="https://bugs.python.org/issue26814#msg266359"&gt;CPython benchmark suite&lt;/a&gt;, I also saw many faster
benchmarks:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;pickle_list: &lt;strong&gt;1.29x faster&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;etree_generate: &lt;strong&gt;1.22x faster&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;pickle_dict: &lt;strong&gt;1.19x faster&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;etree_process: &lt;strong&gt;1.16x faster&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;mako_v2: &lt;strong&gt;1.13x faster&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;telco: &lt;strong&gt;1.09x faster&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;...&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="replies-to-my-email"&gt;
&lt;h2&gt;Replies to my email&lt;/h2&gt;
&lt;p&gt;I got two very positive replies, so I understood that it was ok.&lt;/p&gt;
&lt;p&gt;Brett Canon:&lt;/p&gt;
&lt;blockquote&gt;
I just wanted to say I'm excited about this and I'm glad someone is taking
advantage of what Argument Clinic allows for and what I know Larry had
initially hoped AC would make happen!&lt;/blockquote&gt;
&lt;p&gt;Yury Selivanov:&lt;/p&gt;
&lt;blockquote&gt;
Exceptional results, congrats Victor. Will be happy to help with code
review.&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="real-start"&gt;
&lt;h2&gt;Real start&lt;/h2&gt;
&lt;p&gt;That's how the FASTCALL began for real! I started to push a long serie of
patches adding new private functions and then modify code to call these new
functions.&lt;/p&gt;
&lt;/div&gt;
</content><category term="fastcall"></category><category term="optimization"></category><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2016 Q4</title><link href="https://haypo.github.io/contrib-cpython-2016q4.html" rel="alternate"></link><published>2017-02-16T11:00:00+01:00</published><updated>2017-02-16T11:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-02-16:/contrib-cpython-2016q4.html</id><summary type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2016 Q4
(october, november, december):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg log -r 'date(&amp;quot;2016-10-01&amp;quot;):date(&amp;quot;2016-12-31&amp;quot;)' --no-merges -u Stinner
&lt;/pre&gt;
&lt;p&gt;Statistics: 105 non-merge commits + 31 merge commits (total: 136 commits).&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q3.html"&gt;My contributions to CPython during 2016 Q3&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q1.html"&gt;My contributions to
CPython during 2017 Q1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Table of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2016 Q4
(october, november, december):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg log -r 'date(&amp;quot;2016-10-01&amp;quot;):date(&amp;quot;2016-12-31&amp;quot;)' --no-merges -u Stinner
&lt;/pre&gt;
&lt;p&gt;Statistics: 105 non-merge commits + 31 merge commits (total: 136 commits).&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q3.html"&gt;My contributions to CPython during 2016 Q3&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2017q1.html"&gt;My contributions to
CPython during 2017 Q1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Python startup performance regression&lt;/li&gt;
&lt;li&gt;Optimizations&lt;/li&gt;
&lt;li&gt;Code placement and __attribute__((hot))&lt;/li&gt;
&lt;li&gt;Interesting bug: duplicated filters when tests reload the warnings module&lt;/li&gt;
&lt;li&gt;Contributions&lt;/li&gt;
&lt;li&gt;regrtest&lt;/li&gt;
&lt;li&gt;Other changes&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="python-startup-performance-regression"&gt;
&lt;h2&gt;Python startup performance regression&lt;/h2&gt;
&lt;div class="section" id="regresion"&gt;
&lt;h3&gt;Regresion&lt;/h3&gt;
&lt;p&gt;My work on tracking Python performances started to become useful :-) I
identified a performance slowdown on the &lt;tt class="docutils literal"&gt;bm_python_startup&lt;/tt&gt; benchmark
(average time to start Python).&lt;/p&gt;
&lt;p&gt;Before September 2016, the start took around &lt;strong&gt;17.9 ms&lt;/strong&gt;. At September 15,
after the  &lt;a class="reference external" href="https://haypo.github.io/cpython-sprint-2016.html"&gt;CPython sprint&lt;/a&gt;, it was
better: &lt;strong&gt;13.4 ms&lt;/strong&gt;. But suddenly, at september 19, it became much worse:
&lt;strong&gt;22.8 ms&lt;/strong&gt;. What happened?&lt;/p&gt;
&lt;p&gt;Timeline of Python startup performance on speed.python.org:&lt;/p&gt;
&lt;a class="reference external image-reference" href="https://speed.python.org/timeline/#/?exe=5&amp;amp;ben=python_startup&amp;amp;env=1&amp;amp;revs=50&amp;amp;equid=off&amp;amp;quarts=on&amp;amp;extr=on"&gt;&lt;img alt="Timeline of Python startup performance" src="https://haypo.github.io/images/python_startup_regression.png" /&gt;&lt;/a&gt;
&lt;p&gt;I looked at commits between September 15 and September 19, and I quickly
identified the commit of the &lt;a class="reference external" href="http://bugs.python.org/issue28082"&gt;convert re flags to (much
friendlier) IntFlag constants (issue #28082)&lt;/a&gt;. The &lt;tt class="docutils literal"&gt;re&lt;/tt&gt; module now imports the
&lt;tt class="docutils literal"&gt;enum&lt;/tt&gt; module to get a better representation for their flags.  Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ ./python
Python 3.7.0a0
&amp;gt;&amp;gt;&amp;gt; import re; re.M
&amp;lt;RegexFlag.MULTILINE: 8&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="revert"&gt;
&lt;h3&gt;Revert&lt;/h3&gt;
&lt;p&gt;At November 7, I opened the issue #28637 to propose to revert the commit to get
back better Python startup performance. The revert was approved by Guido van
Rossum, so I pushed it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="better-fix"&gt;
&lt;h3&gt;Better fix&lt;/h3&gt;
&lt;p&gt;I also noticed that the &lt;tt class="docutils literal"&gt;re&lt;/tt&gt; module is not imported by default if Python is
installed or if Python is run from its source code directory. The &lt;tt class="docutils literal"&gt;re&lt;/tt&gt; module
is only imported by default if Python is installed in a virtual environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Serhiy Storchaka&lt;/strong&gt; proposed a change to not import &lt;tt class="docutils literal"&gt;re&lt;/tt&gt; anymore in the
&lt;tt class="docutils literal"&gt;site&lt;/tt&gt; module when Python runs into a virutal environment. Since the benefit
was obvious (avoid an import at startup) and simple, it was quickly merged.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="restore-reverted-enum-change"&gt;
&lt;h3&gt;Restore reverted enum change&lt;/h3&gt;
&lt;p&gt;Since using &lt;tt class="docutils literal"&gt;enum&lt;/tt&gt; in &lt;tt class="docutils literal"&gt;re&lt;/tt&gt; has no more impact on Python startup
performance by default, the &lt;tt class="docutils literal"&gt;enum&lt;/tt&gt; change was restored at November 14.&lt;/p&gt;
&lt;p&gt;Sadly, the &lt;tt class="docutils literal"&gt;enum&lt;/tt&gt; change still have an impact on performance:
&lt;tt class="docutils literal"&gt;re.compile()&lt;/tt&gt; became 1.2x slower (312 ms =&amp;gt; 376 ms: +20%).&lt;/p&gt;
&lt;a class="reference external image-reference" href="https://speed.python.org/timeline/#/?exe=5&amp;amp;ben=regex_compile&amp;amp;env=1&amp;amp;revs=50&amp;amp;equid=off&amp;amp;quarts=on&amp;amp;extr=on"&gt;&lt;img alt="Timeline of re.compile() performance" src="https://haypo.github.io/images/regex_compile_perf.png" /&gt;&lt;/a&gt;
&lt;p&gt;I think that it's ok since it is very easy to use precompiled regular
expressions in an application: store and reuse the result of &lt;tt class="docutils literal"&gt;re.compile()&lt;/tt&gt;,
instead of calling directly &lt;tt class="docutils literal"&gt;re.match()&lt;/tt&gt; for example.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizations"&gt;
&lt;h2&gt;Optimizations&lt;/h2&gt;
&lt;div class="section" id="fastcall"&gt;
&lt;h3&gt;FASTCALL&lt;/h3&gt;
&lt;p&gt;Same than 2016 Q3: I pushed a &lt;em&gt;lot&lt;/em&gt; of changes for FASTCALL optimizations, but
I will write a dedicated article later.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="no-int-int-micro-optimization-thank-you"&gt;
&lt;h3&gt;No int+int micro-optimization, thank you&lt;/h3&gt;
&lt;p&gt;After 2 years of benchmarking and a huge effort of making Python benchmarks more
reliable and stable, I decided to close the issue #21955 &amp;quot;ceval.c: implement
fast path for integers with a single digit&amp;quot; as REJECTED. It became clear to me
that such micro-optimization has no effect on non-trivial code, but only on
specially crafted micro-benchmarks. I added a comment in the C code to prevent
further optimizations attempts:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
/* NOTE(haypo): Please don't try to micro-optimize int+int on
   CPython using bytecode, it is simply worthless.
   See http://bugs.python.org/issue21955 and
   http://bugs.python.org/issue10044 for the discussion. In short,
   no patch shown any impact on a realistic benchmark, only a minor
   speedup on microbenchmarks. */
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="timeit"&gt;
&lt;h3&gt;timeit&lt;/h3&gt;
&lt;p&gt;I enhanced the &lt;tt class="docutils literal"&gt;timeit&lt;/tt&gt; benchmark module to make it more reliable (issue
#28240):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Autorange now starts with a single loop iteration instead of 10. For example,
&lt;tt class="docutils literal"&gt;python3 &lt;span class="pre"&gt;-m&lt;/span&gt; timeit &lt;span class="pre"&gt;-s&lt;/span&gt; 'import time' 'time.sleep(1)'&lt;/tt&gt; now only takes 4
seconds instead of 40 seconds.&lt;/li&gt;
&lt;li&gt;Repeat the benchmarks 5 times by default, instead of only 3, to make
benchmarks more reliable.&lt;/li&gt;
&lt;li&gt;Remove &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-c/--clock&lt;/span&gt;&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-t/--time&lt;/span&gt;&lt;/tt&gt; command line options which were
deprecated since Python 3.3.&lt;/li&gt;
&lt;li&gt;Add &lt;tt class="docutils literal"&gt;nsec&lt;/tt&gt; (nanosecond) unit to format timings&lt;/li&gt;
&lt;li&gt;Enhance formatting of raw timings in verbose mode. Add newlines to the output
for readability.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="micro-optimizations"&gt;
&lt;h3&gt;Micro-optimizations&lt;/h3&gt;
&lt;p&gt;I also pushed two minor micro-optimizations:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Use &lt;tt class="docutils literal"&gt;PyThreadState_GET()&lt;/tt&gt; macro in performance critical code.
&lt;tt class="docutils literal"&gt;_PyThreadState_UncheckedGet()&lt;/tt&gt; calls are not inlined as expected, even
when using &lt;tt class="docutils literal"&gt;gcc &lt;span class="pre"&gt;-O3&lt;/span&gt;&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Modify &lt;tt class="docutils literal"&gt;type_setattro()&lt;/tt&gt; to call directly
&lt;tt class="docutils literal"&gt;_PyObject_GenericSetAttrWithDict()&lt;/tt&gt; instead of
&lt;tt class="docutils literal"&gt;PyObject_GenericSetAttr()&lt;/tt&gt;. &lt;tt class="docutils literal"&gt;PyObject_GenericSetAttr()&lt;/tt&gt; is a thin
wrapper to &lt;tt class="docutils literal"&gt;_PyObject_GenericSetAttrWithDict()&lt;/tt&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="code-placement-and-attribute-hot"&gt;
&lt;h2&gt;Code placement and __attribute__((hot))&lt;/h2&gt;
&lt;p&gt;On &lt;a class="reference external" href="https://speed.python.org/"&gt;speed.python.org&lt;/a&gt;, I still noticed random
performance slowdowns on the evil &lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; benchmark. This benchmark is
a &lt;em&gt;micro&lt;/em&gt;-benchmark measuring the performance of a single Python function call,
it is CPU-bound and very small and so impact by CPU caches. I was bitten again
by significant performance slowdown only caused by code placement.&lt;/p&gt;
&lt;p&gt;It wasn't possible to use &lt;em&gt;Profiled Guided Optimization&lt;/em&gt; (PGO) on the benchmark
runner, since it used Ubuntu 14.04 and GCC crashed with an &amp;quot;internal error&amp;quot;.&lt;/p&gt;
&lt;p&gt;So I tried something different: mark &amp;quot;hot functions&amp;quot; with
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((hot))&lt;/span&gt;&lt;/tt&gt;. It's a GCC and Clang attribute helping code
placements: &amp;quot;hot functions&amp;quot; are moved to a dedicated ELF section and so are
closer in memory, and the compiler tries to optimize these functions even more.&lt;/p&gt;
&lt;p&gt;The following functions are considered as hot according to statistics collected
by Linux &lt;tt class="docutils literal"&gt;perf record&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;perf report&lt;/tt&gt; commands:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;_PyEval_EvalFrameDefault()&lt;/li&gt;
&lt;li&gt;call_function()&lt;/li&gt;
&lt;li&gt;_PyFunction_FastCall()&lt;/li&gt;
&lt;li&gt;PyFrame_New()&lt;/li&gt;
&lt;li&gt;frame_dealloc()&lt;/li&gt;
&lt;li&gt;PyErr_Occurred()&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I added a &lt;tt class="docutils literal"&gt;_Py_HOT_FUNCTION&lt;/tt&gt; macro which uses &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((hot))&lt;/span&gt;&lt;/tt&gt; and
used &lt;tt class="docutils literal"&gt;_Py_HOT_FUNCTION&lt;/tt&gt; on these functions (issue #28618).&lt;/p&gt;
&lt;p&gt;Read also my previous blog article &lt;a class="reference external" href="https://haypo.github.io/analysis-python-performance-issue.html"&gt;Analysis of a Python performance issue&lt;/a&gt; for a deeper analysis.&lt;/p&gt;
&lt;p&gt;Sadly, after I wrote this blog post and after more analysis of &lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt;
benchmark results, I saw that &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((hot))&lt;/span&gt;&lt;/tt&gt; wasn't enough. I still
had random major performance slowdown.&lt;/p&gt;
&lt;p&gt;I dediced to upgrade the performance runner to Ubuntu 16.04. It was dangerous
because nobody has access to the physical server, so it may takes weeks to
repair it if I did a mistake. Hopefully, the upgrade gone smoothly and I was
able to run again all benchmarks using PGO. As expected, using PGO+LTO,
benchmark results are more stable!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="interesting-bug-duplicated-filters-when-tests-reload-the-warnings-module"&gt;
&lt;h2&gt;Interesting bug: duplicated filters when tests reload the warnings module&lt;/h2&gt;
&lt;p&gt;Python test suite has an old bug: the issue #18383 opened in July 2013.
Sometimes, the test suite emits the following warning:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
[247/375] test_warnings
Warning -- warnings.filters was modified by test_warnings
&lt;/pre&gt;
&lt;p&gt;Since it's only a warning and it only occurs in the Python test suite, it was a
low priority and took 3 years to be fixed! It also took time to find the right
design to fix the root cause.&lt;/p&gt;
&lt;div class="section" id="duplicated-filters"&gt;
&lt;h3&gt;Duplicated filters&lt;/h3&gt;
&lt;p&gt;test_warnings imports the &lt;tt class="docutils literal"&gt;warnings&lt;/tt&gt; module 3 times:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
import warnings as original_warnings   # Python
py_warnings = support.import_fresh_module('warnings', blocked=['_warnings'])  # Python
c_warnings = support.import_fresh_module('warnings', fresh=['_warnings'])   # C
&lt;/pre&gt;
&lt;p&gt;The Python &lt;tt class="docutils literal"&gt;warnings&lt;/tt&gt; module (&lt;tt class="docutils literal"&gt;Lib/warnings.py&lt;/tt&gt;) installs warning filters
when the module is loaded:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
_processoptions(sys.warnoptions)
&lt;/pre&gt;
&lt;p&gt;where &lt;tt class="docutils literal"&gt;sys.warnoptions&lt;/tt&gt; contains the value of the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-W&lt;/span&gt;&lt;/tt&gt; command line option.&lt;/p&gt;
&lt;p&gt;If the Python module is loaded more than once, filters are duplicated.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="first-fix-use-the-right-module"&gt;
&lt;h3&gt;First fix: use the right module&lt;/h3&gt;
&lt;p&gt;I pushed a first fix in september 2015.&lt;/p&gt;
&lt;p&gt;Fix test_warnings: don't modify warnings.filters. BaseTest now ensures that
unittest.TestCase.assertWarns() uses the same warnings module than
warnings.catch_warnings(). Otherwise, warnings.catch_warnings() will be unable
to remove the added filter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="second-fix-don-t-add-duplicated-filters"&gt;
&lt;h3&gt;Second fix: don't add duplicated filters&lt;/h3&gt;
&lt;p&gt;Issue #18383: the first patch was proposed by &lt;strong&gt;Florent Xicluna&lt;/strong&gt; in 2013: save
the length of filters, and remove newly added filters after &lt;tt class="docutils literal"&gt;warnings&lt;/tt&gt;
modules are reloaded by &lt;tt class="docutils literal"&gt;test_warnings&lt;/tt&gt;. December 2014, &lt;strong&gt;Serhiy Storchaka&lt;/strong&gt;
reviewed the patch: he didn't like this &lt;em&gt;workaround&lt;/em&gt;, he would like to fix the
&lt;em&gt;root cause&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;March 2015, &lt;strong&gt;Alex Shkop&lt;/strong&gt; proposed a patch which avoids to add duplicated
filters.&lt;/p&gt;
&lt;p&gt;September 2015, &lt;strong&gt;Martin Panter&lt;/strong&gt; proposed to try to save/restore filters on
the C warnings module. I proposed something similar in the issue #26742. But
this solution has the same flaw that Florent's idea: it's only a workaround.&lt;/p&gt;
&lt;p&gt;Martin also proposed add a private flag to say that filters were already set to
not try to add again same filters.&lt;/p&gt;
&lt;p&gt;Finally, in may 2016, Martin updated Alex's patch avoiding duplicated filters
and pushed it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="third-fix"&gt;
&lt;h3&gt;Third fix&lt;/h3&gt;
&lt;p&gt;The filter comparisons wasn't perfect. A filter can be made of a precompiled
regular expression, whereas these objects don't implement comparison.&lt;/p&gt;
&lt;p&gt;November 2016, I opened the issue #28727 to propose to implement rich
comparison for &lt;tt class="docutils literal"&gt;_sre.SRE_Pattern&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;My first patch didn't implement &lt;tt class="docutils literal"&gt;hash()&lt;/tt&gt; and had different bugs. It took me
almost one week and 6 versions to write complete unit tests and handle all
cases: support bytes and Unicode and handle regular expression flags.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Serhiy Storchaka&lt;/strong&gt; found bugs and helps me to write the implementation.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="contributions"&gt;
&lt;h2&gt;Contributions&lt;/h2&gt;
&lt;p&gt;As usual, I reviewed and pushed changes written by other contributors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #27896: Allow passing sphinx options to Doc/Makefile. Patch written by
&lt;strong&gt;Julien Palard&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #28476: Reuse math.factorial() in test_random.
Patch written by &lt;strong&gt;Francisco Couzo&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #28479: Fix reST syntax in windows.rst. Patch written by &lt;strong&gt;Julien Palard&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #26273: Add new constants: &lt;tt class="docutils literal"&gt;socket.TCP_CONGESTION&lt;/tt&gt; (Linux 2.6.13) and
&lt;tt class="docutils literal"&gt;socket.TCP_USER_TIMEOUT&lt;/tt&gt; (Linux 2.6.37).
Patch written by &lt;strong&gt;Omar Sandoval&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #28979: Fix What's New in Python 3.6: compact dict is not faster, but
only more compact. Patch written by &lt;strong&gt;Brendan Donegan&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #28147: Fix a memory leak in split-table dictionaries: &lt;tt class="docutils literal"&gt;setattr()&lt;/tt&gt;
must not convert combined table into split table.
Patch written by &lt;strong&gt;INADA Naoki&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #29109: Enhance tracemalloc documentation:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Wrong parameter name, 'group_by' instead of 'key_type'&lt;/li&gt;
&lt;li&gt;Don't round up numbers when explaining the examples. If they exactly match
what can be read in the script output, it is to easier to understand
(4.8 MiB vs 4855 KiB)&lt;/li&gt;
&lt;li&gt;Fix incorrect method link that was pointing to another module&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Patch written by &lt;strong&gt;Loic Pefferkorn&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="regrtest"&gt;
&lt;h2&gt;regrtest&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;regrtest &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--fromfile&lt;/span&gt;&lt;/tt&gt; now accepts a list of filenames, not only a list of
&lt;em&gt;test&lt;/em&gt; names.&lt;/li&gt;
&lt;li&gt;Issue #28409: regrtest: fix the parser of command line arguments.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other changes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Fix &lt;tt class="docutils literal"&gt;_Py_normalize_encoding()&lt;/tt&gt; function: It was not exactly the same than
Python &lt;tt class="docutils literal"&gt;encodings.normalize_encoding()&lt;/tt&gt;: the C function now also converts
to lowercase.&lt;/li&gt;
&lt;li&gt;Issue #28256: Cleanup &lt;tt class="docutils literal"&gt;_math.c&lt;/tt&gt;: only define fallback implementations when
needed. It avoids producing deadcode when the system provides required math
functions, and so enhance the code coverage.&lt;/li&gt;
&lt;li&gt;_csv: use &lt;tt class="docutils literal"&gt;_PyLong_AsInt()&lt;/tt&gt; to simplify the code, the function checks for
the limits of the C &lt;tt class="docutils literal"&gt;int&lt;/tt&gt; type.&lt;/li&gt;
&lt;li&gt;Issue #28544: Fix &lt;tt class="docutils literal"&gt;_asynciomodule.c&lt;/tt&gt; on Windows. &lt;tt class="docutils literal"&gt;PyType_Ready()&lt;/tt&gt; sets
the reference to &lt;tt class="docutils literal"&gt;&amp;amp;PyType_Type&lt;/tt&gt;. &lt;tt class="docutils literal"&gt;&amp;amp;PyType_Type&lt;/tt&gt; address cannot be
resolved at compilation time (not on Windows?).&lt;/li&gt;
&lt;li&gt;Issue #28082: Add basic unit tests on the new &lt;tt class="docutils literal"&gt;re&lt;/tt&gt; enums.&lt;/li&gt;
&lt;li&gt;Issue #28691: Fix &lt;tt class="docutils literal"&gt;warn_invalid_escape_sequence()&lt;/tt&gt;: handle correctly
&lt;tt class="docutils literal"&gt;DeprecationWarning&lt;/tt&gt; raised as an exception. First clear the current
exception to replace the &lt;tt class="docutils literal"&gt;DeprecationWarning&lt;/tt&gt; exception with a
&lt;tt class="docutils literal"&gt;SyntaxError&lt;/tt&gt; exception. Unit test written by &lt;strong&gt;Serhiy Storchaka&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #28023: Fix python-gdb.py on old GDB versions. Replace
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;int(value.address)+offset&lt;/span&gt;&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;value.cast(unsigned &lt;span class="pre"&gt;char*)+offset&lt;/span&gt;&lt;/tt&gt;.
It seems like &lt;tt class="docutils literal"&gt;int(value.address)&lt;/tt&gt; fails on old GDB versions.&lt;/li&gt;
&lt;li&gt;Issue #28765: &lt;tt class="docutils literal"&gt;_sre.compile()&lt;/tt&gt; now checks the type of &lt;tt class="docutils literal"&gt;groupindex&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;indexgroup&lt;/tt&gt; arguments. &lt;tt class="docutils literal"&gt;groupindex&lt;/tt&gt; must a dictionary and &lt;tt class="docutils literal"&gt;indexgroup&lt;/tt&gt;
must be a tuple.  Previously, &lt;tt class="docutils literal"&gt;indexgroup&lt;/tt&gt; was a list. Use a tuple to
reduce the memory usage.&lt;/li&gt;
&lt;li&gt;Issue #28782: Fix a bug in the implementation &lt;tt class="docutils literal"&gt;yield from&lt;/tt&gt;
(fix &lt;tt class="docutils literal"&gt;_PyGen_yf()&lt;/tt&gt; function). Fix the test checking if the next instruction
is &lt;tt class="docutils literal"&gt;YIELD_FROM&lt;/tt&gt;.  Regression introduced by the new &amp;quot;WordCode&amp;quot; bytecode
(issue #26647). Fix reviewed by &lt;strong&gt;Serhiy Storchaka&lt;/strong&gt; and &lt;strong&gt;Yury Selivanov&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #28792: Remove aliases from &lt;tt class="docutils literal"&gt;_bisect&lt;/tt&gt;. Remove aliases from the C
module.  Always implement &lt;tt class="docutils literal"&gt;bisect()&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;insort()&lt;/tt&gt; aliases in
&lt;tt class="docutils literal"&gt;bisect.py&lt;/tt&gt;.  Remove also the &lt;tt class="docutils literal"&gt;# backward compatibility&lt;/tt&gt; comment: there
is no plan to deprecate nor remove these aliases. When keys are equal, it
makes sense to use &lt;tt class="docutils literal"&gt;bisect.bisect()&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;bisect.insort()&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Fix a &lt;tt class="docutils literal"&gt;ResourceWarning&lt;/tt&gt; in &lt;tt class="docutils literal"&gt;generate_opcode_h.py&lt;/tt&gt;. Use a context manager
to close the Python file. Replace also &lt;tt class="docutils literal"&gt;open()&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;tokenize.open()&lt;/tt&gt; to
handle coding cookie of &lt;tt class="docutils literal"&gt;Lib/opcode.py&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Issue #28740: Add &lt;tt class="docutils literal"&gt;sys.getandroidapilevel()&lt;/tt&gt; function: return the build
time API version of Android as an integer. Function only available on
Android. The availability of this function can be tested to check if Python
is running on Android.&lt;/li&gt;
&lt;li&gt;Issue #28152: Fix &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-Wunreachable-code&lt;/span&gt;&lt;/tt&gt; warnings on Clang.&lt;ul&gt;
&lt;li&gt;Don't declare dead code when the code is compiled with Clang.&lt;/li&gt;
&lt;li&gt;Replace C &lt;tt class="docutils literal"&gt;if()&lt;/tt&gt; with precompiler &lt;tt class="docutils literal"&gt;#if&lt;/tt&gt; to fix a warning on dead code
when using Clang.&lt;/li&gt;
&lt;li&gt;Replace &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;(0)&lt;/tt&gt; to ignore a compiler warning about dead code on
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;((int)(SEM_VALUE_MAX)&lt;/span&gt; &amp;lt; 0)&lt;/tt&gt;: &lt;tt class="docutils literal"&gt;SEM_VALUE_MAX&lt;/tt&gt; is not negative on Linux.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Issue #28835: Fix a regression introduced in &lt;tt class="docutils literal"&gt;warnings.catch_warnings()&lt;/tt&gt;:
call &lt;tt class="docutils literal"&gt;warnings.showwarning()&lt;/tt&gt; if it was overriden inside the context
manager.&lt;/li&gt;
&lt;li&gt;Issue #28915: Replace &lt;tt class="docutils literal"&gt;int&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;Py_ssize_t&lt;/tt&gt; in &lt;tt class="docutils literal"&gt;modsupport&lt;/tt&gt;.
&lt;tt class="docutils literal"&gt;Py_ssize_t&lt;/tt&gt; type is better for indexes. The compiler might emit more
efficient code for &lt;tt class="docutils literal"&gt;i++&lt;/tt&gt;. &lt;tt class="docutils literal"&gt;Py_ssize_t&lt;/tt&gt; is the type of a PyTuple index for
example. Replace also &lt;tt class="docutils literal"&gt;int endchar&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;char endchar&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Initialize variables to fix compiler warnings. Warnings seen on the &amp;quot;AMD64
Debian PGO 3.x&amp;quot; buildbot. Warnings are false positive, but variable
initialization should not harm performances.&lt;/li&gt;
&lt;li&gt;Remove useless variable initialization. Don't initialize variables which are
not used before they are assigned.&lt;/li&gt;
&lt;li&gt;Issue #28838: Cleanup &lt;tt class="docutils literal"&gt;abstract.h&lt;/tt&gt;. Rewrite all comments to use the same style
than other Python header files: comment functions &lt;em&gt;before&lt;/em&gt; their declaration,
no newline between the comment and the declaration. Reformat some comments,
add newlines, to make them easier to read. Quote argument like 'arg' to
mention an argument in a comment.&lt;/li&gt;
&lt;li&gt;Issue #28838: &lt;tt class="docutils literal"&gt;abstract.h&lt;/tt&gt;: remove long outdated comment. The documentation
of the Python C API is more complete and more up to date than this old
comment. Removal suggested by &lt;strong&gt;Antoine Pitrou&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;python-gdb.py: catch &lt;tt class="docutils literal"&gt;gdb.error&lt;/tt&gt; on &lt;tt class="docutils literal"&gt;gdb.selected_frame()&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Issue #28383: &lt;tt class="docutils literal"&gt;__hash__&lt;/tt&gt; documentation recommends naive XOR to combine, but
this is suboptimal. Update the documentation to suggest to reuse the
&lt;tt class="docutils literal"&gt;hash()&lt;/tt&gt; function on a tuple, with an example.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2016 Q3</title><link href="https://haypo.github.io/contrib-cpython-2016q3.html" rel="alternate"></link><published>2017-02-14T19:00:00+01:00</published><updated>2017-02-14T19:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-02-14:/contrib-cpython-2016q3.html</id><summary type="html">&lt;p class="first last"&gt;My contributions to CPython during 2016 Q3&lt;/p&gt;
</summary><content type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2016 Q3
(july, august, september):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg log -r 'date(&amp;quot;2016-07-01&amp;quot;):date(&amp;quot;2016-09-30&amp;quot;)' --no-merges -u Stinner
&lt;/pre&gt;
&lt;p&gt;Statistics: 161 non-merge commits + 29 merge commits (total: 190 commits).&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q2.html"&gt;My contributions to CPython during 2016 Q2&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q4.html"&gt;My contributions to
CPython during 2016 Q4&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Table of Contents:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Two new core developers&lt;/li&gt;
&lt;li&gt;CPython sprint, September, in California&lt;/li&gt;
&lt;li&gt;PEP 524: Make os.urandom() blocking on Linux&lt;/li&gt;
&lt;li&gt;PEP 509: private dictionary version&lt;/li&gt;
&lt;li&gt;FASTCALL: optimization avoiding temporary tuple to call functions&lt;/li&gt;
&lt;li&gt;More efficient CALL_FUNCTION bytecode&lt;/li&gt;
&lt;li&gt;Work on optimization&lt;/li&gt;
&lt;li&gt;Interesting bug: hidden resource warnings&lt;/li&gt;
&lt;li&gt;Contributions&lt;/li&gt;
&lt;li&gt;Bugfixes&lt;/li&gt;
&lt;li&gt;regrtest changes&lt;/li&gt;
&lt;li&gt;Tests changes&lt;/li&gt;
&lt;li&gt;Other changes&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="two-new-core-developers"&gt;
&lt;h2&gt;Two new core developers&lt;/h2&gt;
&lt;p&gt;New core developers is the result of the productive third 2016 quarter.&lt;/p&gt;
&lt;p&gt;At september 25, 2016, Yury Selivanov proposed to give &lt;a class="reference external" href="https://mail.python.org/pipermail/python-committers/2016-September/004013.html"&gt;commit privileges for
INADA Naoki&lt;/a&gt;.
Naoki became a core developer the day after!&lt;/p&gt;
&lt;p&gt;At november 14, 2016, I proposed to &lt;a class="reference external" href="https://mail.python.org/pipermail/python-committers/2016-November/004045.html"&gt;promote Xiang Zhang as a core developer&lt;/a&gt;.
One week later, he also became a core developer! I mentored him during one
month, and later let him push directly changes.&lt;/p&gt;
&lt;p&gt;Most Python core developers are men coming from North America and Europe.
INADA Naoki comes from Japan and Xiang Zhang comes from China: more core
developers from Asia, we increased the diversity of Python core developers!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cpython-sprint-september-in-california"&gt;
&lt;h2&gt;CPython sprint, September, in California&lt;/h2&gt;
&lt;p&gt;I was invited at my first CPython sprint in September! Five days, September
5-9, at Instagram office in California, USA. I reviewed a lot of changes and
pushed many new features! Read my previous blog post: &lt;a class="reference external" href="https://haypo.github.io/cpython-sprint-2016.html"&gt;CPython sprint,
september 2016&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pep-524-make-os-urandom-blocking-on-linux"&gt;
&lt;h2&gt;PEP 524: Make os.urandom() blocking on Linux&lt;/h2&gt;
&lt;p&gt;I pushed the implementation my PEP 524: read my previous blog post: &lt;a class="reference external" href="https://haypo.github.io/pep-524-os-urandom-blocking.html"&gt;PEP 524:
os.urandom() now blocks on Linux in Python 3.6&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pep-509-private-dictionary-version"&gt;
&lt;h2&gt;PEP 509: private dictionary version&lt;/h2&gt;
&lt;p&gt;Another enhancement from my &lt;a class="reference external" href="http://faster-cpython.readthedocs.io/fat_python.html"&gt;FAT Python&lt;/a&gt; project: my &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0509/"&gt;PEP 509:
Add a private version to dict&lt;/a&gt; was
approved at the CPython sprint by Guido van Rossum.&lt;/p&gt;
&lt;p&gt;The dictionary version is used by FAT Python to check quickly if a variable was
modified in a Python namespace. Technically, a Python namespace is a regular
dictionary.&lt;/p&gt;
&lt;p&gt;Using the feedback from the python-ideas mailing list on the first version of
my PEP, I made further changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Use 64-bit unsigned integers on 32-bit system: &amp;quot;A risk of an integer overflow
every 584 years is acceptable.&amp;quot; Using 32-bit, an overflow occurs every 4
seconds!&lt;/li&gt;
&lt;li&gt;Don't expose the version at Python level to prevent users writing
optimizations based on it in Python. Reading the dictionary version in Python
is as slow as a dictionary lookup, wheras the version is usually used to
avoid a &amp;quot;slow&amp;quot; dictionary lookup. The version is only accessible at the C
level.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While my experimental FAT Python static optimizer didn't convince Guido, Yury
Selivanov wrote yet another cache for global variables using the dictionary
version: &lt;a class="reference external" href="http://bugs.python.org/issue28158"&gt;Implement LOAD_GLOBAL opcode cache&lt;/a&gt; (sadly, not merged yet).&lt;/p&gt;
&lt;p&gt;I added the private version to the builtin dict type with the issue #26058. The
global dictionary version is incremented at each dictionary creation and at
each dictionary change, and each dictionary has its own version as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fastcall-optimization-avoiding-temporary-tuple-to-call-functions"&gt;
&lt;h2&gt;FASTCALL: optimization avoiding temporary tuple to call functions&lt;/h2&gt;
&lt;p&gt;Thanks to my work on making Python benchmarks more stable, I confirmed that my
FASTCALL patches don't introduce performance regressions, and make Python
faster in some specific cases.&lt;/p&gt;
&lt;p&gt;I started to push FASTCALL changes. It will take me 6 months to push most
changes to enable fully FASTCALL &amp;quot;everywhere&amp;quot; in the code base and to finish
the implementation.&lt;/p&gt;
&lt;p&gt;Following blog posts will describe FASTCALL changes, its history and
performance enhancements. Spoiler: Python 3.6 is fast!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="more-efficient-call-function-bytecode"&gt;
&lt;h2&gt;More efficient CALL_FUNCTION bytecode&lt;/h2&gt;
&lt;p&gt;I reviewed and merged Demur Rumed's patch to make the CALL_FUNCTION opcodes
more efficient. Demur implemented the design proposed by Serhiy Storchaka.
Serhiy Storchaka also reviewied the implementation with me.&lt;/p&gt;
&lt;p&gt;Issue #27213: Rework CALL_FUNCTION* opcodes to produce shorter and more
efficient bytecode:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;CALL_FUNCTION&lt;/tt&gt; now only accepts positional arguments&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;CALL_FUNCTION_KW&lt;/tt&gt; accepts positional arguments and keyword arguments,
keys of keyword arguments are packed into a constant tuple.&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;CALL_FUNCTION_EX&lt;/tt&gt; is the most generic opcode: it expects a tuple and a
dict for positional and keyword arguments.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;CALL_FUNCTION_VAR&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;CALL_FUNCTION_VAR_KW&lt;/tt&gt; opcodes have been removed.&lt;/p&gt;
&lt;p&gt;Demur Rumed also implemented &amp;quot;Wordcode&amp;quot;, a new bytecode format using fixed
units of 16-bit: 8-bit opcode with 8-bit argument. Wordcode was merged in May
2016, see &lt;a class="reference external" href="http://bugs.python.org/issue26647"&gt;issue #26647: ceval: use Wordcode, 16-bit bytecode&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All instructions have an argument: opcodes without argument use the argument
&lt;tt class="docutils literal"&gt;0&lt;/tt&gt;. It allowed to remove the following conditional code in the very hot code
of &lt;tt class="docutils literal"&gt;Python/ceval.c&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
if (HAS_ARG(opcode))
    oparg = NEXTARG();
&lt;/pre&gt;
&lt;p&gt;The bytecode is now fetched using 16-bit words, instead of loading one or two
8-bit words per instruction.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="work-on-optimization"&gt;
&lt;h2&gt;Work on optimization&lt;/h2&gt;
&lt;p&gt;I continued with work on the &lt;a class="reference external" href="https://github.com/python/performance"&gt;performance&lt;/a&gt; Python benchmark suite. The suite
works on CPython and PyPy, but it's maybe not fine tuned for PyPy yet.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #27938: Add a fast-path for us-ascii encoding&lt;/li&gt;
&lt;li&gt;Issue #15369: Remove the (old version of) pybench microbenchmark. Please use
the new &amp;quot;performance&amp;quot; benchmark suite which includes a more recent version of
pybench.&lt;/li&gt;
&lt;li&gt;Issue #15369. Remove old and unreliable pystone microbenchmark. Please use
the new &amp;quot;performance&amp;quot; benchmark suite which is much more reliable.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="interesting-bug-hidden-resource-warnings"&gt;
&lt;h2&gt;Interesting bug: hidden resource warnings&lt;/h2&gt;
&lt;p&gt;At 2016-08-22, I started to investigate why &amp;quot;Warning -- xxx was modfied by
test_xxx&amp;quot; warnings were not logged on some buildbots (issue #27829).&lt;/p&gt;
&lt;p&gt;I modified the code logging the warning to flush immediatly stderr:
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;print(...,&lt;/span&gt; flush=True)&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;19 days later, I tried to remove a quiet flag &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-q&lt;/span&gt;&lt;/tt&gt; on the Windows build...
but it was a mistake, this flag doesn't mean quiet in the modified batch script
:-)&lt;/p&gt;
&lt;p&gt;13 days later, I finally understood that the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-W&lt;/span&gt;&lt;/tt&gt; option of regrtest was
eating stderr if the test pass but the environment was modified.&lt;/p&gt;
&lt;p&gt;I fixed regrtest to log stderr in all cases, except if the test pass! It should
now be easier to fix &amp;quot;environment changed&amp;quot; warnings emitted by regrtest.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="contributions"&gt;
&lt;h2&gt;Contributions&lt;/h2&gt;
&lt;p&gt;As usual, I reviewed and pushed changes written by other contributors:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #27350: I reviewed and pushed the implementation of compact
dictionaries preserving insertion order. This resulted in dictionaries using
20% to 25% less memory when compared to Python 3.5. The implementation was
written by &lt;strong&gt;INADA Naoki&lt;/strong&gt;, based on the PyPy implementation, with a design
by Raymond Hettinger.&lt;/li&gt;
&lt;li&gt;&amp;quot;make tags&amp;quot;: remove &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-t&lt;/span&gt;&lt;/tt&gt; option of &lt;tt class="docutils literal"&gt;ctags&lt;/tt&gt;. The option was kept for
backward compatibility, but it was completly removed recently. Patch written
by &lt;strong&gt;Stéphane Wirtel&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #27558: Fix a &lt;tt class="docutils literal"&gt;SystemError&lt;/tt&gt; in the implementation of &amp;quot;raise&amp;quot; statement.
In a brand new thread, raise a RuntimeError since there is no active
exception to reraise. Patch written by &lt;strong&gt;Xiang Zhang&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #28120: Fix &lt;tt class="docutils literal"&gt;dict.pop()&lt;/tt&gt; for splitted dictionary when trying to remove a
&amp;quot;pending key&amp;quot;: a key not yet inserted in split-table. Patch by &lt;strong&gt;Xiang
Zhang&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="bugfixes"&gt;
&lt;h2&gt;Bugfixes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;socket: Fix &lt;tt class="docutils literal"&gt;internal_select()&lt;/tt&gt; function. Bug found by &lt;strong&gt;Pavel Belikov&lt;/strong&gt;
(&amp;quot;Fragment N1&amp;quot;): &lt;a class="reference external" href="http://www.viva64.com/en/b/0414/#ID0ECDAE"&gt;http://www.viva64.com/en/b/0414/#ID0ECDAE&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;socket: use INVALID_SOCKET.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Replace &lt;tt class="docutils literal"&gt;fd = &lt;span class="pre"&gt;-1&lt;/span&gt;&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;fd = INVALID_SOCKET&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Replace &lt;tt class="docutils literal"&gt;fd &amp;lt; 0&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;fd == INVALID_SOCKET&lt;/tt&gt;:
SOCKET_T is unsigned on Windows&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Bug found by Pavel Belikov (&amp;quot;Fragment N1&amp;quot;):
&lt;a class="reference external" href="http://www.viva64.com/en/b/0414/#ID0ECDAE"&gt;http://www.viva64.com/en/b/0414/#ID0ECDAE&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #11048: ctypes, fix &lt;tt class="docutils literal"&gt;CThunkObject_new()&lt;/tt&gt;&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Initialize restype and flags fields to fix a crash when Python runs on a
read-only file system&lt;/li&gt;
&lt;li&gt;Use &lt;tt class="docutils literal"&gt;Py_ssize_t&lt;/tt&gt; type rather than &lt;tt class="docutils literal"&gt;int&lt;/tt&gt; for the &lt;tt class="docutils literal"&gt;i&lt;/tt&gt; iterator variable&lt;/li&gt;
&lt;li&gt;Reorder assignements to be able to more easily check if all fields are
initialized&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Initial patch written by &lt;strong&gt;Marcin Bachry&lt;/strong&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #27744: socket: Fix memory leak in &lt;tt class="docutils literal"&gt;sendmsg()&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;sendmsg_afalg()&lt;/tt&gt;.  Release &lt;tt class="docutils literal"&gt;msg.msg_iov&lt;/tt&gt; memory block. Release memory
on &lt;tt class="docutils literal"&gt;PyMem_Malloc(controllen)&lt;/tt&gt; failure&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #27866: ssl: Fix refleak in &lt;tt class="docutils literal"&gt;cipher_to_dict()&lt;/tt&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #28077: Fix dict type, &lt;tt class="docutils literal"&gt;find_empty_slot()&lt;/tt&gt; only supports combined
dictionaries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #28200: Fix memory leak in &lt;tt class="docutils literal"&gt;path_converter()&lt;/tt&gt;. Replace
&lt;tt class="docutils literal"&gt;PyUnicode_AsWideCharString()&lt;/tt&gt; &lt;tt class="docutils literal"&gt;with PyUnicode_AsUnicodeAndSize()&lt;/tt&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #27955: Catch permission error (&lt;tt class="docutils literal"&gt;EPERM&lt;/tt&gt;) in &lt;tt class="docutils literal"&gt;py_getrandom()&lt;/tt&gt;.
Fallback on reading from the &lt;tt class="docutils literal"&gt;/dev/urandom&lt;/tt&gt; device when the &lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt;
syscall fails with &lt;tt class="docutils literal"&gt;EPERM&lt;/tt&gt;, for example if blocked by SECCOMP.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #27778: Fix a memory leak in &lt;tt class="docutils literal"&gt;os.getrandom()&lt;/tt&gt; when the
&lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt; is interrupted by a signal and a signal handler raises a
Python exception.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #28233: Fix &lt;tt class="docutils literal"&gt;PyUnicode_FromFormatV()&lt;/tt&gt; error handling. Fix a memory
leak if the format string contains a non-ASCII character: destroy the unicode
writer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="regrtest-changes"&gt;
&lt;h2&gt;regrtest changes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;regrtest: rename &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--slow&lt;/span&gt;&lt;/tt&gt; option to &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--slowest&lt;/span&gt;&lt;/tt&gt; (to get same option name
than the &lt;tt class="docutils literal"&gt;testr&lt;/tt&gt; tool). Thanks to optparse, --slow syntax still works ;-)
Add --slowest option to buildbots. Display the top 10 slowest tests.&lt;/li&gt;
&lt;li&gt;regrtest: nicer output for durations. Use milliseconds and minutes units, not
only seconds.&lt;/li&gt;
&lt;li&gt;regrtest: Add a summary of the tests at the end of tests output:
&amp;quot;Tests result: xxx&amp;quot;. It was sometimes hard to check quickly if tests
succeeded, failed or something bad happened.&lt;/li&gt;
&lt;li&gt;regrtest: accept options after test names. For example, &lt;tt class="docutils literal"&gt;./python &lt;span class="pre"&gt;-m&lt;/span&gt; test
test_os &lt;span class="pre"&gt;-v&lt;/span&gt;&lt;/tt&gt; runs &lt;tt class="docutils literal"&gt;test_os&lt;/tt&gt; in verbose mode. Before, regrtest tried to run
a test called &amp;quot;-v&amp;quot;!&lt;/li&gt;
&lt;li&gt;Issue #28195: Fix &lt;tt class="docutils literal"&gt;test_huntrleaks_fd_leak()&lt;/tt&gt; of test_regrtest. Don't expect
the fd leak message to be on a specific line number, just make sure that the
line is present in the output.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example of a recent (2017-02-15) successful test run, truncated output:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
...
0:08:20 [403/404] test_codecs passed
0:08:21 [404/404] test_threading passed
391 tests OK.

10 slowest tests:
- test_multiprocessing_spawn: 1 min 24 sec
- test_concurrent_futures: 1 min 3 sec
- test_multiprocessing_forkserver: 60 sec
...

13 tests skipped:
    test_devpoll test_ioctl test_kqueue ...

Total duration: 8 min 22 sec
Tests result: SUCCESS
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="tests-changes"&gt;
&lt;h2&gt;Tests changes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;script_helper: kill the subprocess on error. If Popen.communicate() raises an
exception, kill the child process to not leave a running child process in
background and maybe create a zombi process. This change fixes a
ResourceWarning in Python 3.6 when unit tests are interrupted by CTRL+c.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #27181: Skip test_statistics tests known to fail until a fix is found.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #18401: Fix test_pdb if $HOME is not set. HOME is not set on Windows
for example.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;test_eintr: Fix &lt;tt class="docutils literal"&gt;ResourceWarning&lt;/tt&gt; warnings&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Buildbot: give 20 minute per test file. It seems like at least 2 buildbots
need more than 15 minutes per test file.  Example with &amp;quot;AMD64 Snow Leop 3.x&amp;quot;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
10 slowest tests:
- test_tools: 14 min 40 sec
- test_tokenize: 11 min 57 sec
- test_datetime: 11 min 25 sec
- ...
&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #28176: test_asynico: fix test_sock_connect_sock_write_race(), increase
the timeout from 10 seconds to 60 seconds.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="other-changes"&gt;
&lt;h2&gt;Other changes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #22624: Python 3 now requires the &lt;tt class="docutils literal"&gt;clock()&lt;/tt&gt; function to build to
simplify the C code.&lt;/li&gt;
&lt;li&gt;Issue #27404: tag security related changes with the &amp;quot;[Security]&amp;quot; prefix in
the changelog Misc/NEWS.&lt;/li&gt;
&lt;li&gt;Issue #27776: &lt;tt class="docutils literal"&gt;dev_urandom(raise=0)&lt;/tt&gt; now closes the file descriptor on error&lt;/li&gt;
&lt;li&gt;Issue #27128, #18295: Use &lt;tt class="docutils literal"&gt;Py_ssize_t&lt;/tt&gt; in &lt;tt class="docutils literal"&gt;_PyEval_EvalCodeWithName()&lt;/tt&gt;.
Replace &lt;tt class="docutils literal"&gt;int&lt;/tt&gt; type with &lt;tt class="docutils literal"&gt;Py_ssize_t&lt;/tt&gt; for index variables used for
positional arguments.  It should help to avoid integer overflow and help to
emit better machine code for &lt;tt class="docutils literal"&gt;i++&lt;/tt&gt; (no trap needed for overflow). Make also
the &lt;tt class="docutils literal"&gt;total_args&lt;/tt&gt; variable constant.&lt;/li&gt;
&lt;li&gt;Fix &amp;quot;make tags&amp;quot;: set locale to C to call sort. vim expects that the tags file
is sorted using english collation, so it fails if the locale is french for
example. Use LC_ALL=C to force english sorting order. Issue #27726.&lt;/li&gt;
&lt;li&gt;Issue #27698: Add &lt;tt class="docutils literal"&gt;socketpair&lt;/tt&gt; function to &lt;tt class="docutils literal"&gt;socket.__all__&lt;/tt&gt; on Windows&lt;/li&gt;
&lt;li&gt;Issue #27786: Simplify (optimize?) PyLongObject private function &lt;tt class="docutils literal"&gt;x_sub()&lt;/tt&gt;:
the &lt;tt class="docutils literal"&gt;z&lt;/tt&gt; variable is known to be a new object which cannot be shared,
&lt;tt class="docutils literal"&gt;Py_SIZE()&lt;/tt&gt; can be used directly to negate the number.&lt;/li&gt;
&lt;li&gt;Fix a clang warning in grammar.c. Clang is smarter than GCC and emits a
warning for dead code on a function declared with
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((__noreturn__))&lt;/span&gt;&lt;/tt&gt; (the &lt;tt class="docutils literal"&gt;Py_FatalError()&lt;/tt&gt; function in this
case).&lt;/li&gt;
&lt;li&gt;Issue #28114: Add unit tests on &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;os.spawn*()&lt;/span&gt;&lt;/tt&gt; to prepare to fix a crash
with bytes environment.&lt;/li&gt;
&lt;li&gt;Issue #28127: Add &lt;tt class="docutils literal"&gt;_PyDict_CheckConsistency()&lt;/tt&gt;: function checking that a
dictionary remains consistent after any change. By default, only basic
attributes are tested, table content is not checked because the impact on
Python performance is too important. &lt;tt class="docutils literal"&gt;DEBUG_PYDICT&lt;/tt&gt; must be defined (ex:
&lt;tt class="docutils literal"&gt;gcc &lt;span class="pre"&gt;-D&lt;/span&gt; DEBUG_PYDICT&lt;/tt&gt;) to check also dictionaries content.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>CPython sprint, september 2016</title><link href="https://haypo.github.io/cpython-sprint-2016.html" rel="alternate"></link><published>2017-02-14T18:00:00+01:00</published><updated>2017-02-14T18:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-02-14:/cpython-sprint-2016.html</id><summary type="html">&lt;p&gt;I was invited at my first CPython sprint in September! Five days, September
5-9, at Instagram office in California, USA. The sprint was sponsored by
Instagram, Microsoft, and the PSF.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First little game:&lt;/strong&gt; Many happy faces, but &lt;em&gt;Where is Victor?&lt;/em&gt;&lt;/p&gt;
&lt;a class="reference external image-reference" href="http://blog.python.org/2016/09/python-core-development-sprint-2016-36.html"&gt;&lt;img alt="CPython developers at the Facebook sprint" src="https://haypo.github.io/images/cpython_sprint_2016_photo.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;IMHO it was the most productive CPython week ever :-) Having …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was invited at my first CPython sprint in September! Five days, September
5-9, at Instagram office in California, USA. The sprint was sponsored by
Instagram, Microsoft, and the PSF.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;First little game:&lt;/strong&gt; Many happy faces, but &lt;em&gt;Where is Victor?&lt;/em&gt;&lt;/p&gt;
&lt;a class="reference external image-reference" href="http://blog.python.org/2016/09/python-core-development-sprint-2016-36.html"&gt;&lt;img alt="CPython developers at the Facebook sprint" src="https://haypo.github.io/images/cpython_sprint_2016_photo.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;IMHO it was the most productive CPython week ever :-) Having Guido van Rossum
in a room helped to get many PEPs accepted. Having a lot of highly skilled
reviewers in the same room helped to get many new features and many PEP
implementations merged much faster than usual.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Second little game:&lt;/strong&gt; try to spot the sprint on the CPython commit statistics of
the last 12 months (Feb, 2016-Feb, 2017) ;-)&lt;/p&gt;
&lt;a class="reference external image-reference" href="https://github.com/python/cpython/graphs/commit-activity"&gt;&lt;img alt="CPython commits statistics" src="https://haypo.github.io/images/cpython_sprint_2016_commits.png" /&gt;&lt;/a&gt;
&lt;div class="section" id="compact-dict"&gt;
&lt;h2&gt;Compact dict&lt;/h2&gt;
&lt;p&gt;Issue #27350: I reviewed and pushed the &amp;quot;compact dict&amp;quot; implementation which
makes Python dictionaries ordered (by insertion order) by default. It reduces
the memory usage of dictionaries betwen 20% and 25%.&lt;/p&gt;
&lt;p&gt;The implementation was written by INADA Naoki, based on the PyPy
implementation, with a design by Raymond Hettinger.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fastcall"&gt;
&lt;h2&gt;FASTCALL&lt;/h2&gt;
&lt;p&gt;&amp;quot;Fast calls&amp;quot;: Python 3.6 has a new private C API and a new METH_FASTCALL
calling convention which avoids temporary tuple for positional arguments and
avoids temporary dictionary for keyword arguments. Changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Add a new C calling convention: METH_FASTCALL&lt;/li&gt;
&lt;li&gt;Add _PyArg_ParseStack() function&lt;/li&gt;
&lt;li&gt;Add _PyCFunction_FastCallKeywords() function: issue #27810&lt;/li&gt;
&lt;li&gt;Add _PyObject_FastCallKeywords() function: issue #27830&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="more-efficient-call-function-bytecode"&gt;
&lt;h2&gt;More efficient CALL_FUNCTION bytecode&lt;/h2&gt;
&lt;p&gt;I reviewed and pushed: &amp;quot;Rework CALL_FUNCTION* opcodes to produce shorter and
more efficient bytecode&amp;quot; (issue #27213).&lt;/p&gt;
&lt;p&gt;Patch writen by Demur Rumed, design by Serhiy Storchaka, reviewed by Serhiy
Storchaka and me.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pep-509-add-a-private-version-to-dict"&gt;
&lt;h2&gt;PEP 509: Add a private version to dict&lt;/h2&gt;
&lt;p&gt;Guido approved my PEP 509 &amp;quot;Add a new private version to the builtin dict type&amp;quot;.&lt;/p&gt;
&lt;p&gt;I pushed the implementation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pep-524-make-os-urandom-blocking-on-linux"&gt;
&lt;h2&gt;PEP 524: Make os.urandom() blocking on Linux&lt;/h2&gt;
&lt;p&gt;I pushed the implementation of my PEP 524: &amp;quot;Make os.urandom() blocking on
Linux&amp;quot;.&lt;/p&gt;
&lt;p&gt;Issue #27776: The os.urandom() function does now block on Linux 3.17 and newer
until the system urandom entropy pool is initialized to increase the security.&lt;/p&gt;
&lt;p&gt;Read my previous blog post for the painful story behind the PEP:
&lt;a class="reference external" href="https://haypo.github.io/pep-524-os-urandom-blocking.html"&gt;PEP 524: os.urandom() now blocks on Linux&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="asynchronous-pep-525-and-530"&gt;
&lt;h2&gt;Asynchronous PEP 525 and 530&lt;/h2&gt;
&lt;p&gt;Guido van Rossum approved two PEPs of Yury Selivanov:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;PEP 525: Asynchronous Generators&lt;/li&gt;
&lt;li&gt;PEP 530: Asynchronous Comprehensions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I reviewed the huge C implementation with Yury on my side :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="unicode-escape-codec-optimization"&gt;
&lt;h2&gt;unicode_escape codec optimization&lt;/h2&gt;
&lt;p&gt;I reviewed and pushed &amp;quot;Optimize unicode_escape and raw_unicode_escape&amp;quot; (the
isue #16334), patch written by Serhiy Storchaka.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-6-bugfixes"&gt;
&lt;h2&gt;Python 3.6 bugfixes&lt;/h2&gt;
&lt;p&gt;I happily found many issues including a major one: regular list-comprehension
were completely broken :-)&lt;/p&gt;
&lt;p&gt;Another minor issue: SyntaxError didn't reported the correct line number in a
specific case.&lt;/p&gt;
&lt;p&gt;Don't worry, Yury fixed both ;-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="official-sprint-report"&gt;
&lt;h2&gt;Official sprint report&lt;/h2&gt;
&lt;p&gt;Read also the official report: &lt;a class="reference external" href="http://blog.python.org/2016/09/python-core-development-sprint-2016-36.html"&gt;Python Core Development Sprint 2016: 3.6 and
beyond!&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>PEP 524: os.urandom() now blocks on Linux in Python 3.6</title><link href="https://haypo.github.io/pep-524-os-urandom-blocking.html" rel="alternate"></link><published>2017-02-14T12:00:00+01:00</published><updated>2017-02-14T12:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-02-14:/pep-524-os-urandom-blocking.html</id><summary type="html">&lt;div class="section" id="getrandom-avoids-file-descriptors"&gt;
&lt;h2&gt;getrandom() avoids file descriptors&lt;/h2&gt;
&lt;p&gt;Last years, I'm making sometimes enhancements in the Python code used to
generate random numbers, the C implementation of &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt;. My main two
changes were to use the new &lt;tt class="docutils literal"&gt;getentropy()&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt; functions when
available on Linux, Solaris, OpenBSD, etc.&lt;/p&gt;
&lt;p&gt;In 2013, &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; opened …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;div class="section" id="getrandom-avoids-file-descriptors"&gt;
&lt;h2&gt;getrandom() avoids file descriptors&lt;/h2&gt;
&lt;p&gt;Last years, I'm making sometimes enhancements in the Python code used to
generate random numbers, the C implementation of &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt;. My main two
changes were to use the new &lt;tt class="docutils literal"&gt;getentropy()&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt; functions when
available on Linux, Solaris, OpenBSD, etc.&lt;/p&gt;
&lt;p&gt;In 2013, &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; opened a file descriptor to read from
&lt;tt class="docutils literal"&gt;/dev/urandom&lt;/tt&gt; and then closed it. It was decided to use a single private
file descriptor and keep it open to prevent &lt;tt class="docutils literal"&gt;EMFILE&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;ENFILE&lt;/tt&gt; errors
(too many open files) under high system loads with many threads: see the issue
#18756.&lt;/p&gt;
&lt;p&gt;The private file descriptor introduced a backward incompatible change in badly
written programs. The code was modified to call &lt;tt class="docutils literal"&gt;fstat()&lt;/tt&gt; to check if the
file descriptor was closed and then replaced with a different file descriptor
(but same number): check if &lt;tt class="docutils literal"&gt;st_dev&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;st_ino&lt;/tt&gt; attributes changed.&lt;/p&gt;
&lt;p&gt;In 2014, the new Linux kernel 3.17 added a new &lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt; syscall which
gives access to random bytes without having to handle a file descriptor. I
modified &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; to call &lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt; to avoid file descriptors,
but a different issue appeared.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="getrandom-hangs-at-system-startup"&gt;
&lt;h2&gt;getrandom() hangs at system startup&lt;/h2&gt;
&lt;p&gt;On embedded devices and virtual machines, Python 3.5 started to hang at
startup.&lt;/p&gt;
&lt;p&gt;On Debian, a systemd script used Python to compute a MD5 checksum, but Python
was blocked during its initialization. Other users reported that Python blocked
on importing the &lt;tt class="docutils literal"&gt;random&lt;/tt&gt; module, sometimes imported indirectly by a
different module.&lt;/p&gt;
&lt;p&gt;Python was blocked on the &lt;tt class="docutils literal"&gt;getrandom(0)&lt;/tt&gt; syscall, waiting until the system
collected enough entropy to initialize the urandom pool. It took longer than 90
seconds, so systemd killed the service with a timeout. As a consequence, the
system boot takes longer than 90 seconds or can even fail!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fix-python-startup"&gt;
&lt;h2&gt;Fix Python startup&lt;/h2&gt;
&lt;p&gt;The fix was obvious: call &lt;tt class="docutils literal"&gt;getrandom(GRND_NONBLOCK)&lt;/tt&gt; which fails immediately
if the call would block, and fall back on reading from &lt;tt class="docutils literal"&gt;/dev/urandom&lt;/tt&gt; which
doesn't block even if the entropy pool is not initialized yet.&lt;/p&gt;
&lt;p&gt;Quickly, our security experts complained that falling back on &lt;tt class="docutils literal"&gt;/dev/urandom&lt;/tt&gt;
makes Python less secure. When the fall back path is taken, &lt;tt class="docutils literal"&gt;/dev/urandom&lt;/tt&gt;
returns random number not suitable for security purpose (initialized with low
entropy), wheras &lt;a class="reference external" href="https://docs.python.org/dev/library/os.html#os.urandom"&gt;os.urandom() documenation&lt;/a&gt; says: &amp;quot;The returned
data should be unpredictable enough for cryptographic applications&amp;quot; (and
&amp;quot;though its exact quality depends on the OS implementation.&amp;quot;).&lt;/p&gt;
&lt;p&gt;Calling &lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt; in blocking mode for &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; makes Python more
secure, but it doesn't fix the startup bug.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="discussion-storm"&gt;
&lt;h2&gt;Discussion storm&lt;/h2&gt;
&lt;p&gt;The proposed change started a huge rain of messages. More than 200 messages,
maybe even more than 500 messages, on the bug tracker and python-dev mailing
list. Everyone became a security expert and wanted to give his/her very
important opinion, without listening to other arguments.&lt;/p&gt;
&lt;p&gt;Two Python security experts left the discussion.&lt;/p&gt;
&lt;p&gt;I also ignored new messages. I simply had not enough time to read all of them,
and the discussion tone made me angry.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="new-mailing-list-and-two-new-peps"&gt;
&lt;h2&gt;New mailing list and two new PEPs&lt;/h2&gt;
&lt;p&gt;A new &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;security-sig&lt;/span&gt;&lt;/tt&gt; mailing list, subtitled &amp;quot;os.urandom rehab clinic&amp;quot;, was
created just to take a decision on &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt;!&lt;/p&gt;
&lt;p&gt;Nick Coghlan wrote the &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0522/"&gt;PEP 522: Allow BlockingIOError in security sensitive
APIs&lt;/a&gt;. Basically: he considers
that there is no good default behaviour when &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; would block, so
raise an exception to let users decide.&lt;/p&gt;
&lt;p&gt;I wrote  &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0524/"&gt;PEP 524: Make os.urandom() blocking on Linux&lt;/a&gt;. My PEP proposes to make
&lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; blocking, &lt;em&gt;but&lt;/em&gt; also modify Python startup to fall back on
non-blocking RNG to initialize the secret hash seed and the &lt;tt class="docutils literal"&gt;random&lt;/tt&gt; module
(which is &lt;em&gt;not&lt;/em&gt; sensitive for security, except of &lt;tt class="docutils literal"&gt;random.SystemRandom&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;Nick's PEP describes an important use case: be able to check if
&lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; would block. Instead of adding a flag to &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt;,
I chose to expose the low-level C
&lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt; function as a new Python &lt;tt class="docutils literal"&gt;os.getrandom()&lt;/tt&gt; function. Calling
&lt;tt class="docutils literal"&gt;os.getrandom(1, os.GRND_NONBLOCK)&lt;/tt&gt; raises a &lt;tt class="docutils literal"&gt;BlockingIOError&lt;/tt&gt; exception,
as Nick proposed for &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt;, so it's possible to decide what to do in
this case.&lt;/p&gt;
&lt;p&gt;While both PEPs are valid, IMHO my PEP was &lt;em&gt;less&lt;/em&gt; backward incompatible,
simpler and maybe closer to what users &lt;em&gt;expect&lt;/em&gt;. The &amp;quot;os.urandom() would block&amp;quot;
case is a special case with my PEP, but my PEP allows to decide what to do in
that case (thanks to &lt;tt class="docutils literal"&gt;os.getrandom()&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;Guido van Rossum approved my PEP and rejected Nick's PEP. I worked with Nick to
implement my PEP.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-6-changes"&gt;
&lt;h2&gt;Python 3.6 changes&lt;/h2&gt;
&lt;p&gt;I added a new &lt;tt class="docutils literal"&gt;os.getrandom()&lt;/tt&gt; function: expose the Linux
&lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt; syscall (issue #27778). I also added the two getrandom() flags:
&lt;tt class="docutils literal"&gt;os.GRND_NONBLOCK&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;os.GRND_RANDOM&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I modified &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; to block on Linux: call &lt;tt class="docutils literal"&gt;getrandom(0)&lt;/tt&gt;
instead of &lt;tt class="docutils literal"&gt;getrandom(GRND_NONBLOCK)&lt;/tt&gt; (issue #27776).&lt;/p&gt;
&lt;p&gt;I also added a private &lt;tt class="docutils literal"&gt;_PyOS_URandomNonblock()&lt;/tt&gt; function used to initialize
the hash secret and used by &lt;tt class="docutils literal"&gt;random.Random.seed()&lt;/tt&gt; (used to initialize the
&lt;tt class="docutils literal"&gt;random&lt;/tt&gt; module).&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; function now blocks in Python 3.6 on Linux 3.17 and newer
until the system urandom entropy pool is initialized to increase the security.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="read-also-lwn-articles"&gt;
&lt;h2&gt;Read also LWN articles&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://lwn.net/Articles/606141/"&gt;A system call for random numbers: getrandom()&lt;/a&gt; (July 2014)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://lwn.net/Articles/693189/"&gt;Python's os.urandom() in the absence of entropy&lt;/a&gt; (July 2016) -- this story&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://lwn.net/Articles/711013/"&gt;The long road to getrandom() in glibc&lt;/a&gt; (January 2017)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="cpython"></category><category term="security"></category></entry><entry><title>My contributions to CPython during 2016 Q2</title><link href="https://haypo.github.io/contrib-cpython-2016q2.html" rel="alternate"></link><published>2017-02-12T18:00:00+01:00</published><updated>2017-02-12T18:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-02-12:/contrib-cpython-2016q2.html</id><summary type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2016 Q2
(april, may, june):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg log -r 'date(&amp;quot;2016-04-01&amp;quot;):date(&amp;quot;2016-06-30&amp;quot;)' --no-merges -u Stinner
&lt;/pre&gt;
&lt;p&gt;Statistics: 52 non-merge commits + 22 merge commits (total: 74 commits).&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q1.html"&gt;My contributions to CPython during 2016 Q1&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q3.html"&gt;My contributions to
CPython during 2016 Q3&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="start-of-my-work-on-optimization"&gt;
&lt;h2&gt;Start of …&lt;/h2&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2016 Q2
(april, may, june):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg log -r 'date(&amp;quot;2016-04-01&amp;quot;):date(&amp;quot;2016-06-30&amp;quot;)' --no-merges -u Stinner
&lt;/pre&gt;
&lt;p&gt;Statistics: 52 non-merge commits + 22 merge commits (total: 74 commits).&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q1.html"&gt;My contributions to CPython during 2016 Q1&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q3.html"&gt;My contributions to
CPython during 2016 Q3&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="start-of-my-work-on-optimization"&gt;
&lt;h2&gt;Start of my work on optimization&lt;/h2&gt;
&lt;p&gt;During 2016 Q2, I started to spend more time on optimizing CPython.&lt;/p&gt;
&lt;p&gt;I experimented a change on CPython: a new FASTCALL calling convention to avoid
the creation of a temporary tuple to pass positional argulments: &lt;a class="reference external" href="http://bugs.python.org/issue26814"&gt;issue26814&lt;/a&gt;. Early results were really good: calling
builtin functions became between 20% and 50% faster!&lt;/p&gt;
&lt;p&gt;Quickly, my optimization work was blocked by unreliable benchmarks. I spent the
rest of the year 2016 analyzing benchmarks and making benchmarks more stable.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="subprocess-now-emits-resourcewarning"&gt;
&lt;h2&gt;subprocess now emits ResourceWarning&lt;/h2&gt;
&lt;p&gt;subprocess.Popen destructor now emits a ResourceWarning warning if the child
process is still running (issue #26741). The warning helps to track and fix
zombi processes. I updated asyncio to prevent a false ResourceWarning (warning
whereas the child process completed): asyncio now copies the child process exit
status to the internal Popen object.&lt;/p&gt;
&lt;p&gt;I also fixed the POSIX implementation of subprocess.Popen._execute_child(): it
now sets the returncode attribute from the child process exit status when exec
failed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="security-fix-potential-shell-injections-in-ctypes-util"&gt;
&lt;h2&gt;Security: fix potential shell injections in ctypes.util&lt;/h2&gt;
&lt;p&gt;I rewrote methods of the ctypes.util module using &lt;tt class="docutils literal"&gt;os.popen()&lt;/tt&gt;. I replaced
&lt;tt class="docutils literal"&gt;os.popen()&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;subprocess.Popen&lt;/tt&gt; without shell (issue #22636) to fix a
class of security vulneratiblity, &amp;quot;shell injection&amp;quot; (inject arbitrary shell
commands to take the control of a computer).&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;os.popen()&lt;/tt&gt; function uses a shell, so there is a risk if the command
line arguments are not properly escaped for shell. Using &lt;tt class="docutils literal"&gt;subproces.Popen&lt;/tt&gt;
without shell fixes completely the risk.&lt;/p&gt;
&lt;p&gt;Note: the &lt;tt class="docutils literal"&gt;ctypes&lt;/tt&gt; is generally not considered as &amp;quot;safe&amp;quot;, but it doesn't harm
to make it more secure ;-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="optimization-pymem-malloc-now-uses-pymalloc"&gt;
&lt;h2&gt;Optimization: PyMem_Malloc() now uses pymalloc&lt;/h2&gt;
&lt;p&gt;PyMem_Malloc() now uses the fast Python &amp;quot;pymalloc&amp;quot; memory allocator which is
optimized for small objects with a short lifetime (issue #26249). The change
makes some benchmarks up to 4% faster.&lt;/p&gt;
&lt;p&gt;This change was possible thanks to the whole preparation work I did in the 2016
Q1, especially the new GIL check in memory allocator debug hooks and the new
&lt;tt class="docutils literal"&gt;PYTHONMALLOC=debug&lt;/tt&gt; environment variable enabling these hooks on a Python
compiled in released mode.&lt;/p&gt;
&lt;p&gt;I tested lxml, Pillow, cryptography and numpy before pushing the change,
as asked by Marc-Andre Lemburg. All these projects work with the change, except
of numpy. I wrote a fix for numpy: &lt;a class="reference external" href="https://github.com/numpy/numpy/pull/7404"&gt;Use PyMem_RawMalloc on Python 3.4 and newer&lt;/a&gt;, merged one month later (my first
contribution to numy!).&lt;/p&gt;
&lt;p&gt;The change indirectly helped to identify and fix a memory leak in the
&lt;tt class="docutils literal"&gt;formatfloat()&lt;/tt&gt; function used to format bytes strings: &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;b&amp;quot;%f&amp;quot;&lt;/span&gt; % 1.2&lt;/tt&gt; (issue
#25349, #26249).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="optimization"&gt;
&lt;h2&gt;Optimization&lt;/h2&gt;
&lt;p&gt;Issue #27056: Optimize pickle.load() and pickle.loads(), up to 10% faster to
deserialize a lot of small objects. I found this optimization using Linux perf
on Python compiled with PGO. My change implements manually the optimization if
Python is not compiled with PGO.&lt;/p&gt;
&lt;p&gt;Issue #26770: When &lt;tt class="docutils literal"&gt;set_inheritable()&lt;/tt&gt; is implemented with &lt;tt class="docutils literal"&gt;fcntl()&lt;/tt&gt;, don't
call &lt;tt class="docutils literal"&gt;fcntl()&lt;/tt&gt; twice if the &lt;tt class="docutils literal"&gt;FD_CLOEXEC&lt;/tt&gt; flag is already set to the
requested value. Linux uses &lt;tt class="docutils literal"&gt;ioctl()&lt;/tt&gt; and so always only need a single
syscall.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="changes"&gt;
&lt;h2&gt;Changes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #26716: Replace IOError with OSError in fcntl documentation, IOError is
a deprecated alias to OSError since Python 3.3.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #26639: Replace the deprecated &lt;tt class="docutils literal"&gt;imp&lt;/tt&gt; module with the &lt;tt class="docutils literal"&gt;importlib&lt;/tt&gt;
module in &lt;tt class="docutils literal"&gt;Tools/i18n/pygettext.py&lt;/tt&gt;. Remove &lt;tt class="docutils literal"&gt;_get_modpkg_path()&lt;/tt&gt;,
replaced with &lt;tt class="docutils literal"&gt;importlib.util.find_spec()&lt;/tt&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #26735: Fix os.urandom() on Solaris 11.3 and newer when reading more
than 1024 bytes: call getrandom() multiple times with a limit of 1024 bytes
per call.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;configure: fix &lt;tt class="docutils literal"&gt;HAVE_GETRANDOM_SYSCALL&lt;/tt&gt; check, syscall() function requires
&lt;tt class="docutils literal"&gt;#include &amp;lt;unistd.h&amp;gt;&lt;/tt&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #26766: Fix _PyBytesWriter_Finish(). Return a bytearray object when
bytearray is requested and when the small buffer is used. Fix also
test_bytes: bytearray%args must return a bytearray type.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #26777: Fix random failure of test_asyncio.test_timeout_disable() on
the &amp;quot;AMD64 FreeBSD 9.x 3.5&amp;quot; buildbot:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
File &amp;quot;.../Lib/test/test_asyncio/test_tasks.py&amp;quot;, line 2398, in go
  self.assertTrue(0.09 &amp;lt; dt &amp;lt; 0.11, dt)
AssertionError: False is not true : 0.11902812402695417
&lt;/pre&gt;
&lt;p&gt;Replace &lt;tt class="docutils literal"&gt;&amp;lt; 0.11&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;&amp;lt; 0.15&lt;/tt&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Backport test_gdb fix for s390x buildbots to Python 3.5.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Cleanup import.c: replace &lt;tt class="docutils literal"&gt;PyUnicode_RPartition()&lt;/tt&gt; with
&lt;tt class="docutils literal"&gt;PyUnicode_FindChar()&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;PyUnicode_Substring()&lt;/tt&gt; to avoid the creation
of a temporary tuple. Use &lt;tt class="docutils literal"&gt;PyUnicode_FromFormat()&lt;/tt&gt; to build a string and
avoid the single_dot ('.') singleton.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;regrtest now uses subprocesses when the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-j1&lt;/span&gt;&lt;/tt&gt; command line option is used:
each test file runs in a fresh child process. Before, the -j1 option was
ignored. &lt;tt class="docutils literal"&gt;Tools/buildbot/test.bat&lt;/tt&gt; script now uses -j1 by default to run
each test file in fresh child process.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;regrtest: display test result (passed, failed, ...) after each test
completion. In multiprocessing mode: always display the result. In sequential
mode: only display the result if the test did not pass&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p class="first"&gt;Issue #27278: Fix &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; implementation using &lt;tt class="docutils literal"&gt;getrandom()&lt;/tt&gt; on
Linux. Truncate size to &lt;tt class="docutils literal"&gt;INT_MAX&lt;/tt&gt; and loop until we collected enough random
bytes, instead of casting a directly &lt;tt class="docutils literal"&gt;Py_ssize_t&lt;/tt&gt; to &lt;tt class="docutils literal"&gt;int&lt;/tt&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="contributions"&gt;
&lt;h2&gt;Contributions&lt;/h2&gt;
&lt;p&gt;I also pushed a few changes written by other contributors.&lt;/p&gt;
&lt;p&gt;Issue #26839: &lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; doesn't block on Linux anymore. On Linux,
&lt;tt class="docutils literal"&gt;os.urandom()&lt;/tt&gt; now calls getrandom() with &lt;tt class="docutils literal"&gt;GRND_NONBLOCK&lt;/tt&gt; to fall back on
reading &lt;tt class="docutils literal"&gt;/dev/urandom&lt;/tt&gt; if the urandom entropy pool is not initialized yet.
Patch written by &lt;strong&gt;Colm Buckley&lt;/strong&gt;. This issue started a huge annoying discussion
around random number generation on the bug tracker and the python-dev mailing
list.  I later wrote the &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0524/"&gt;PEP 524: Make os.urandom() blocking on Linux&lt;/a&gt; to fix the issue!&lt;/p&gt;
&lt;p&gt;Other changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #26647: Cleanup opcode: simplify code to build &lt;tt class="docutils literal"&gt;opcode.opname&lt;/tt&gt;. Patch
written by &lt;strong&gt;Demur Rumed&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #26647: Cleanup modulefinder: use &lt;tt class="docutils literal"&gt;dis.opmap[name]&lt;/tt&gt; rather than
&lt;tt class="docutils literal"&gt;dis.opname.index(name)&lt;/tt&gt;. Patch written by &lt;strong&gt;Demur Rumed&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #26801: Fix error handling in &lt;tt class="docutils literal"&gt;shutil.get_terminal_size()&lt;/tt&gt;: catch
AttributeError instead of NameError. Skip the functional test of test_shutil
using the &lt;tt class="docutils literal"&gt;stty size&lt;/tt&gt; command if the &lt;tt class="docutils literal"&gt;os.get_terminal_size()&lt;/tt&gt; function is
missing. Patch written by &lt;strong&gt;Emanuel Barry&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #26802: Optimize function calls only using unpacking like
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;func(*tuple)&lt;/span&gt;&lt;/tt&gt; (no other positional argument, no keyword argument): avoid
copying the tuple. Patch written by &lt;strong&gt;Joe Jevnik&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #21668: Add missing libm dependency in setup.py: link audioop,
_datetime, _ctypes_test modules to libm, except on Mac OS X. Patch written by
&lt;strong&gt;Chi Hsuan Yen&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #26799: Fix python-gdb.py: don't get C types at startup, only on
demand. The C types can change if python-gdb.py is loaded before loading the
Python executable in gdb. Patch written by &lt;strong&gt;Thomas Ilsche&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #27057: Fix os.set_inheritable() on Android, ioctl() is blocked by
SELinux and fails with EACCESS. The function now falls back to fcntl(). Patch
written by &lt;strong&gt;Michał Bednarski&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #26647: Fix typo in test_grammar. Patch written by &lt;strong&gt;Demur Rumed&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2016 Q1</title><link href="https://haypo.github.io/contrib-cpython-2016q1.html" rel="alternate"></link><published>2017-02-09T17:00:00+01:00</published><updated>2017-02-09T17:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2017-02-09:/contrib-cpython-2016q1.html</id><summary type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2016 Q1
(january, februrary, march):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg log -r 'date(&amp;quot;2016-01-01&amp;quot;):date(&amp;quot;2016-03-31&amp;quot;)' --no-merges -u Stinner
&lt;/pre&gt;
&lt;p&gt;Statistics: 196 non-merge commits + 33 merge commits (total: 229 commits).&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2015q4.html"&gt;My contributions to CPython during 2015 Q4&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q2.html"&gt;My contributions to
CPython during 2016 Q2&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Since …&lt;/p&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2016 Q1
(january, februrary, march):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg log -r 'date(&amp;quot;2016-01-01&amp;quot;):date(&amp;quot;2016-03-31&amp;quot;)' --no-merges -u Stinner
&lt;/pre&gt;
&lt;p&gt;Statistics: 196 non-merge commits + 33 merge commits (total: 229 commits).&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2015q4.html"&gt;My contributions to CPython during 2015 Q4&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q2.html"&gt;My contributions to
CPython during 2016 Q2&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;Since this report is much longer than I expected, here are the highlights:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Python 8: no pep8, no chocolate!&lt;/li&gt;
&lt;li&gt;AST enhancements coming from FAT Python&lt;/li&gt;
&lt;li&gt;faulthandler now catchs Windows fatal exceptions&lt;/li&gt;
&lt;li&gt;New PYTHONMALLOC environment variable&lt;/li&gt;
&lt;li&gt;tracemalloc: new C API and support multiple address spaces&lt;/li&gt;
&lt;li&gt;ResourceWarning warnings now come with a traceback&lt;/li&gt;
&lt;li&gt;PyMem_Malloc() now fails if the GIL is not held&lt;/li&gt;
&lt;li&gt;Interesting bug: reentrant flag in tracemalloc&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="python-8-no-pep8-no-chocolate"&gt;
&lt;h2&gt;Python 8: no pep8, no chocolate!&lt;/h2&gt;
&lt;p&gt;I prepared an April Fool: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2016-March/143603.html"&gt;[Python-Dev] The next major Python version will be
Python 8&lt;/a&gt; :-)&lt;/p&gt;
&lt;p&gt;I increased Python version to 8, added the &lt;tt class="docutils literal"&gt;pep8&lt;/tt&gt; module and modified
&lt;tt class="docutils literal"&gt;importlib&lt;/tt&gt; to raise an &lt;tt class="docutils literal"&gt;ImportError&lt;/tt&gt; if a module is not PEP8-compliant!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="ast-enhancements-coming-from-fat-python"&gt;
&lt;h2&gt;AST enhancements coming from FAT Python&lt;/h2&gt;
&lt;p&gt;Changes coming from my &lt;a class="reference external" href="http://faster-cpython.readthedocs.io/fat_python.html"&gt;FAT Python&lt;/a&gt; (AST optimizer, run
ahead of time):&lt;/p&gt;
&lt;p&gt;The compiler now ignores constant statements like &lt;tt class="docutils literal"&gt;b'bytes'&lt;/tt&gt; (issue #26204).
I had to replace constant statement with expressions to prepare the change (ex:
replace &lt;tt class="docutils literal"&gt;b'bytes'&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;x = b'bytes'&lt;/tt&gt;). First, the compiler emited a
&lt;tt class="docutils literal"&gt;SyntaxWarning&lt;/tt&gt;, but it was quickly decided to let linters to emit such
warnings to not annoy users: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2016-February/143163.html"&gt;read the thread on python-dev&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Example, Python 3.5:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
&amp;gt;&amp;gt;&amp;gt; def f():
...  b'bytes'
...
&amp;gt;&amp;gt;&amp;gt; import dis; dis.dis(f)
  2           0 LOAD_CONST               1 (b'bytes')
              3 POP_TOP
              4 LOAD_CONST               0 (None)
              7 RETURN_VALUE
&lt;/pre&gt;
&lt;p&gt;Python 3.6:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
&amp;gt;&amp;gt;&amp;gt; def f():
...  b'bytes'
...
&amp;gt;&amp;gt;&amp;gt; import dis; dis.dis(f)
  1           0 LOAD_CONST               0 (None)
              2 RETURN_VALUE
&lt;/pre&gt;
&lt;p&gt;Other changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #26107: The format of the co_lnotab attribute of code objects changes
to support negative line number delta. It allows AST optimizers to move
instructions without breaking Python tracebacks. Change needed by the loop
unrolling optimization of FAT Python.&lt;/li&gt;
&lt;li&gt;Issue #26146: Add a new kind of AST node: &lt;tt class="docutils literal"&gt;ast.Constant&lt;/tt&gt;. It can be used by
external AST optimizers like FAT Python, but the compiler does not emit
directly such node. Update code to accept ast.Constant instead of ast.Num
and/or ast.Str.&lt;/li&gt;
&lt;li&gt;Issue #26146: &lt;tt class="docutils literal"&gt;marshal.loads()&lt;/tt&gt; now uses the empty frozenset singleton. It
fixes a test failure in FAT Python and reduces the memory footprint.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="faulthandler-now-catchs-windows-fatal-exceptions"&gt;
&lt;h2&gt;faulthandler now catchs Windows fatal exceptions&lt;/h2&gt;
&lt;p&gt;I enhanced the faulthandler.enable() function on Windows to set a
handler for Windows fatal exceptions using &lt;tt class="docutils literal"&gt;AddVectoredExceptionHandler()&lt;/tt&gt;
(issue #23848).&lt;/p&gt;
&lt;p&gt;Windows exceptions are the native way to handle fatal errors on Windows,
whereas UNIX signals SIGSEGV, SIGFPE and SIGABRT are &amp;quot;emulated&amp;quot; on top of that.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="new-pythonmalloc-environment-variable"&gt;
&lt;h2&gt;New PYTHONMALLOC environment variable&lt;/h2&gt;
&lt;p&gt;I added a new &lt;tt class="docutils literal"&gt;PYTHONMALLOC&lt;/tt&gt; environment variable (issue #26516) to set the
Python memory allocators.&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;PYTHONMALLOC=debug&lt;/tt&gt; enables debug hooks on a Python compiled in release
mode, whereas Python 3.5 requires to recompile Python in debug mode. These
hooks implements various checks:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Detect &lt;strong&gt;buffer underflow&lt;/strong&gt;: write before the start of the buffer&lt;/li&gt;
&lt;li&gt;Detect &lt;strong&gt;buffer overflow&lt;/strong&gt;: write after the end of the buffer&lt;/li&gt;
&lt;li&gt;Detect API violations, ex: &lt;tt class="docutils literal"&gt;PyObject_Free()&lt;/tt&gt; called on a buffer
allocated by &lt;tt class="docutils literal"&gt;PyMem_Malloc()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;Check if the GIL is held when allocator functions of PYMEM_DOMAIN_OBJ (ex:
&lt;tt class="docutils literal"&gt;PyObject_Malloc()&lt;/tt&gt;) and PYMEM_DOMAIN_MEM (ex: &lt;tt class="docutils literal"&gt;PyMem_Malloc()&lt;/tt&gt;) domains
are called&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Moreover, logging a fatal memory error now uses the tracemalloc module to get
the traceback where a memory block was allocated. Example of a buffer overflow
using &lt;tt class="docutils literal"&gt;python3.6 &lt;span class="pre"&gt;-X&lt;/span&gt; tracemalloc=5&lt;/tt&gt; (store 5 frames in traces):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Debug memory block at address p=0x7fbcd41666f8: API 'o'
    4 bytes originally requested
    The 7 pad bytes at p-7 are FORBIDDENBYTE, as expected.
    The 8 pad bytes at tail=0x7fbcd41666fc are not all FORBIDDENBYTE (0xfb):
        at tail+0: 0x02 *** OUCH
        at tail+1: 0xfb
        at tail+2: 0xfb
        ...
    The block was made by call #1233329 to debug malloc/realloc.
    Data at p: 1a 2b 30 00

Memory block allocated at (most recent call first):
  File &amp;quot;test/test_bytes.py&amp;quot;, line 323
  File &amp;quot;unittest/case.py&amp;quot;, line 600
  ...

Fatal Python error: bad trailing pad byte

Current thread 0x00007fbcdbd32700 (most recent call first):
  File &amp;quot;test/test_bytes.py&amp;quot;, line 323 in test_hex
  File &amp;quot;unittest/case.py&amp;quot;, line 600 in run
  ...
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;PYTHONMALLOC=malloc&lt;/tt&gt; forces the usage of the system &lt;tt class="docutils literal"&gt;malloc()&lt;/tt&gt; allocator.
This option can be used with Valgrind. Without this option, Valgrind emits tons
of false alarms in the Python &lt;tt class="docutils literal"&gt;pymalloc&lt;/tt&gt; memory allocator.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="tracemalloc-new-c-api-and-support-multiple-address-spaces"&gt;
&lt;h2&gt;tracemalloc: new C API and support multiple address spaces&lt;/h2&gt;
&lt;p&gt;Antoine Pitrou and Nathaniel Smith asked me to enhance the tracemalloc module:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Add a C API to be able to manually track/untrack memory blocks, to track
the memory allocated by custom memory allocators. For example, numpy uses
allocators with a specific memory alignment for SIMD instructions.&lt;/li&gt;
&lt;li&gt;Support tracking memory of different address spaces. For example, central
(CPU) memory and GPU memory for numpy.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="section" id="support-multiple-address-spaces"&gt;
&lt;h3&gt;Support multiple address spaces&lt;/h3&gt;
&lt;p&gt;I made deep changes in the &lt;tt class="docutils literal"&gt;hashtable.c&lt;/tt&gt; code (simple C implementation of an
hash table used by &lt;tt class="docutils literal"&gt;_tracemalloc&lt;/tt&gt;) to support keys of a variable size (issue
#26588), instead of using an hardcoded &lt;tt class="docutils literal"&gt;void *&lt;/tt&gt; size. It allows to support
keys larger than &lt;tt class="docutils literal"&gt;sizeof(void*)&lt;/tt&gt;, but also to use &lt;em&gt;less&lt;/em&gt; memory for keys
smaller than &lt;tt class="docutils literal"&gt;sizeof(void*)&lt;/tt&gt; (ex: &lt;tt class="docutils literal"&gt;int&lt;/tt&gt; keys).&lt;/p&gt;
&lt;p&gt;Then I extended the C &lt;tt class="docutils literal"&gt;_tracemalloc&lt;/tt&gt; module and the Python &lt;tt class="docutils literal"&gt;tracemalloc&lt;/tt&gt; to
add a new &lt;tt class="docutils literal"&gt;domain&lt;/tt&gt; attribute to traces: add &lt;tt class="docutils literal"&gt;Trace.domain&lt;/tt&gt; attribute and
&lt;tt class="docutils literal"&gt;tracemalloc.DomainFilter&lt;/tt&gt; class.&lt;/p&gt;
&lt;p&gt;The final step was to optimize the memory footprint of _tracemalloc. Start with
compact keys (&lt;tt class="docutils literal"&gt;Py_uintptr_t&lt;/tt&gt; type) and only switch to &lt;tt class="docutils literal"&gt;pointer_t&lt;/tt&gt; keys when
the first memory block with a non-zero domain is tracked (when one more one
address space is used). So the &lt;tt class="docutils literal"&gt;_tracemalloc&lt;/tt&gt; memory usage doesn't change by
default in Python 3.6!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="c-api"&gt;
&lt;h3&gt;C API&lt;/h3&gt;
&lt;p&gt;I added a private C API (issue #26530):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
int _PyTraceMalloc_Track(_PyTraceMalloc_domain_t domain, Py_uintptr_t ptr, size_t size);
int _PyTraceMalloc_Untrack(_PyTraceMalloc_domain_t domain, Py_uintptr_t ptr);
&lt;/pre&gt;
&lt;p&gt;I waited for Antoine and Nathaniel feedback on this API, but the API remains
private in Python 3.6 since none reviewed it.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="resourcewarning-warnings-now-come-with-a-traceback"&gt;
&lt;h2&gt;ResourceWarning warnings now come with a traceback&lt;/h2&gt;
&lt;div class="section" id="final-result"&gt;
&lt;h3&gt;Final result&lt;/h3&gt;
&lt;p&gt;Before going to explain the long development of the feature, let's see an
example of the final result! Example with the script &lt;tt class="docutils literal"&gt;example.py&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
import warnings

def func():
    return open(__file__)

f = func()
f = None
&lt;/pre&gt;
&lt;p&gt;Output of the command &lt;tt class="docutils literal"&gt;python3.6 &lt;span class="pre"&gt;-Wd&lt;/span&gt; &lt;span class="pre"&gt;-X&lt;/span&gt; tracemalloc=5 example.py&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
example.py:7: ResourceWarning: unclosed file &amp;lt;_io.TextIOWrapper name='example.py' mode='r' encoding='UTF-8'&amp;gt;
  f = None
Object allocated at (most recent call first):
  File &amp;quot;example.py&amp;quot;, lineno 4
    return open(__file__)
  File &amp;quot;example.py&amp;quot;, lineno 6
    f = func()
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;Object allocated at &lt;span class="pre"&gt;(...)&lt;/span&gt;&lt;/tt&gt; part is the new feature ;-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="add-source-parameter-to-warnings"&gt;
&lt;h3&gt;Add source parameter to warnings&lt;/h3&gt;
&lt;p&gt;Python 3 logs &lt;tt class="docutils literal"&gt;ResourceWarning&lt;/tt&gt; warnings when a resource is not closed
properly to help developers to handle resources correctly. The problem is that
the warning is only logged when the object is destroy, which can occur far from
the object creation and can occur on a line unrelated to the object because of
the garbage collector.&lt;/p&gt;
&lt;p&gt;I added a new &lt;tt class="docutils literal"&gt;tracemalloc&lt;/tt&gt; module to Python 3.4 which has an interesting
&lt;tt class="docutils literal"&gt;tracemalloc.get_object_traceback()&lt;/tt&gt; function. If tracemalloc traced the
allocation of an object, it is able to provide later the traceback where the
object was allocated.&lt;/p&gt;
&lt;p&gt;I wanted to modify the &lt;tt class="docutils literal"&gt;warnings&lt;/tt&gt; module to call
&lt;tt class="docutils literal"&gt;get_object_traceback()&lt;/tt&gt;, but I noticed that it wasn't possible
to easily extend the &lt;tt class="docutils literal"&gt;warnings&lt;/tt&gt; API because this module allows to override
&lt;tt class="docutils literal"&gt;showwarning()&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;formatwarning()&lt;/tt&gt; functions and these
functions have a fixed number of parameters. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def showwarning(message, category, filename, lineno, file=None, line=None):
    ...
&lt;/pre&gt;
&lt;p&gt;With the issue #26568, I added new  &lt;tt class="docutils literal"&gt;_showwarnmsg()&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;_formatwarnmsg()&lt;/tt&gt;
functions to the warnings module which get a &lt;tt class="docutils literal"&gt;warnings.WarningMessage&lt;/tt&gt; object
instead of a list of parameters:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def _showwarnmsg(msg):
    ...
&lt;/pre&gt;
&lt;p&gt;I added a &lt;tt class="docutils literal"&gt;source&lt;/tt&gt; attribute to &lt;tt class="docutils literal"&gt;warnings.WarningMessage&lt;/tt&gt; (issue #26567)
and a new optional &lt;tt class="docutils literal"&gt;source&lt;/tt&gt; parameter to &lt;tt class="docutils literal"&gt;warnings.warn()&lt;/tt&gt; (issue #26604):
the leaked resource object. I modified &lt;tt class="docutils literal"&gt;_formatwarnmsg()&lt;/tt&gt; to log the
traceback where resource was allocated, if available.&lt;/p&gt;
&lt;p&gt;The tricky part was to fix corner cases when the following functions of the
&lt;tt class="docutils literal"&gt;warnings&lt;/tt&gt; module are overriden:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;formatwarning()&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;showwarning()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;_formatwarnmsg()&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;_showwarnmsg()&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="set-the-source-parameter"&gt;
&lt;h3&gt;Set the source parameter&lt;/h3&gt;
&lt;p&gt;I started to modify modules to set the source parameter when logging
&lt;tt class="docutils literal"&gt;ResourceWarning&lt;/tt&gt; warnings.&lt;/p&gt;
&lt;p&gt;The easy part was to modify &lt;tt class="docutils literal"&gt;asyncore&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;asyncio&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;_pyio&lt;/tt&gt; modules to
set the &lt;tt class="docutils literal"&gt;source&lt;/tt&gt; parameter. These modules are implemented in Python, the
change was just to add &lt;tt class="docutils literal"&gt;source=self&lt;/tt&gt;. Example of &lt;tt class="docutils literal"&gt;asyncio&lt;/tt&gt; destructor:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def __del__(self):
    if not self.is_closed():
        warnings.warn(&amp;quot;unclosed event loop %r&amp;quot; % self, ResourceWarning,
                      source=self)
        if not self.is_running():
            self.close()
&lt;/pre&gt;
&lt;p&gt;Note: The warning is logged before the resource is closed to provide more
information in &lt;tt class="docutils literal"&gt;repr()&lt;/tt&gt;. Many objects clear most information in their
&lt;tt class="docutils literal"&gt;close()&lt;/tt&gt; method.&lt;/p&gt;
&lt;p&gt;Modifying C modules was more tricky than expected. I had to implement
&amp;quot;finalizers&amp;quot; (&lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0442/"&gt;PEP 432: Safe object finalization&lt;/a&gt;) for the &lt;tt class="docutils literal"&gt;_socket.socket&lt;/tt&gt; type
(issue #26590) and for the &lt;tt class="docutils literal"&gt;os.scandir()&lt;/tt&gt; iterator (issue #26603).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="more-reliable-warnings"&gt;
&lt;h3&gt;More reliable warnings&lt;/h3&gt;
&lt;p&gt;The Python shutdown process is complex, and some Python functions are broken
during the shutdown. I enhanced the warnings module to handle nicely these
failures and try to log warnings anyway.&lt;/p&gt;
&lt;p&gt;I modified &lt;tt class="docutils literal"&gt;warnings.formatwarning()&lt;/tt&gt; to catch &lt;tt class="docutils literal"&gt;linecache.getline()&lt;/tt&gt;
failures on formatting the traceback.&lt;/p&gt;
&lt;p&gt;Logging the resource traceback is complex, so I only implemented it in Python.
Python tries to use the Python &lt;tt class="docutils literal"&gt;warnings&lt;/tt&gt; module if it was imported, or falls
back on the C &lt;tt class="docutils literal"&gt;_warnings&lt;/tt&gt; module. To get the resource traceback at Python
shutdown, I modified the C module to try to import the Python warning:
&lt;tt class="docutils literal"&gt;_warnings.warn_explicit()&lt;/tt&gt; now tries to import the Python warnings module if
the source parameter is set to be able to log the traceback where the source
was allocated (issue #26592).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fix-resourcewarning-warnings"&gt;
&lt;h3&gt;Fix ResourceWarning warnings&lt;/h3&gt;
&lt;p&gt;Since it became easy to debug these warnings, I fixed some of them in the
Python test suite:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #26620: Fix ResourceWarning in test_urllib2_localnet. Use context
manager on urllib objects and use self.addCleanup() to cleanup resources even
if a test is interrupted with CTRL+c&lt;/li&gt;
&lt;li&gt;Issue #25654: multiprocessing: open file with &lt;tt class="docutils literal"&gt;closefd=False&lt;/tt&gt; to avoid
ResourceWarning. _test_multiprocessing: open file with &lt;tt class="docutils literal"&gt;O_EXCL&lt;/tt&gt; to detect
bugs in tests (if a previous test forgot to remove TESTFN).
&lt;tt class="docutils literal"&gt;test_sys_exit()&lt;/tt&gt;: remove TESTFN after each loop iteration&lt;/li&gt;
&lt;li&gt;Fix &lt;tt class="docutils literal"&gt;ResourceWarning&lt;/tt&gt; in test_unittest when interrupted&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="pymem-malloc-now-fails-if-the-gil-is-not-held"&gt;
&lt;h2&gt;PyMem_Malloc() now fails if the GIL is not held&lt;/h2&gt;
&lt;p&gt;Since using the mall object allocator (&lt;tt class="docutils literal"&gt;pymalloc)&lt;/tt&gt;) for dictionary key
storage showed speedup for the dict type (issue #23601), I proposed to
generalize the change, use &lt;tt class="docutils literal"&gt;pymalloc&lt;/tt&gt; for &lt;tt class="docutils literal"&gt;PyMem_Malloc()&lt;/tt&gt;: &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2016-February/143084.html"&gt;[Python-Dev]
Modify PyMem_Malloc to use pymalloc for performance&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The main issue was that the change means that &lt;tt class="docutils literal"&gt;PyMem_Malloc()&lt;/tt&gt; now requires
to hold the GIL, whereas it didn't before since it called directly
&lt;tt class="docutils literal"&gt;malloc()&lt;/tt&gt;.&lt;/p&gt;
&lt;div class="section" id="check-if-the-gil-is-held"&gt;
&lt;h3&gt;Check if the GIL is held&lt;/h3&gt;
&lt;p&gt;CPython has a &lt;tt class="docutils literal"&gt;PyGILState_Check()&lt;/tt&gt; function to check if the GIL is held.
Problem: the function doesn't work with subinterpreters: see issues #10915 and
#15751.&lt;/p&gt;
&lt;p&gt;I added an internal flag to &lt;tt class="docutils literal"&gt;PyGILState_Check()&lt;/tt&gt; (issue #26558) to skip the
test. The flag value is false at startup, set to true once the GIL is fully
initialized (Python initialization), set to false again when the GIL is
destroyed (Python finalization). The flag is also set to false when the first
subinterpreter is created.&lt;/p&gt;
&lt;p&gt;This hack works around &lt;tt class="docutils literal"&gt;PyGILState_Check()&lt;/tt&gt; limitations allowing to call
&lt;cite&gt;PyGILState_Check()`&lt;/cite&gt; anytime to debug more bugs earlier.&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;_Py_dup()&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;_Py_fstat()&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;_Py_read()&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;_Py_write()&lt;/tt&gt; are
low-level helper functions for system functions, but these functions require
the GIL to be held.  Thanks to the &lt;tt class="docutils literal"&gt;PyGILState_Check()&lt;/tt&gt; enhancement, it
became possible to check the GIL using an assertion.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pymem-malloc-and-gil"&gt;
&lt;h3&gt;PyMem_Malloc() and GIL&lt;/h3&gt;
&lt;p&gt;Issue #26563: Debug hooks on Python memory allocators now raise a fatal error
if memory allocator functions like PyMem_Malloc() and PyMem_Malloc() are called
without holding the GIL.&lt;/p&gt;
&lt;p&gt;The change spotted two bugs which I fixed:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #26563: Replace PyMem_Malloc() with PyMem_RawMalloc() in the Windows
implementation of os.stat(), the code is called without holding the
GIL.&lt;/li&gt;
&lt;li&gt;Issue #26563: Fix usage of PyMem_Malloc() in overlapped.c. Replace
PyMem_Malloc() with PyMem_RawFree() since PostToQueueCallback() calls
PyMem_Free() in a new C thread which doesn't hold the GIL.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I wasn't able to switch &lt;tt class="docutils literal"&gt;PyMem_Malloc()&lt;/tt&gt; to &lt;tt class="docutils literal"&gt;pymalloc&lt;/tt&gt; in this quarter,
since it took more a lot of time to implement requested checks and test third
party modules.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="fatal-error-and-faulthandler"&gt;
&lt;h3&gt;Fatal error and faulthandler&lt;/h3&gt;
&lt;p&gt;I enhanced the faulthandler module to work in non-Python threads (issue
#26563). I fixed &lt;tt class="docutils literal"&gt;Py_FatalError()&lt;/tt&gt; if called without holding the GIL: don't
try to print the current exception, nor try to flush stdout and stderr: only
dump the traceback of Python threads.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="interesting-bug-reentrant-flag-in-tracemalloc"&gt;
&lt;h2&gt;Interesting bug: reentrant flag in tracemalloc&lt;/h2&gt;
&lt;p&gt;A bug annoyed me a lot: a random assertion error related to a reentrant flag in
the _tracemalloc module.&lt;/p&gt;
&lt;p&gt;Story starting in the &lt;a class="reference external" href="http://bugs.python.org/issue26588#msg262125"&gt;middle of the issue #26588 (2016-03-21)&lt;/a&gt;. While working on issue #26588,
&amp;quot;_tracemalloc: add support for multiple address spaces (domains)&amp;quot;, I noticed an
assertion failure in set_reentrant(), a helper function to set a &lt;em&gt;Thread Local
Storage&lt;/em&gt; (TLS), on a buildbot:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
python: ./Modules/_tracemalloc.c:195: set_reentrant:
    Assertion `PyThread_get_key_value(tracemalloc_reentrant_key) == ((PyObject *) &amp;amp;_Py_TrueStruct)' failed.
&lt;/pre&gt;
&lt;p&gt;I was unable to reproduce the bug on my Fedora 23 (AMD64). After changes on my
patch, I pushed it the day after, but the assertion failed again. I added
assertions and debug informations. More failures, an interesting one on Windows
which uses a single process.&lt;/p&gt;
&lt;p&gt;I added an assertion in tracemalloc_init() to ensure that the reeentrant flag
is set at the end of the function. The reentrant flag was no more set at
tracemalloc_start() entry for an unknown reason. I changed the module
initialization to no call tracemalloc_init() anymore, it's only called on
tracemalloc.start().&lt;/p&gt;
&lt;p&gt;&amp;quot;The bug was seen on 5 buildbots yet: PPC Fedora, AMD64 Debian, s390x RHEL,
AMD64 Windows, x86 Ubuntu.&amp;quot;&lt;/p&gt;
&lt;p&gt;I finally understood and fixed the bug with the &lt;a class="reference external" href="https://hg.python.org/cpython/rev/af1c1149784a"&gt;change af1c1149784a&lt;/a&gt;: tracemalloc_start() and
tracemalloc_stop() don't clear/set the reentrant flag anymore.&lt;/p&gt;
&lt;p&gt;The problem was that I expected that tracemalloc_init() and tracemalloc_start()
functions would always be called in the same thread, whereas it occurred that
tracemalloc_init() was called in thread A when the tracemalloc module is
imported, whereas tracemalloc_start() was called in thread B.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-commits"&gt;
&lt;h2&gt;Other commits&lt;/h2&gt;
&lt;div class="section" id="enhancements"&gt;
&lt;h3&gt;Enhancements&lt;/h3&gt;
&lt;p&gt;The developers of the &lt;tt class="docutils literal"&gt;vmprof&lt;/tt&gt; profiler asked me to expose the atomic
variable &lt;tt class="docutils literal"&gt;_PyThreadState_Current&lt;/tt&gt;. The private variable was removed from
Python 3.5.1 API because the implementation of atomic variables depends on the
compiler, compiler options, etc. and so caused compilation issues. I added a
new private &lt;tt class="docutils literal"&gt;_PyThreadState_UncheckedGet()&lt;/tt&gt; function (issue #26154) which
gets the value of the variable without exposing its implementation.&lt;/p&gt;
&lt;p&gt;Other enhancements:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #26099: The site module now writes an error into stderr if
sitecustomize module can be imported but executing the module raise an
ImportError. Same change for usercustomize.&lt;/li&gt;
&lt;li&gt;Issue #26516: Enhance Python memory allocators documentation. Add link to
PYTHONMALLOCSTATS environment variable. Add parameters to PyMem macros like
PyMem_MALLOC().&lt;/li&gt;
&lt;li&gt;Issue #26569: Fix pyclbr.readmodule() and pyclbr.readmodule_ex() to support
importing packages.&lt;/li&gt;
&lt;li&gt;Issue #26564, #26516, #26563: Enhance documentation on memory allocator debug
hooks.&lt;/li&gt;
&lt;li&gt;doctest now supports packages. Issue #26641: doctest.DocFileTest and
doctest.testfile() now support packages (module splitted into multiple
directories) for the package parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="bugfixes"&gt;
&lt;h3&gt;Bugfixes&lt;/h3&gt;
&lt;p&gt;Issue #25843: When compiling code, don't merge constants if they are equal but
have a different types. For example, &lt;tt class="docutils literal"&gt;f1, f2 = lambda: 1, lambda: 1.0&lt;/tt&gt; is now
correctly compiled to two different functions: &lt;tt class="docutils literal"&gt;f1()&lt;/tt&gt; returns &lt;tt class="docutils literal"&gt;1&lt;/tt&gt; (int) and
&lt;tt class="docutils literal"&gt;f2()&lt;/tt&gt; returns &lt;tt class="docutils literal"&gt;1.0&lt;/tt&gt; (int), even if 1 and 1.0 are equal.&lt;/p&gt;
&lt;p&gt;Other fixes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #26101: Fix test_compilepath() of test_compileall. Exclude Lib/test/
from sys.path in test_compilepath(). The directory contains invalid Python
files like Lib/test/badsyntax_pep3120.py, whereas the test ensures that all
files can be compiled.&lt;/li&gt;
&lt;li&gt;Issue #24520: Replace fpgetmask() with fedisableexcept(). On FreeBSD,
fpgetmask() was deprecated long time ago.  fedisableexcept() is now
preferred.&lt;/li&gt;
&lt;li&gt;Issue #26161: Use Py_uintptr_t instead of void* for atomic pointers in
pyatomic.h. Use atomic_uintptr_t when &amp;lt;stdatomic.h&amp;gt; is used. Using void*
causes compilation warnings depending on which implementation of atomic types
is used.&lt;/li&gt;
&lt;li&gt;Issue #26637: The importlib module now emits an ImportError rather than a
TypeError if __import__() is tried during the Python shutdown process but
sys.path is already cleared (set to None).&lt;/li&gt;
&lt;li&gt;doctest: fix _module_relative_path() error message. Write the module name
rather than &amp;lt;module&amp;gt; in the error message, if module has no __file__
attribute (ex: package).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="fix-type-downcasts-on-windows-64-bit"&gt;
&lt;h3&gt;Fix type downcasts on Windows 64-bit&lt;/h3&gt;
&lt;p&gt;In my spare time, I'm trying to fix a few compiler warnings on Windows 64-bit
where the C &lt;tt class="docutils literal"&gt;long&lt;/tt&gt; type is only 32-bit, whereas pointers are &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;64-bit&lt;/span&gt;&lt;/tt&gt; long:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;posix_getcwd(): limit to INT_MAX on Windows. It's more to fix a compiler
warning during compilation, I don't think that Windows support current
working directories larger than 2 GB :-)&lt;/li&gt;
&lt;li&gt;_pickle: Fix load_counted_tuple(), use Py_ssize_t for size. Fix a warning on
Windows 64-bit.&lt;/li&gt;
&lt;li&gt;getpathp.c: fix compiler warning, wcsnlen_s() result type is size_t.&lt;/li&gt;
&lt;li&gt;compiler.c: fix compiler warnings on Windows&lt;/li&gt;
&lt;li&gt;_msi.c: try to fix compiler warnings&lt;/li&gt;
&lt;li&gt;longobject.c: fix compilation warning on Windows 64-bit. We know that
Py_SIZE(b) is -1 or 1 an so fits into the sdigit type.&lt;/li&gt;
&lt;li&gt;On Windows, socket.setsockopt() now raises an OverflowError if the socket
option is larger than INT_MAX bytes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="unicode-bugfixes"&gt;
&lt;h3&gt;Unicode bugfixes&lt;/h3&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #26227: On Windows, getnameinfo(), gethostbyaddr() and
gethostbyname_ex() functions of the socket module now decode the hostname
from the ANSI code page rather than UTF-8.&lt;/li&gt;
&lt;li&gt;Issue #26217: Unicode resize_compact() must set wstr_length to 0 after
freeing the wstr string. Otherwise, an assertion fails in
_PyUnicode_CheckConsistency().&lt;/li&gt;
&lt;li&gt;Issue #26464: Fix str.translate() when string is ASCII and first replacements
removes characters, but next replacements use a non-ASCII character or a
string longer than 1 character. Regression introduced in Python 3.5.0.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="buildbot-tests"&gt;
&lt;h3&gt;Buildbot, tests&lt;/h3&gt;
&lt;p&gt;Just to give you an idea of the work required to keep a working CI, here is the
list of changes I maded in a single quarter to make tests and Python buildbots
more reliable.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #26610: Skip test_venv.test_with_pip() if ctypes miss&lt;/li&gt;
&lt;li&gt;test_asyncio: fix test_timeout_time(). Accept time delta up to 0.12 second,
instead of 0.11, for the &amp;quot;AMD64 FreeBSD 9.x&amp;quot; buildbot slave.&lt;/li&gt;
&lt;li&gt;Issue #13305: Always test datetime.datetime.strftime(&amp;quot;%4Y&amp;quot;) for years &amp;lt; 1900.
Change quickly reverted, strftime(&amp;quot;%4Y&amp;quot;) fails on most platforms.&lt;/li&gt;
&lt;li&gt;Issue #17758: Skip test_site if site.USER_SITE directory doesn't exist and
cannot be created.&lt;/li&gt;
&lt;li&gt;Fix test_venv on FreeBSD buildbot. Ignore pip warning in
test_venv.test_with_venv().&lt;/li&gt;
&lt;li&gt;Issue #26566: Rewrite test_signal.InterProcessSignalTests. Don't use
os.fork() with a subprocess to not inherit existing signal handlers or
threads: start from a fresh process. Use a timeout of 10 seconds to wait for
the signal instead of 1 second&lt;/li&gt;
&lt;li&gt;Issue #26538: regrtest: Fix module.__path__. libregrtest: Fix setup_tests()
to keep module.__path__ type (_NamespacePath), don't convert to a list.
Add _NamespacePath.__setitem__() method to importlib._bootstrap_external.&lt;/li&gt;
&lt;li&gt;regrtest: add time to output. Timestamps should help to debug slow buildbots,
and timeout and hang on buildbots.&lt;/li&gt;
&lt;li&gt;regrtest: add timeout to main process when using -jN. libregrtest: add a
watchdog to run_tests_multiprocess() using faulthandler.dump_traceback_later().&lt;/li&gt;
&lt;li&gt;Makefile: change default value of TESTTIMEOUT from 1 hour to 15 min.
The whole test suite takes 6 minutes on my laptop. It takes less than 30
minutes on most buildbots. The TESTTIMEOUT is the timeout for a single test
file.&lt;/li&gt;
&lt;li&gt;Buildbots: change also Windows timeout from 1 hour to 15 min&lt;/li&gt;
&lt;li&gt;regrtest: display test duration in sequential mode. Only display duration if
a test takes more than 30 seconds.&lt;/li&gt;
&lt;li&gt;Issue #18787: Try to fix test_spwd on OpenIndiana. Try to get the &amp;quot;root&amp;quot;
entry which should exist on all UNIX instead of &amp;quot;bin&amp;quot; which doesn't exist on
OpenIndiana.&lt;/li&gt;
&lt;li&gt;regrtest: fix --fromfile feature. Update code for the name regrtest output
format. Enhance also test_regrtest test on --fromfile&lt;/li&gt;
&lt;li&gt;regrtest: mention if tests run sequentially or in parallel&lt;/li&gt;
&lt;li&gt;regrtest: when parallel tests are interrupted, display progress&lt;/li&gt;
&lt;li&gt;support.temp_dir(): call support.rmtree() instead of shutil.rmtree(). Try
harder to remove directories on Windows.&lt;/li&gt;
&lt;li&gt;rt.bat: use -m test instead of Libtestregrtest.py&lt;/li&gt;
&lt;li&gt;Refactor regrtest.&lt;/li&gt;
&lt;li&gt;Fix test_warnings.test_improper_option(). test_warnings: only run
test_improper_option() and test_warnings_bootstrap() once. The unit test
doesn't depend on self.module.&lt;/li&gt;
&lt;li&gt;Fix test_os.test_symlink(): remove created symlink.&lt;/li&gt;
&lt;li&gt;Issue #26643: Add missing shutil resources to regrtest.py&lt;/li&gt;
&lt;li&gt;test_urllibnet: set timeout on test_fileno(). Use the default timeout of 30
seconds to avoid blocking forever.&lt;/li&gt;
&lt;li&gt;Issue #26295: When using &amp;quot;python3 -m test --testdir=TESTDIR&amp;quot;, regrtest
doesn't add &amp;quot;test.&amp;quot; prefix to test module names. regrtest also prepends
testdir to sys.path.&lt;/li&gt;
&lt;li&gt;Issue #26295: test_regrtest now uses a temporary directory&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="contributions"&gt;
&lt;h3&gt;Contributions&lt;/h3&gt;
&lt;p&gt;I also pushed a few changes written by other contributors:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #25907: Use {% trans %} tags in HTML templates to ease the translation
of the documentation. The tag comes from Jinja templating system, used by
Sphinx. Patch written by &lt;strong&gt;Julien Palard&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #26248: Enhance os.scandir() doc, patch written by Ben Hoyt:&lt;/li&gt;
&lt;li&gt;Fix error message in asyncio.selector_events. Patch written by &lt;strong&gt;Carlo
Beccarini&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #16851: Fix inspect.ismethod() doc, return also True if object is an
unbound method. Patch written by &lt;strong&gt;Anna Koroliuk&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #26574: Optimize bytes.replace(b'', b'.') and bytearray.replace(b'', b'.'):
up to 80% faster. Patch written by &lt;strong&gt;Josh Snider&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>Analysis of a Python performance issue</title><link href="https://haypo.github.io/analysis-python-performance-issue.html" rel="alternate"></link><published>2016-11-19T00:30:00+01:00</published><updated>2016-11-19T00:30:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-11-19:/analysis-python-performance-issue.html</id><summary type="html">&lt;p&gt;I am working on the CPython benchmark suite (&lt;a class="reference external" href="https://github.com/python/performance"&gt;performance&lt;/a&gt;) and I run the benchmark suite to
upload results to &lt;a class="reference external" href="http://speed.python.org/"&gt;speed.python.org&lt;/a&gt;. While
analying results, I noticed a temporary peak on the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt;
benchmark at October 19th:&lt;/p&gt;
&lt;img alt="call_method microbenchmark" src="https://haypo.github.io/images/call_method.png" /&gt;
&lt;p&gt;The graphic shows the performance of the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt; microbenchmark between
Feb 29, 2016 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I am working on the CPython benchmark suite (&lt;a class="reference external" href="https://github.com/python/performance"&gt;performance&lt;/a&gt;) and I run the benchmark suite to
upload results to &lt;a class="reference external" href="http://speed.python.org/"&gt;speed.python.org&lt;/a&gt;. While
analying results, I noticed a temporary peak on the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt;
benchmark at October 19th:&lt;/p&gt;
&lt;img alt="call_method microbenchmark" src="https://haypo.github.io/images/call_method.png" /&gt;
&lt;p&gt;The graphic shows the performance of the &lt;tt class="docutils literal"&gt;call_method&lt;/tt&gt; microbenchmark between
Feb 29, 2016 and November 17, 2016 on the &lt;tt class="docutils literal"&gt;default&lt;/tt&gt; branch of CPython. The average
is around 17.2 ms, whereas the peak is at 29.0 ms: &lt;strong&gt;68% slower&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;The server has two &amp;quot;Intel(R) Xeon(R) CPU X5680  &amp;#64; 3.33GHz&amp;quot; CPUs, total: 24
logical cores (12 physical cores with HyperThreading). This CPU was launched in
2010 and based on the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Gulftown"&gt;Westmere-EP microarchitecture&lt;/a&gt;. Westmere-EP is based on Westmere,
which is the 32 nm shrink of the Nehalem microarchitecture.&lt;/p&gt;
&lt;div class="section" id="reproduce-results"&gt;
&lt;h2&gt;Reproduce results&lt;/h2&gt;
&lt;p&gt;Before going too far, the first step is to validate that results are
reproductible: reboot the computer, recompile Python, run again the benchmark.&lt;/p&gt;
&lt;p&gt;Instead of running the full benchmark suite, install Python, ..., we will run
directly the benchmark manually using the Python freshly built in its source
code directory.&lt;/p&gt;
&lt;p&gt;Interesting dots on the graphic (can be seen at speed.python.org, not on the
screenshot):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;678fe178da0d, Oct 09, 17.0 ms: &amp;quot;Fast&amp;quot;&lt;/li&gt;
&lt;li&gt;1ce50f7027c1, Oct 19, 28.9 ms: &amp;quot;Slow&amp;quot;&lt;/li&gt;
&lt;li&gt;36af3566b67a, Nov 3, 16.9 ms: Fast again&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I use the following directories:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;~/perf: GitHub haypo/perf project&lt;/li&gt;
&lt;li&gt;~/performance: GitHub python/performance project&lt;/li&gt;
&lt;li&gt;~/cpython: Mercurial CPython repository&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tune the system for benchmarks:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo python3 -m perf system tune
&lt;/pre&gt;
&lt;p&gt;Note: all &lt;tt class="docutils literal"&gt;system&lt;/tt&gt; commands in this article are optional. They help to reduce
the operating system jitter (make benchmarks more reliablee).&lt;/p&gt;
&lt;p&gt;Fast:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 678fe178da0d
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ mv python python-fast
$ PYTHONPATH=~/perf ./python-fast ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 17.0 ms +- 0.1 ms
&lt;/pre&gt;
&lt;p&gt;Slow:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 1ce50f7027c1
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ mv python python-slow
$ PYTHONPATH=~/perf ./python-slow ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 29.3 ms +- 0.9 ms
&lt;/pre&gt;
&lt;p&gt;We reproduced the significant benchmark result: 17 ms =&amp;gt; 29 ms.&lt;/p&gt;
&lt;p&gt;I use &lt;tt class="docutils literal"&gt;./configure&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;make clean&lt;/tt&gt; instead of incremental compilation,
&lt;tt class="docutils literal"&gt;make&lt;/tt&gt; command, to avoid compilation errors, and to avoid potential side
effects only caused by the incremental compilation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="analysis-with-the-linux-perf-tool"&gt;
&lt;h2&gt;Analysis with the Linux perf tool&lt;/h2&gt;
&lt;p&gt;To collect perf events, we will run the benchmark with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--worker&lt;/span&gt;&lt;/tt&gt; to run a
single process and with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-w0&lt;/span&gt; &lt;span class="pre"&gt;-n100&lt;/span&gt;&lt;/tt&gt; to run the benchmark long enough: 100
samples means at least 10 seconds (a single sample takes at least 100 ms).&lt;/p&gt;
&lt;p&gt;First, reset the system configuration to reset the Linux perf configuration:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo python3 -m perf system reset
&lt;/pre&gt;
&lt;p&gt;Note: &lt;tt class="docutils literal"&gt;python3 &lt;span class="pre"&gt;-m&lt;/span&gt; perf system tune&lt;/tt&gt; reduces the sampling rate of Linux perf
to reduce operating system jitter.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="perf-stat"&gt;
&lt;h2&gt;perf stat&lt;/h2&gt;
&lt;p&gt;Command to get general statistics on the benchmark:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ perf stat ./python-slow ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -v -w0 -n100
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Fast&amp;quot; results:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for ./python-fast:

      3773.585194 task-clock (msec)         #    0.998 CPUs utilized
              369 context-switches          #    0.098 K/sec
                0 cpu-migrations            #    0.000 K/sec
            8,300 page-faults               #    0.002 M/sec
   12,981,234,867 cycles                    #    3.440 GHz                     [83.27%]
    1,460,980,720 stalled-cycles-frontend   #   11.25% frontend cycles idle    [83.36%]
      435,806,788 stalled-cycles-backend    #    3.36% backend  cycles idle    [66.72%]
   29,982,530,201 instructions              #    2.31  insns per cycle
                                            #    0.05  stalled cycles per insn [83.40%]
    5,613,631,616 branches                  # 1487.612 M/sec                   [83.40%]
       16,006,564 branch-misses             #    0.29% of all branches         [83.27%]

      3.780064486 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Slow&amp;quot; results:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for ./python-slow:

      5906.239860 task-clock (msec)         #    0.998 CPUs utilized
              556 context-switches          #    0.094 K/sec
                0 cpu-migrations            #    0.000 K/sec
            8,393 page-faults               #    0.001 M/sec
   20,651,474,102 cycles                    #    3.497 GHz                     [83.36%]
    8,480,803,345 stalled-cycles-frontend   #   41.07% frontend cycles idle    [83.37%]
    4,247,826,420 stalled-cycles-backend    #   20.57% backend  cycles idle    [66.64%]
   30,011,465,614 instructions              #    1.45  insns per cycle
                                            #    0.28  stalled cycles per insn [83.32%]
    5,612,485,730 branches                  #  950.264 M/sec                   [83.36%]
       13,584,136 branch-misses             #    0.24% of all branches         [83.29%]

      5.915402403 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;Significant differences, Fast =&amp;gt; Slow:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Instruction per cycle: 2.31 =&amp;gt; 1.45&lt;/li&gt;
&lt;li&gt;stalled-cycles-frontend: &lt;strong&gt;11.25% =&amp;gt; 41.07%&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;stalled-cycles-backend: &lt;strong&gt;3.36% =&amp;gt; 20.57%&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The increase of stalled cycles is interesting. Since the code is supposed to be
identical, it probably means that fetching instructions is slower. It sounds
like an issue with CPU caches.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="statistics-on-the-cpu-l1-instruction-cache"&gt;
&lt;h2&gt;Statistics on the CPU L1 instruction cache&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf list&lt;/tt&gt; command can be used to get the name of events collecting
statistics on the CPU L1 instruction cache:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ perf list | grep L1
  L1-icache-loads                                    [Hardware cache event]
  L1-icache-load-misses                              [Hardware cache event]
  (...)
&lt;/pre&gt;
&lt;p&gt;Collect statistics on the CPU L1 instruction cache:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PYTHONPATH=~/perf perf stat -e L1-icache-loads,L1-icache-load-misses ./python-slow ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -w0 -n10
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Fast&amp;quot; statistics:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for './python-fast (...)':

   10,134,106,571 L1-icache-loads
       10,917,606 L1-icache-load-misses     #    0.11% of all L1-icache hits

      3.775067668 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;&amp;quot;Slow&amp;quot; statistics:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
Performance counter stats for './python-slow (...)':

   10,753,371,258 L1-icache-loads
      848,511,308 L1-icache-load-misses     #    7.89% of all L1-icache hits

      6.020490449 seconds time elapsed
&lt;/pre&gt;
&lt;p&gt;Cache misses on the L1 cache: &lt;strong&gt;0.1%&lt;/strong&gt; (Fast) =&amp;gt; &lt;strong&gt;8.0%&lt;/strong&gt; (Slow).&lt;/p&gt;
&lt;p&gt;The slow Python has &lt;strong&gt;71.7x more L1 cache misses&lt;/strong&gt; than the fast Python! It can
explain the significant performance drop.&lt;/p&gt;
&lt;div class="section" id="perf-report"&gt;
&lt;h3&gt;perf report&lt;/h3&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf record&lt;/tt&gt; command can be used to collect statistics on the functions
where the benchmark spends most of its time. Commands:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PYTHONPATH=~/perf perf record ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -v -w0 -n100
perf report
&lt;/pre&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
40.27%  python  python              [.] _PyEval_EvalFrameDefault
10.30%  python  python              [.] call_function
10.21%  python  python              [.] PyFrame_New
 8.56%  python  python              [.] frame_dealloc
 5.51%  python  python              [.] PyObject_GenericGetAttr
 (...)
&lt;/pre&gt;
&lt;p&gt;More than 64% of the time is spent in these 5 functions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="system-tune"&gt;
&lt;h3&gt;system tune&lt;/h3&gt;
&lt;p&gt;To run benchmark, tune again the system for benchmarks:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo python3 -m perf system tune
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="hg-bisect"&gt;
&lt;h2&gt;hg bisect&lt;/h2&gt;
&lt;p&gt;To find the revision which introduces the performance slowdown, we use a
shell script to automate the bisection of the Mercurial history.&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;cmd.sh&lt;/tt&gt; script checking if a revision is fast or slow:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
set -e -x
./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
rm -f json
PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --worker -o json -v
PYTHONPATH=~/perf python3 cmd.py json
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;cmd.sh&lt;/tt&gt; uses the following &lt;tt class="docutils literal"&gt;cmd.py&lt;/tt&gt; script which checks if the benchmark
is slow: if it takes longer than 23 ms (average between 17 ans 29 ms):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
import perf, sys
bench = perf.Benchmark.load('json')
bad = (29 + 17) / 2.0
ms = bench.median() * 1e3
if ms &amp;gt;= bad:
    print(&amp;quot;BAD! %.1f ms &amp;gt;= %.1f ms&amp;quot; % (ms, bad))
    sys.exit(1)
else:
    print(&amp;quot;good: %.1f ms &amp;lt; %.1f ms&amp;quot; % (ms, bad))
&lt;/pre&gt;
&lt;p&gt;In the bisection, &amp;quot;good&amp;quot; means &amp;quot;fast&amp;quot; (17 ms), whereas &amp;quot;bad&amp;quot; means &amp;quot;slow&amp;quot; (29
ms).  The peak, revision 1ce50f7027c1, is used as the first &amp;quot;bad&amp;quot; revision. The
previous fast revision before the peak is 678fe178da0d, our first &amp;quot;good&amp;quot;
revision.&lt;/p&gt;
&lt;p&gt;Commands to identify the first revision which introduced the slowdown:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg bisect --reset
hg bisect -b 1ce50f7027c1
hg bisect -g 678fe178da0d
time hg bisect -c ./cmd.sh
&lt;/pre&gt;
&lt;p&gt;3 min 52 sec later:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
The first bad revision is:
changeset:   104531:83877018ef97
parent:      104528:ce85a1f129e3
parent:      104530:2d352bf2b228
user:        Serhiy Storchaka &amp;lt;storchaka&amp;#64;gmail.com&amp;gt;
date:        Tue Oct 18 13:27:54 2016 +0300
files:       Misc/NEWS
description:
Issue #23782: Fixed possible memory leak in _PyTraceback_Add() and exception
loss in PyTraceBack_Here().
&lt;/pre&gt;
&lt;p&gt;Thank you &lt;tt class="docutils literal"&gt;hg bisect&lt;/tt&gt;! I love this tool.&lt;/p&gt;
&lt;p&gt;Even if I trust &lt;tt class="docutils literal"&gt;hg bisect&lt;/tt&gt;, I don't trust benchmarks, so I recheck manually:&lt;/p&gt;
&lt;p&gt;Slow:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 83877018ef97
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 29.4 ms +- 1.8 ms
&lt;/pre&gt;
&lt;p&gt;Use &lt;tt class="docutils literal"&gt;hg parents&lt;/tt&gt; to get the latest fast revision:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg parents -r 83877018ef97
changeset:   104528:ce85a1f129e3
(...)

changeset:   104530:2d352bf2b228
branch:      3.6
(...)
&lt;/pre&gt;
&lt;p&gt;Check the parent:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r ce85a1f129e3
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 17.1 ms +- 0.1 ms
&lt;/pre&gt;
&lt;p&gt;The revision ce85a1f129e3 is fast and the following revision 83877018ef97 is
slow. &lt;strong&gt;The revision 83877018ef97 introduced the slowdown&lt;/strong&gt;.  We found it!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="analysis-of-the-revision-introducing-the-slowdown"&gt;
&lt;h2&gt;Analysis of the revision introducing the slowdown&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://hg.python.org/cpython/rev/83877018ef97/"&gt;revision 83877018ef97&lt;/a&gt;
changes two files: Misc/NEWS and Python/traceback.c. The NEWS file is only
documentation and so must not impact performances.  Python/traceback.c is part
of the C code and so is more interesting.&lt;/p&gt;
&lt;p&gt;The commit only changes two C functions: &lt;tt class="docutils literal"&gt;PyTraceBack_Here()&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;_PyTraceback_Add()&lt;/tt&gt;, but &lt;tt class="docutils literal"&gt;perf report&lt;/tt&gt; didn't show these functions as &amp;quot;hot&amp;quot;.
In fact, these functions are never called by the benchmark.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The commit doesn't touch the C code used in the benchmark.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Unrelated C change impacting performances reminds me my previous &lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-deadcode.html"&gt;deadcode
horror story&lt;/a&gt;. The performance
difference is probably caused by &lt;strong&gt;&amp;quot;code placement&amp;quot;&lt;/strong&gt;: &lt;tt class="docutils literal"&gt;perf stat&lt;/tt&gt; showed a
significant increase of the cache miss rate on the L1 instruction cache.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="use-gcc-attribute-hot"&gt;
&lt;h2&gt;Use GCC __attribute__((hot))&lt;/h2&gt;
&lt;p&gt;Using PGO compilation was the solution for deadcode, but PGO doesn't work on
Ubuntu 14.04 (the OS used by the benchmark server, speed-python) and PGO seems
to make benchmarks less reliable.&lt;/p&gt;
&lt;p&gt;I wanted to try something else: mark hot functions using the GCC
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((hot))&lt;/span&gt;&lt;/tt&gt; attribute. PGO compilation does this automatically.&lt;/p&gt;
&lt;p&gt;This attribute only has an impact on the code placement: where functions are
loaded in memory. The flag declares functions in the &lt;tt class="docutils literal"&gt;.text.hot&lt;/tt&gt; ELF section
rather than the &lt;tt class="docutils literal"&gt;.text&lt;/tt&gt; ELF section. Grouping hot functions in the same
functions helps to reduce the distance between functions and so enhance the
usage of CPU caches.&lt;/p&gt;
&lt;p&gt;I wrote and then pushed a patch in the &lt;a class="reference external" href="http://bugs.python.org/issue28618"&gt;issue #28618&lt;/a&gt;: &amp;quot;Decorate hot functions using
__attribute__((hot)) to optimize Python&amp;quot;.&lt;/p&gt;
&lt;p&gt;The patch marks 6 functions as hot:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;_PyEval_EvalFrameDefault()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;call_function()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;_PyFunction_FastCall()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;PyFrame_New()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;frame_dealloc()&lt;/tt&gt;&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;PyErr_Occurred()&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let's try the patch:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 83877018ef97
$ wget https://hg.python.org/cpython/raw-rev/59b91b4e9506 -O patch
$ patch -p1 &amp;lt; patch
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 16.7 ms +- 0.3 ms
&lt;/pre&gt;
&lt;p&gt;It's easy to make mistakes and benchmarks are always suprising, so let's retry
without the patch:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ hg up -C -r 83877018ef97
$ ./configure --with-lto -C &amp;amp;&amp;amp; make clean &amp;amp;&amp;amp; make
$ PYTHONPATH=~/perf ./python ~/performance/performance/benchmarks/bm_call_method.py --inherit-environ=PYTHONPATH --fast
call_method: Median +- std dev: 29.3 ms +- 0.6 ms
&lt;/pre&gt;
&lt;p&gt;The check confirms that the GCC attribute fixed the issue!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;On modern Intel CPUs, the code placement can have a major impact on the
performance of microbenchmarks.&lt;/p&gt;
&lt;p&gt;The GCC &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;__attribute__((hot))&lt;/span&gt;&lt;/tt&gt; attribute can be used manually to make &amp;quot;hot
functions&amp;quot; close in memory to enhance the usage of CPU caches.&lt;/p&gt;
&lt;p&gt;To know more about the impact of code placement, see the very good talk of Zia
Ansari (Intel) at the LLVM Developers' Meeting 2016: &lt;a class="reference external" href="https://llvmdevelopersmeetingbay2016.sched.org/event/8YzY/causes-of-performance-instability-due-to-code-placement-in-x86"&gt;Causes of Performance
Swings Due to Code Placement in IA&lt;/a&gt;.
He describes well &amp;quot;performance swings&amp;quot; like the one described in this article
and explains how CPUs work internally and how code placement impacts CPU
performances.&lt;/p&gt;
&lt;/div&gt;
</content><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>Intel CPUs (part 2): Turbo Boost, temperature, frequency and Pstate C0 bug</title><link href="https://haypo.github.io/intel-cpus-part2.html" rel="alternate"></link><published>2016-09-23T23:00:00+02:00</published><updated>2016-09-23T23:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-09-23:/intel-cpus-part2.html</id><summary type="html">&lt;p class="first last"&gt;Intel CPUs (part 2): Turbo Boost, temperature, frequency and Pstate C0 bug&lt;/p&gt;
</summary><content type="html">&lt;p&gt;My first article &lt;a class="reference external" href="https://haypo.github.io/intel-cpus.html"&gt;Intel CPUs&lt;/a&gt; is a general
introduction on modern CPU technologies having an impact on benchmarks.&lt;/p&gt;
&lt;p&gt;This second article is much more concrete with numbers and a concrete bug
having a major impact on benchmarks: a benchmark suddenly becomes 2x faster!&lt;/p&gt;
&lt;p&gt;I will tell you how I first noticed the bug, which tests I ran to analyze the
issue, how I found commands to reproduce the bug, and finally how I identified
the bug.&lt;/p&gt;
&lt;div class="section" id="glitch-in-benchmarks"&gt;
&lt;h2&gt;&amp;quot;Glitch&amp;quot; in benchmarks&lt;/h2&gt;
&lt;p&gt;Last week I ran a benchmark to check if enabling Profile Guided Optimization
(PGO) when compiling Python makes benchmark results less stable. I recompiled
Python 5 times, and after each compilation I ran a benchmark. I tested
different commands and options to compile Python. Everything was fine until
the last benchmark of the last compilation. &lt;strong&gt;The benchmark suddenly became 2
times faster.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hopefully, my perf module collects a lot of metadata. I was able to analyze
in depth what happened.&lt;/p&gt;
&lt;p&gt;The &amp;quot;glitch&amp;quot; occurred in a benchmark having 400 runs (benchmark run in 400
different processes), between the run 105 (20.3 ms) and the run 106
(11.0 ms).&lt;/p&gt;
&lt;p&gt;I noticed that the CPU temperature was between 69°C and 72°C until the run 105,
and then decreased to from 69°C to 58°C.&lt;/p&gt;
&lt;p&gt;The system load slowly increased from 1.25 up to 1.62 around the run 108 and
then slowly decreased to 1.00.&lt;/p&gt;
&lt;p&gt;The system was not idle while the benchmark was running. I was working on the
PC too! But according to timestamps, it seems like the glitch was close to when
I stopped working. When I stopped working, I closed all applications (except of
the benchmark running in background) and turned of my two monitors.&lt;/p&gt;
&lt;p&gt;Well, at this point, it's hard to correlate for sure an event with the major
performance change.&lt;/p&gt;
&lt;p&gt;So I started to analyze different factors affecting CPUs and benchmarks: Turbo
Boost, CPU temperature and CPU frequency.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="impact-of-turbo-boost-on-benchmarks"&gt;
&lt;h2&gt;Impact of Turbo Boost on benchmarks&lt;/h2&gt;
&lt;p&gt;Without Turbo Boost, the maximum frequency of the &amp;quot;Intel(R) Core(TM) i7-3520M
CPU &amp;#64; 2.90GHz&amp;quot; of my laptop is 2.9 GHz. With Turbo Boost, the maximum
frequency is 3.6 GHz if only one core is active, or 3.4 GHz otherwise:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ sudo cpupower frequency-info
  ...
  boost state support:
    Supported: yes
    Active: yes
    3400 MHz max turbo 4 active cores
    3400 MHz max turbo 3 active cores
    3400 MHz max turbo 2 active cores
    3600 MHz max turbo 1 active cores
&lt;/pre&gt;
&lt;p&gt;I ran the bm_call_simple.py microbenchmark (CPU-bound) of performance 0.2.2.&lt;/p&gt;
&lt;p&gt;Turbo Boost disabled:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1 physical CPU active: 2.9 GHz, Median +- std dev: 14.6 ms +- 0.3 ms&lt;/li&gt;
&lt;li&gt;2 physical CPU active: 2.9 GHz, Median +- std dev: 14.7 ms +- 0.5 ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Turbo Boost enabled:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1 physical CPU active: 3.6 GHz, Median +- std dev: 11.8 ms +- 0.3 ms&lt;/li&gt;
&lt;li&gt;2 physical CPU active: 3.4 GHz, Median +- std dev: 12.4 ms +- 0.1 ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The maximum performance boost is 19% faster&lt;/strong&gt; (14.6 ms =&amp;gt; 11.8 ms), the
minimum boost if 15% faster (14.6 ms =&amp;gt; 12.4 ms).&lt;/p&gt;
&lt;p&gt;Hum, I don't think that Turbo Boost can explain the bug.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="impact-of-the-cpu-temperature-on-benchmarks"&gt;
&lt;h2&gt;Impact of the CPU temperature on benchmarks&lt;/h2&gt;
&lt;p&gt;The CPU temperature is mentionned in Intel Turbo Boost documentation as a
factor used to decide which P-state will be used. I always wanted to check how
the CPU temperature impacts its performance.&lt;/p&gt;
&lt;div class="section" id="burn-the-cpu-of-my-desktop-pc"&gt;
&lt;h3&gt;Burn the CPU of my desktop PC&lt;/h3&gt;
&lt;p&gt;CPU of my desktop PC: &amp;quot;Intel(R) Core(TM) i7-2600 CPU &amp;#64; 3.40GHz&amp;quot;.&lt;/p&gt;
&lt;p&gt;I used my &lt;a class="reference external" href="https://bitbucket.org/haypo/misc/src/tip/bin/system_load.py"&gt;system_load.py script&lt;/a&gt; to generate a
system load higher than 10.&lt;/p&gt;
&lt;p&gt;When the fan is cooling correctly the CPU, all CPU run at 3.4 GHz (Turbo Boost
was disabled) and the CPU temperature is 66°C.&lt;/p&gt;
&lt;p&gt;I used a simple sheet of paper to block the fan of my CPU. Yeah, I really
wanted to &lt;a class="reference external" href="https://www.youtube.com/watch?v=Xf0VuRG7MN4"&gt;burn my CPU&lt;/a&gt;! More
seriously, I checked the CPU temperature every second using the &lt;tt class="docutils literal"&gt;sensors&lt;/tt&gt;
command and was prepared to unblock the fan if sometimes gone wrong.&lt;/p&gt;
&lt;img alt="Sheet of paper blocking the CPU fan" src="https://haypo.github.io/images/paper_blocks_cpu_fan.jpg" /&gt;
&lt;p&gt;After one minute, the CPU reached 97°C. I expected a system crash, smoke or
something worse, but I was disappointed. &lt;strong&gt;At 97°C, I was still able to use my
computer as everything was fine. The CPU was slowly down automatically to the
minimum CPU frequency: 1533 MHz&lt;/strong&gt; according to turbostat (the minimum frequency
of this CPU is 1.6 GHz).&lt;/p&gt;
&lt;p&gt;When I unblocked the fan, the temperature decreased quickly to go back to its
previous state (62°C) and the CPU frequency quickly increased to 3.4 GHz as
well.&lt;/p&gt;
&lt;p&gt;My Intel CPU is really impressive! I didn't expect such very efficient
protection against overheating!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="burn-my-laptop-cpu"&gt;
&lt;h3&gt;Burn my laptop CPU&lt;/h3&gt;
&lt;p&gt;I used my system_load.py script to get a system load over 200. I also opened 4
tabs in Firefox playing Youtube videos to stress also the GPU which is
integrated into the CPU (IGP) on such laptop.&lt;/p&gt;
&lt;img alt="Stress test playing Youtube videos in Firefox, CPU at 102°" src="https://haypo.github.io/images/burn_cpu_firefox.jpg" /&gt;
&lt;p&gt;With such crazy stress test, the CPU temperature was &amp;quot;only&amp;quot; 83°C.&lt;/p&gt;
&lt;p&gt;Using a simple tissue, I closed the air hole used by the CPU fan. &lt;strong&gt;When the
CPU temperature increased from 100°C to 101°C, the CPU frequency started slowly
to decrease from 3391 MHz to 3077 MHz&lt;/strong&gt; (with steps between 10 MHz and 50 MHz
every second, or something like that).&lt;/p&gt;
&lt;p&gt;When pushing hard the tissue and waiting longer than 5 minutes, the CPU
temperature increased up to 102°C, but the CPU frequency was only decreased
from 3.4 GHz (Turbo Mode with 4 active logical CPUs) to 3.1 GHz.&lt;/p&gt;
&lt;p&gt;The maximum frequency is 2.9 GHz. Frequencies higher than 2.9 GHz means that
the Turbo Mode was enabled! It means that &lt;strong&gt;even with overheating, the CPU is
still fine and able to &amp;quot;overclock&amp;quot; itself!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Again, I was disapointed. With a CPU at 102°C, my laptop was still super fast
and reactive.  It seems like mobile CPUs handle even better overheating than
desktop CPUs (which is not something suprising at all).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="impact-of-the-cpu-frequency-on-benchmarks"&gt;
&lt;h2&gt;Impact of the CPU frequency on benchmarks&lt;/h2&gt;
&lt;p&gt;I ran the bm_call_simple.py microbenchmark (CPU-bound) of performance 0.2.2
on my desktop PC.&lt;/p&gt;
&lt;p&gt;Command to set the frequency of CPU 0 to the minimum frequency (1.6 GHz):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_min_freq|sudo tee  /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq
1600000
&lt;/pre&gt;
&lt;p&gt;Command to set the frequency of CPU 0 to the maximum frequency (3.4 GHz):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/cpu0/cpufreq/cpuinfo_max_freq|sudo tee  /sys/devices/system/cpu/cpu0/cpufreq/scaling_max_freq
3400000
&lt;/pre&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;CPU running at 1.6 GHz (min freq): Median +- std dev: 27.7 ms +- 0.7 ms&lt;/li&gt;
&lt;li&gt;CPU running at 3.4 GHz (min freq): Median +- std dev: 12.9 ms +- 0.2 ms&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The impact of the CPU frequency is quite obvious: &lt;strong&gt;when the CPU frequency is
doubled, the performance is also doubled&lt;/strong&gt;. The benchmark is 53% faster (27.7
ms =&amp;gt; 12.9 ms).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="bug-reproduced-and-then-identified-in-the-linux-cpu-driver"&gt;
&lt;h2&gt;Bug reproduced and then identified in the Linux CPU driver&lt;/h2&gt;
&lt;p&gt;Two days ago, I ran a very simple &amp;quot;timeit&amp;quot; microbenchmark to try to bisect a
performance regression in Python 3.6 on &lt;tt class="docutils literal"&gt;functools.partial&lt;/tt&gt;. Again, suddenly,
the microbenchmark became 2x faster!&lt;/p&gt;
&lt;p&gt;But this time, I found something: I noticed that running or stopping &lt;tt class="docutils literal"&gt;cpupower
monitor&lt;/tt&gt; and/or &lt;tt class="docutils literal"&gt;turbostat&lt;/tt&gt; can &amp;quot;enable&amp;quot; or &amp;quot;disable&amp;quot; the bug.&lt;/p&gt;
&lt;p&gt;After a lot of tests, I understood that running the benchmark with turbostat
&amp;quot;disables&amp;quot; the bug, whereas running &amp;quot;cpupower monitor&amp;quot; while running a
benchmark enables the bug.&lt;/p&gt;
&lt;p&gt;I reported the bug in the Fedora bug tracker, on the component kernel:
&lt;a class="reference external" href="https://bugzilla.redhat.com/show_bug.cgi?id=1378529"&gt;intel_pstate C0 bug on isolated CPUs with the performance governor and
NOHZ_FULL&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It seems like the bug is related to CPU isolation and NOHZ_FULL. The NOHZ_FULL
option is able to fully disable the scheduler clock interruption  on isolated
CPUs. I understood the the &lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; driver uses a callback on the
scheduler to update the Pstate of the CPU. According to an Intel engineer, the
&lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; driver was never tested with CPU isolation.&lt;/p&gt;
&lt;p&gt;The issue is not fully analyzed yet, but at least I succeeded to write a list
of commands to reproduce it with a success rate of 100% :-) Moreover, the Intel
engineer suggested to add an extra parameter to the Linux kernel command
(&lt;tt class="docutils literal"&gt;rcu_nocbs=3,7&lt;/tt&gt;) line which works around the issue.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This article describes how I found and then identified a bug in the Linux
driver of my CPU.&lt;/p&gt;
&lt;p&gt;Summary:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The maximum speedup of Turbo Boost is 20%&lt;/li&gt;
&lt;li&gt;Overheating on a dekstop PC can decrease the CPU frequency to its minimum
(half of the maximum in my case) which imply a slowdown of 50%&lt;/li&gt;
&lt;li&gt;A bug in the Linux CPU driver changes suddenly the CPU frequency from its
minimum to maximum (or the opposite) which means a speedup of 50%
(or slowdown of 50%)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;To get stable benchmarks, the safest fix for all these issues is probably to
set the CPU frequency of the CPUs used by benchmarks to the minimum.&lt;/strong&gt;
It seems like nothing can reduce the frequency of a CPU below its minimum.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When running benchmarks, raw timings and CPU performance don't matter. Only
comparisons between benchmark results and stable performances matter.&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
</content><category term="optimization"></category><category term="benchmark"></category><category term="cpu"></category></entry><entry><title>Intel CPUs: P-state, C-state, Turbo Boost, CPU frequency, etc.</title><link href="https://haypo.github.io/intel-cpus.html" rel="alternate"></link><published>2016-07-15T12:00:00+02:00</published><updated>2016-07-15T12:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-07-15:/intel-cpus.html</id><summary type="html">&lt;p class="first last"&gt;Intel CPUs: Hyper-threading, Turbo Boost, CPU frequency, etc.&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Ten years ago, most computers were desktop computers designed for best
performances and their CPU frequency was fixed. Nowadays, most devices are
embedded and use &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Low-power_electronics"&gt;low power consumption&lt;/a&gt; processors like ARM
CPUs. The power consumption now matters more than performance peaks.&lt;/p&gt;
&lt;p&gt;Intel CPUs evolved from a single core to multiple physical cores in the same
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/CPU_socket"&gt;package&lt;/a&gt; and got new features:
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Hyper-threading"&gt;Hyper-threading&lt;/a&gt; to run two
threads on the same physical core and &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Turbo_Boost"&gt;Turbo Boost&lt;/a&gt; to maximum performances.
CPU cores can be completely turned off (CPU HALT, frequency of 0) temporarily to
reduce the power consumption, and the frequency of cores changes regulary
depending on many factors like the workload and temperature. The power
consumption is now an important part in the design of modern CPUs.&lt;/p&gt;
&lt;p&gt;Warning! This article is a summary of what I learnt last weeks from random
articles. It may be full of mistakes, don't hesitate to report them, so I can
enhance the article! It's hard to find simple articles explaining performances
of modern Intel CPUs, so I tried to write mine.&lt;/p&gt;
&lt;div class="section" id="tools-used-in-this-article"&gt;
&lt;h2&gt;Tools used in this article&lt;/h2&gt;
&lt;p&gt;This article mentions various tools. Commands to install them on Fedora 24:&lt;/p&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;dnf install &lt;span class="pre"&gt;-y&lt;/span&gt; &lt;span class="pre"&gt;util-linux&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;lscpu&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;dnf install &lt;span class="pre"&gt;-y&lt;/span&gt; &lt;span class="pre"&gt;kernel-tools&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://linux.die.net/man/1/cpupower"&gt;cpupower&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;turbostat&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;sudo dnf install &lt;span class="pre"&gt;-y&lt;/span&gt; &lt;span class="pre"&gt;msr-tools&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;rdmsr&lt;/li&gt;
&lt;li&gt;wrmsr&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other interesting tools, not used in this article: i7z (sadly no more
maintained), lshw, dmidecode, sensors.&lt;/p&gt;
&lt;p&gt;The sensors tool is supposed to report the current CPU voltage, but it doesn't
provide this information on my computers. At least, it gives the temperature of
different components, but also the speed of fans.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-of-intel-cpus"&gt;
&lt;h2&gt;Example of Intel CPUs&lt;/h2&gt;
&lt;div class="section" id="my-laptop-cpu-proc-cpuinfo"&gt;
&lt;h3&gt;My laptop CPU: /proc/cpuinfo&lt;/h3&gt;
&lt;p&gt;On Linux, the most common way to retrieve information on the CPU is to read
&lt;tt class="docutils literal"&gt;/proc/cpuinfo&lt;/tt&gt;. Example on my laptop:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cat /proc/cpuinfo
processor  : 0
vendor_id  : GenuineIntel
model name : Intel(R) Core(TM) i7-3520M CPU &amp;#64; 2.90GHz
cpu MHz    : 1200.214
...

processor  : 1
vendor_id  : GenuineIntel
model name : Intel(R) Core(TM) i7-3520M CPU &amp;#64; 2.90GHz
cpu MHz    : 3299.882
...
&lt;/pre&gt;
&lt;p&gt;&amp;quot;i7-3520M&amp;quot; CPU is a model designed for Mobile Platforms (see the &amp;quot;M&amp;quot; suffix).
It was built in 2012 and is the third generation of the Intel i7
microarchitecture: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Ivy_Bridge_(microarchitecture)"&gt;Ivy Bridge&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The CPU has two physical cores, I disabled HyperThreading in the BIOS.&lt;/p&gt;
&lt;p&gt;The first strange thing is that the CPU announces &amp;quot;2.90 GHz&amp;quot; but Linux reports
1.2 GHz on the first core, and 3.3 GHz on the second core. 3.3 GHz is greater
than 2.9 GHz!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="my-desktop-cpu-cpu-topology-with-lscpu"&gt;
&lt;h3&gt;My desktop CPU: CPU topology with lscpu&lt;/h3&gt;
&lt;p&gt;cpuinfo:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
smithers$ cat /proc/cpuinfo
processor   : 0
physical id : 0
core id     : 0
...
model name  : Intel(R) Core(TM) i7-2600 CPU &amp;#64; 3.40GHz
cpu cores   : 4
...

processor   : 1
physical id : 0
core id     : 1
...

(...)

processor   : 7
physical id : 0
core id     : 3
...
&lt;/pre&gt;
&lt;p&gt;The CPU i7-2600 is the 2nd generation: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Sandy_Bridge"&gt;Sandy Bridge microarchitecture&lt;/a&gt;. There are 8 logical cores and 4
physical cores (so with Hyper-threading).&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;lscpu&lt;/tt&gt; renders a short table which helps to understand the CPU topology:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
smithers$ lscpu -a -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE MAXMHZ    MINMHZ
0   0    0      0    0:0:0:0       yes    3800.0000 1600.0000
1   0    0      1    1:1:1:0       yes    3800.0000 1600.0000
2   0    0      2    2:2:2:0       yes    3800.0000 1600.0000
3   0    0      3    3:3:3:0       yes    3800.0000 1600.0000
4   0    0      0    0:0:0:0       yes    3800.0000 1600.0000
5   0    0      1    1:1:1:0       yes    3800.0000 1600.0000
6   0    0      2    2:2:2:0       yes    3800.0000 1600.0000
7   0    0      3    3:3:3:0       yes    3800.0000 1600.0000
&lt;/pre&gt;
&lt;p&gt;There are 8 logical CPUs (&lt;tt class="docutils literal"&gt;CPU &lt;span class="pre"&gt;0..7&lt;/span&gt;&lt;/tt&gt;), all on the same node (&lt;tt class="docutils literal"&gt;NODE 0&lt;/tt&gt;) and
the same socket (&lt;tt class="docutils literal"&gt;SOCKET 0&lt;/tt&gt;).  There are only 4 physical cores (&lt;tt class="docutils literal"&gt;CORE
&lt;span class="pre"&gt;0..3&lt;/span&gt;&lt;/tt&gt;). For example, the physical core &lt;tt class="docutils literal"&gt;2&lt;/tt&gt; is made of the two logical CPUs:
&lt;tt class="docutils literal"&gt;2&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;6&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Using the &lt;tt class="docutils literal"&gt;L1d:L1i:L2:L3&lt;/tt&gt; column, we can see that each pair of two logical
cores share the same physical core caches for levels 1 (L1 data, L1
instruction) and 2 (L2).  All physical cores share the same cache level 3 (L3).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="p-states"&gt;
&lt;h2&gt;P-states&lt;/h2&gt;
&lt;p&gt;A new CPU driver &lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; was added to the Linux kernel 3.9 (April
2009). First, it only supported SandyBridge CPUs (2nd generation), Linux 3.10
extended it to Ivybridge generation CPUs (3rd gen), and so on and so forth.&lt;/p&gt;
&lt;p&gt;This driver supports recent features and thermal control of modern Intel CPUs.
Its name comes from P-states.&lt;/p&gt;
&lt;p&gt;The processor P-state is the capability of running the processor at different
voltage and/or frequency levels. Generally, P0 is the highest state resulting
in maximum performance, while P1, P2, and so on, will save power but at some
penalty to CPU performance.&lt;/p&gt;
&lt;p&gt;It is possible to force the legacy CPU driver (&lt;tt class="docutils literal"&gt;acpi_cpufreq&lt;/tt&gt;) using
&lt;tt class="docutils literal"&gt;intel_pstate=disable&lt;/tt&gt; option in the kernel command line.&lt;/p&gt;
&lt;p&gt;See also:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/cpu-freq/intel-pstate.txt"&gt;Documentation of the intel-pstate driver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://plus.google.com/+ArjanvandeVen/posts/dLn9T4ehywL"&gt;Some basics on CPU P states on Intel processors&lt;/a&gt; (2013) by Arjan
van de Ven (Intel)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://events.linuxfoundation.org/sites/events/files/slides/LinuxConEurope_2015.pdf"&gt;Balancing Power and Performance in the Linux Kernel&lt;/a&gt;
talk at LinuxCon Europe 2015 by Kristen Accardi (Intel)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://software.intel.com/en-us/blogs/2008/05/29/what-exactly-is-a-p-state-pt-1"&gt;What exactly is a P-state? (Pt. 1)&lt;/a&gt;
(2008) by Taylor K. (Intel)&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="idle-states-c-states"&gt;
&lt;h2&gt;Idle states: C-states&lt;/h2&gt;
&lt;p&gt;C-states are idle power saving states, in contrast to P-states, which are
execution power saving states.&lt;/p&gt;
&lt;p&gt;During a P-state, the processor is still executing instructions, whereas during
a C-state (other than C0), the processor is idle, meaning that nothing is
executing.&lt;/p&gt;
&lt;p&gt;C-states:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;C0 is the operational state, meaning that the CPU is doing useful work&lt;/li&gt;
&lt;li&gt;C1 is the first idle state&lt;/li&gt;
&lt;li&gt;C2 is the second idle state: The external I/O Controller Hub blocks
interrupts to the processor.&lt;/li&gt;
&lt;li&gt;etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;When a logical processor is idle (C-state except of C0), its frequency is
typically 0 (HALT).&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;cpupower &lt;span class="pre"&gt;idle-info&lt;/span&gt;&lt;/tt&gt; command lists supported C-states:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cpupower idle-info
CPUidle driver: intel_idle
CPUidle governor: menu
analyzing CPU 0:

Number of idle states: 6
Available idle states: POLL C1-IVB C1E-IVB C3-IVB C6-IVB C7-IVB
...
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;cpupower monitor&lt;/tt&gt; shows statistics on C-states:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
smithers$ sudo cpupower monitor -m Idle_Stats
    |Idle_Stats
CPU | POLL | C1-S | C1E- | C3-S | C6-S
   0|  0,00|  0,19|  0,09|  0,58| 96,23
   4|  0,00|  0,00|  0,00|  0,00| 99,90
   1|  0,00|  2,34|  0,00|  0,00| 97,63
   5|  0,00|  0,00|  0,17|  0,00| 98,02
   2|  0,00|  0,00|  0,00|  0,00|  0,00
   6|  0,00|  0,00|  0,00|  0,00|  0,00
   3|  0,00|  0,00|  0,00|  0,00|  0,00
   7|  0,00|  0,00|  0,00|  0,00| 49,97
&lt;/pre&gt;
&lt;p&gt;See also: &lt;a class="reference external" href="https://software.intel.com/en-us/articles/power-management-states-p-states-c-states-and-package-c-states"&gt;Power Management States: P-States, C-States, and Package C-States&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;Turbo Boost&lt;/h2&gt;
&lt;p&gt;In 2005, Intel introduced &lt;a class="reference external" href="https://en.wikipedia.org/wiki/SpeedStep"&gt;SpeedStep&lt;/a&gt;, a serie of dynamic frequency
scaling technologies to reduce the power consumption of laptop CPUs. Turbo
Boost is an enhancement of these technologies, now also used on desktop and
server CPUs.&lt;/p&gt;
&lt;p&gt;Turbo Boost allows to run one or many CPU cores to higher P-states than usual.
The maximum P-state is constrained by the following factors:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;The number of active cores (in C0 or C1 state)&lt;/li&gt;
&lt;li&gt;The estimated current consumption of the processor (Imax)&lt;/li&gt;
&lt;li&gt;The estimated power consumption (TDP - Thermal Design Power) of processor&lt;/li&gt;
&lt;li&gt;The temperature of the processor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example on my laptop:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cat /proc/cpuinfo
model name : Intel(R) Core(TM) i7-3520M CPU &amp;#64; 2.90GHz
...

selma$ sudo cpupower frequency-info
analyzing CPU 0:
  driver: intel_pstate
  ...
  boost state support:
    Supported: yes
    Active: yes
    3400 MHz max turbo 4 active cores
    3400 MHz max turbo 3 active cores
    3400 MHz max turbo 2 active cores
    3600 MHz max turbo 1 active cores
&lt;/pre&gt;
&lt;p&gt;The CPU base frequency is 2.9 GHz. If more than one physical cores is &amp;quot;active&amp;quot;
(busy), their frequency can be increased up to 3.4 GHz. If only 1 physical core
is active, its frequency can be increased up to 3.6 GHz.&lt;/p&gt;
&lt;p&gt;In this example, Turbo Boost is supported and active.&lt;/p&gt;
&lt;p&gt;See also the &lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/cpu-freq/boost.txt"&gt;Linux cpu-freq documentation on CPU boost&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="turbo-boost-msr"&gt;
&lt;h3&gt;Turbo Boost MSR&lt;/h3&gt;
&lt;p&gt;The bit 38 of the &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Model-specific_register"&gt;Model-specific register
(MSR)&lt;/a&gt; &lt;tt class="docutils literal"&gt;0x1a0&lt;/tt&gt; can
be used to check if the Turbo Boost is enabled:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ sudo rdmsr -f 38:38 0x1a0
0
&lt;/pre&gt;
&lt;p&gt;&lt;tt class="docutils literal"&gt;0&lt;/tt&gt; means that Turbo Boost is enabled, whereas &lt;tt class="docutils literal"&gt;1&lt;/tt&gt; means disabled (no
turbo). (The &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-f&lt;/span&gt; 38:38&lt;/tt&gt; option asks to only display the bit 38.)&lt;/p&gt;
&lt;p&gt;If the command doesn't work, you may have to load the &lt;tt class="docutils literal"&gt;msr&lt;/tt&gt; kernel module:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo modprobe msr
&lt;/pre&gt;
&lt;p&gt;Note: I'm not sure that all Intel CPU uses the same MSR.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="intel-state-no-turbo"&gt;
&lt;h3&gt;intel_state/no_turbo&lt;/h3&gt;
&lt;p&gt;Turbo Boost can also be disabled at runtime in the &lt;tt class="docutils literal"&gt;intel_pstate&lt;/tt&gt; driver.&lt;/p&gt;
&lt;p&gt;Check if Turbo Boost is enabled:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cat /sys/devices/system/cpu/intel_pstate/no_turbo
0
&lt;/pre&gt;
&lt;p&gt;where &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; means that Turbo Boost is enabled. Disable Turbo Boost:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ echo 1|sudo tee /sys/devices/system/cpu/intel_pstate/no_turbo
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="cpu-flag-ida"&gt;
&lt;h3&gt;CPU flag &amp;quot;ida&amp;quot;&lt;/h3&gt;
&lt;p&gt;It looks like the Turbo Boost status (supported or not) can also be read by the
CPUID(6): &amp;quot;Thermal/Power Management&amp;quot;. It gives access to the flag &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Dynamic_Acceleration"&gt;Intel
Dynamic Acceleration (IDA)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;ida&lt;/tt&gt; flag can also be seen in CPU flags of &lt;tt class="docutils literal"&gt;/proc/cpuinfo&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="read-the-cpu-frequency"&gt;
&lt;h2&gt;Read the CPU frequency&lt;/h2&gt;
&lt;p&gt;General information using &lt;tt class="docutils literal"&gt;cpupower &lt;span class="pre"&gt;frequency-info&lt;/span&gt;&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ cpupower -c 0 frequency-info
analyzing CPU 0:
  driver: intel_pstate
  ...
  hardware limits: 1.20 GHz - 3.60 GHz
  ...
&lt;/pre&gt;
&lt;p&gt;The frequency of CPUs is between 1.2 GHz and 3.6 GHz (the base frequency is
2.9 GHz on this CPU).&lt;/p&gt;
&lt;div class="section" id="get-the-frequency-of-cpus-turbostat"&gt;
&lt;h3&gt;Get the frequency of CPUs: turbostat&lt;/h3&gt;
&lt;p&gt;It looks like the most reliable way to get a relialistic estimation of the CPUs
frequency is to use the tool &lt;tt class="docutils literal"&gt;turbostat&lt;/tt&gt;:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ sudo turbostat
     CPU Avg_MHz   Busy% Bzy_MHz TSC_MHz
       -     224    7.80    2878    2893
       0     448   15.59    2878    2893
       1       0    0.01    2762    2893
     CPU Avg_MHz   Busy% Bzy_MHz TSC_MHz
       -     139    5.65    2469    2893
       0     278   11.29    2469    2893
       1       0    0.01    2686    2893
    ...
&lt;/pre&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;Avg_MHz&lt;/tt&gt;: average frequency, based on APERF&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;Busy%&lt;/tt&gt;: CPU usage in percent&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;Bzy_MHz&lt;/tt&gt;: busy frequency, based on MPERF&lt;/li&gt;
&lt;li&gt;&lt;tt class="docutils literal"&gt;TSC_MHz&lt;/tt&gt;: fixed frequency, TSC stands for &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Time_Stamp_Counter"&gt;Time Stamp Counter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;APERF (average) and MPERF (maximum) are MSR registers that can provide feedback
on current CPU frequency.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-tools-to-get-the-cpu-frequency"&gt;
&lt;h3&gt;Other tools to get the CPU frequency&lt;/h3&gt;
&lt;p&gt;It looks like the following tools are less reliable to estimate the CPU
frequency.&lt;/p&gt;
&lt;p&gt;cpuinfo:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ grep MHz /proc/cpuinfo
cpu MHz : 1372.289
cpu MHz : 3401.042
&lt;/pre&gt;
&lt;p&gt;In April 2016, Len Brown proposed a patch modifying cpuinfo to use APERF and
MPERF MSR to estimate the CPU frequency: &lt;a class="reference external" href="https://lkml.org/lkml/2016/4/1/7"&gt;x86: Calculate MHz using APERF/MPERF
for cpuinfo and scaling_cur_freq&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;tsc&lt;/tt&gt; clock source logs the CPU frequency in kernel logs:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ dmesg|grep 'MHz processor'
[    0.000000] tsc: Detected 2893.331 MHz processor
&lt;/pre&gt;
&lt;p&gt;cpupower frequency-info:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ for core in $(seq 0 1); do sudo cpupower -c $core frequency-info|grep 'current CPU'; done
  current CPU frequency: 3.48 GHz (asserted by call to hardware)
  current CPU frequency: 3.40 GHz (asserted by call to hardware)
&lt;/pre&gt;
&lt;p&gt;cpupower monitor:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
selma$ sudo cpupower monitor -m 'Mperf'
    |Mperf
CPU | C0   | Cx   | Freq
   0|  4.77| 95.23|  1924
   1|  0.01| 99.99|  1751
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Modern Intel CPUs use various technologies to provide best performances without
killing the power consumption. It became harder to monitor and understand CPU
performances, than with older CPUs, since the performance now depends on much
more factors.&lt;/p&gt;
&lt;p&gt;It also becomes common to get an integrated graphics processor (IGP) in the
same package, which makes the exact performance even more complex to predict,
since the IGP produces heat and so has an impact on the CPU P-state.&lt;/p&gt;
&lt;p&gt;I should also explain that P-state are &amp;quot;voted&amp;quot; between CPU cores, but I didn't
understand this part. I'm not sure that understanding the exact algorithm
matters much. I tried to not give too much information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="annex-amt-and-the-me-power-management-coprocessor"&gt;
&lt;h2&gt;Annex: AMT and the ME (power management coprocessor)&lt;/h2&gt;
&lt;p&gt;Computers with Intel vPro technology includes &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Active_Management_Technology"&gt;Intel Active Management
Technology (AMT)&lt;/a&gt;: &amp;quot;hardware
and firmware technology for remote out-of-band management of personal
computers&amp;quot;. AMT has many features which includes power management.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Intel_Active_Management_Technology#Hardware"&gt;Management Engine (ME)&lt;/a&gt;
is the hardware part: an isolated and protected coprocessor, embedded as a
non-optional part in all current (as of 2015) Intel chipsets. The coprocessor
is a special 32-bit ARC microprocessor (RISC architecture) that's physically
located inside the PCH chipset (or MCH on older chipsets). The coprocessor can
for example be found on Intel MCH chipsets Q35 and Q45.&lt;/p&gt;
&lt;p&gt;See &lt;a class="reference external" href="https://boingboing.net/2016/06/15/intel-x86-processors-ship-with.html"&gt;Intel x86s hide another CPU that can take over your machine (you can't
audit it)&lt;/a&gt; for
more information on the coprocessor.&lt;/p&gt;
&lt;p&gt;More recently, the Intel Xeon Phi CPU (2016) also includes a coprocessor for
power management. I didn't understand if it is the same coprocessor or not.&lt;/p&gt;
&lt;/div&gt;
</content><category term="optimization"></category><category term="benchmark"></category><category term="cpu"></category></entry><entry><title>Visualize the system noise using perf and CPU isolation</title><link href="https://haypo.github.io/perf-visualize-system-noise-with-cpu-isolation.html" rel="alternate"></link><published>2016-06-16T13:30:00+02:00</published><updated>2016-06-16T13:30:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-06-16:/perf-visualize-system-noise-with-cpu-isolation.html</id><summary type="html">&lt;p&gt;I developed a new &lt;a class="reference external" href="http://perf.readthedocs.io/"&gt;perf module&lt;/a&gt; designed to run
stable benchmarks, give fine control on benchmark parameters and compute
statistics on results. With such tool, it becomes simple to &lt;em&gt;visualize&lt;/em&gt;
sources of noise. The CPU isolation will be used to visualize the system noise.
Running a benchmark on isolated CPUs …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I developed a new &lt;a class="reference external" href="http://perf.readthedocs.io/"&gt;perf module&lt;/a&gt; designed to run
stable benchmarks, give fine control on benchmark parameters and compute
statistics on results. With such tool, it becomes simple to &lt;em&gt;visualize&lt;/em&gt;
sources of noise. The CPU isolation will be used to visualize the system noise.
Running a benchmark on isolated CPUs isolates it from the system noise.&lt;/p&gt;
&lt;div class="section" id="isolate-cpus"&gt;
&lt;h2&gt;Isolate CPUs&lt;/h2&gt;
&lt;p&gt;My computer has 4 physical CPU cores. I isolated half of them using
&lt;tt class="docutils literal"&gt;isolcpus=2,3&lt;/tt&gt; parameter of the Linux kernel. I modified manually the command
line in GRUB to add this parameter.&lt;/p&gt;
&lt;p&gt;Check that CPUs are isolated:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/isolated
2-3
&lt;/pre&gt;
&lt;p&gt;The CPU supports HyperThreading, but I disabled it in the BIOS.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="run-a-benchmark"&gt;
&lt;h2&gt;Run a benchmark&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf&lt;/tt&gt; module automatically detects and uses isolated CPU cores. I will
use the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--affinity=0,1&lt;/span&gt;&lt;/tt&gt; option to force running the benchmark on the CPUs
which are not isolated.&lt;/p&gt;
&lt;p&gt;Microbenchmark with and without CPU isolation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf.timeit --json-file=timeit_isolcpus.json --verbose -s 'x=1; y=2' 'x+y'
Pin process to isolated CPUs: 2-3
.........................
Median +- std dev: 36.6 ns +- 0.1 ns (25 runs x 3 samples x 10^7 loops; 1 warmup)

$ python3 -m perf.timeit --affinity=0,1 --json-file=timeit_no_isolcpus.json --verbose -s 'x=1; y=2' 'x+y'
Pin process to CPUs: 0-1
.........................
Median +- std dev: 36.7 ns +- 1.3 ns (25 runs x 3 samples x 10^7 loops; 1 warmup)
&lt;/pre&gt;
&lt;p&gt;My computer was not 100% idle, I was using it while the benchmarks were
running.&lt;/p&gt;
&lt;p&gt;The median is almost the same (36.6 ns and 36.7 ns). The first major difference
is the standard deviation: it is much larger without CPU isolation: 0.1 ns =&amp;gt;
1.3 ns (13x larger).&lt;/p&gt;
&lt;p&gt;Just in case, check manually CPU affinity in metadata:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf show timeit_isolcpus.json --metadata | grep cpu
- cpu_affinity: 2-3 (isolated)
- cpu_count: 4
- cpu_model_name: Intel(R) Core(TM) i7-2600 CPU &amp;#64; 3.40GHz

$ python3 -m perf show timeit_no_isolcpus.json --metadata | grep cpu_affinity
- cpu_affinity: 0-1
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="statistics"&gt;
&lt;h2&gt;Statistics&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf stats&lt;/tt&gt; command computes statistics on the distribution of samples:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf stats timeit_isolcpus.json
Number of samples: 75

Minimum: 36.5 ns (-0.1%)
Median +- std dev: 36.6 ns +- 0.1 ns (36.5 ns .. 36.7 ns)
Maximum: 36.7 ns (+0.4%)

$ python3 -m perf stats timeit_no_isolcpus.json
Number of samples: 75

Minimum: 36.5 ns (-0.5%)
Median +- std dev: 36.7 ns +- 1.3 ns (35.4 ns .. 38.0 ns)
Maximum: 43.0 ns (+17.0%)
&lt;/pre&gt;
&lt;p&gt;The minimum is the same. The second major difference is the maximum: it is much
larger without CPU isolation: 36.7 ns (+0.4%) =&amp;gt; 43.0 ns (+17.0%).&lt;/p&gt;
&lt;p&gt;The difference between the maximum and the median is 63x larger without CPU
isolation: 0.1 ns (&lt;tt class="docutils literal"&gt;36.7 - 36.6&lt;/tt&gt;) =&amp;gt; 6.3 ns (&lt;tt class="docutils literal"&gt;43.0 - 36.7&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;Depending on the system load, a single sample of the microbenchmark is up to
17% slower (maximum of 43.0 ns with a median of 36.7 ns) without CPU isolation.
The difference is smaller with CPU isolation: only 0.4% slower (for the
maximum, and 0.1% faster for the minimum).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="histogram"&gt;
&lt;h2&gt;Histogram&lt;/h2&gt;
&lt;p&gt;Another way to analyze the distribution of samples is to render an histogram:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m perf hist --bins=8 timeit_isolcpus.json timeit_no_isolcpus.json
[ timeit_isolcpus ]
36.1 ns: 75 ################################################
36.9 ns:  0 |
37.7 ns:  0 |
38.5 ns:  0 |
39.3 ns:  0 |
40.1 ns:  0 |
40.9 ns:  0 |
41.7 ns:  0 |
42.5 ns:  0 |

[ timeit_no_isolcpus ]
36.1 ns: 52 ################################################
36.9 ns: 13 ############
37.7 ns:  1 #
38.5 ns:  4 ####
39.3 ns:  2 ##
40.1 ns:  0 |
40.9 ns:  1 #
41.7 ns:  0 |
42.5 ns:  2 ##
&lt;/pre&gt;
&lt;p&gt;I choose the number of bars to get a small histogram and to get all samples of
the first benchmark on the same bar. With 8 bars, each bar is a range of 0.8
ns.&lt;/p&gt;
&lt;p&gt;The last major difference is the shape of these histogram. Without CPU
isolation, there is a &amp;quot;long tail&amp;quot; at the right of the median: &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Outlier"&gt;outliers&lt;/a&gt; in the range [37.7 ns; 42.5 ns].
The outliers come from the &amp;quot;noise&amp;quot; caused by the multitasking system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;perf&lt;/tt&gt; module provides multiple tools to analyze the distribution of
benchmark samples. Three tools show a major difference without CPU isolation
compared to results with CPU isolation:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Standard deviation: 13x larger without isolation&lt;/li&gt;
&lt;li&gt;Maximum: difference to median 63x larger without isolation&lt;/li&gt;
&lt;li&gt;Shape of the histogram: long tail at the right of the median&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It explains why CPU isolation helps to make benchmarks more stable.&lt;/p&gt;
&lt;/div&gt;
</content><category term="benchmark"></category></entry><entry><title>My journey to stable benchmark, part 3 (average)</title><link href="https://haypo.github.io/journey-to-stable-benchmark-average.html" rel="alternate"></link><published>2016-05-23T23:00:00+02:00</published><updated>2016-05-23T23:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-05-23:/journey-to-stable-benchmark-average.html</id><summary type="html">&lt;p class="first last"&gt;My journey to stable benchmark, part 3 (average)&lt;/p&gt;
</summary><content type="html">&lt;a class="reference external image-reference" href="https://www.flickr.com/photos/stanzim/11100202065/"&gt;&lt;img alt="Fog" src="https://haypo.github.io/images/fog.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;&lt;em&gt;Stable benchmarks are so close, but ...&lt;/em&gt;&lt;/p&gt;
&lt;div class="section" id="address-space-layout-randomization"&gt;
&lt;h2&gt;Address Space Layout Randomization&lt;/h2&gt;
&lt;p&gt;When I started to work on removing the noise of the system, I was told that
disabling &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Address_space_layout_randomization"&gt;Address Space Layout Randomization (ASLR)&lt;/a&gt; makes
benchmarks more stable.&lt;/p&gt;
&lt;p&gt;I followed this advice without trying to understand it. We will see in this
article that it was a bad idea, but I had to hit other issues to really
understand the root issue with disabling ASLR.&lt;/p&gt;
&lt;p&gt;Example of command to see the effect of ASLR, the first number of the output is
the start address of the heap memory:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python -c 'import os; os.system(&amp;quot;grep heap /proc/%s/maps&amp;quot; % os.getpid())'
55e6a716c000-55e6a7235000 rw-p 00000000 00:00 0                          [heap]
&lt;/pre&gt;
&lt;p&gt;Heap address of 3 runs with ASLR enabled (random):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;55e6a716c000&lt;/li&gt;
&lt;li&gt;561c218eb000&lt;/li&gt;
&lt;li&gt;55e6f628f000&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Disable ASLR:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo bash -c 'echo 0 &amp;gt;| /proc/sys/kernel/randomize_va_space'
&lt;/pre&gt;
&lt;p&gt;Heap addresses of 3 runs with ASLR disabled (all the same):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;555555756000&lt;/li&gt;
&lt;li&gt;555555756000&lt;/li&gt;
&lt;li&gt;555555756000&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: To reenable ASLR, it's better to use the value 2, the value 1 only
partially enables the feature:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
sudo bash -c 'echo 2 &amp;gt;| /proc/sys/kernel/randomize_va_space'
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="python-randomized-hash-function"&gt;
&lt;h2&gt;Python randomized hash function&lt;/h2&gt;
&lt;p&gt;With &lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-system.html"&gt;system tuning  (part 1)&lt;/a&gt;, a
&lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-deadcode.html"&gt;Python compiled with PGO (part 2)&lt;/a&gt;
and ASLR disabled, I still I failed to get the same result when running
manually &lt;tt class="docutils literal"&gt;bm_call_simple.py&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;On Python 3, the hash function is now randomized by default: &lt;a class="reference external" href="http://bugs.python.org/issue13703"&gt;issue #13703&lt;/a&gt;. The problem is that for a
microbenchmark, the number of hash collisions of an &amp;quot;hot&amp;quot; dictionary has a
non-negligible impact on performances.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; environment variable can be used to get a fixed hash
function. Example with the patch:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ PYTHONHASHSEED=1 taskset -c 1 ./python bm_call_simple.py -n 1
0.198
$ PYTHONHASHSEED=2 taskset -c 1 ./python bm_call_simple.py -n 1
0.201
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1
0.207
$ PYTHONHASHSEED=4 taskset -c 1 ./python bm_call_simple.py -n 1
0.187
$ PYTHONHASHSEED=5 taskset -c 1 ./python bm_call_simple.py -n 1
0.180
&lt;/pre&gt;
&lt;p&gt;Timings of the reference python:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ PYTHONHASHSEED=1 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.204
$ PYTHONHASHSEED=2 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.206
$ PYTHONHASHSEED=3 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.195
$ PYTHONHASHSEED=4 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.192
$ PYTHONHASHSEED=5 taskset -c 1 ./ref_python bm_call_simple.py -n 1
0.187
&lt;/pre&gt;
&lt;p&gt;The minimums is 180 ms for the reference and 186 ms for the patch. The patched
Python is 3% faster, yeah!&lt;/p&gt;
&lt;p&gt;Wait. What if we only test PYTHONHASHSEED from 1 to 3? In this case, the
minimum is 195 ms for the reference and 198 ms for the patch. The patched
Python becomes 2% slower, oh no!&lt;/p&gt;
&lt;p&gt;Faster? Slower? Who is right?&lt;/p&gt;
&lt;p&gt;Maybe I should write a script to find a &lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; value for which my
patch is always faster :-)&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="command-line-and-environment-variables"&gt;
&lt;h2&gt;Command line and environment variables&lt;/h2&gt;
&lt;p&gt;Well, let's say that we will use a fixed PYTHONHASHSEED value. Anyway, my
patch doesn't touch the hash function. So it doesn't matter.&lt;/p&gt;
&lt;p&gt;While running benchmarks, I noticed differences when running the benchmark from
a different directory:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cd /home/haypo/prog/python/fastcall
$ PYTHONHASHSEED=3 taskset -c 1 pgo/python ../benchmarks/performance/bm_call_simple.py -n 1
0.215

$ cd /home/haypo/prog/python/benchmarks
$ PYTHONHASHSEED=3 taskset -c 1 ../fastcall/pgo/python ../benchmarks/performance/bm_call_simple.py -n 1
0.203

$ cd /home/haypo/prog/python
$ PYTHONHASHSEED=3 taskset -c 1 fastcall/pgo/python benchmarks/performance/bm_call_simple.py -n 1
0.200
&lt;/pre&gt;
&lt;p&gt;In fact, a different command line is enough so get different results (added
arguments are ignored):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1
0.201
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1
0.198
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1 arg2 arg3
0.203
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1 arg2 arg3 arg4 arg5
0.206
$ PYTHONHASHSEED=3 taskset -c 1 ./python bm_call_simple.py -n 1 arg1 arg2 arg3 arg4 arg5 arg6
0.210
&lt;/pre&gt;
&lt;p&gt;I also noticed minor differences when the environment changes (added variables
are ignored):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 1
0.201
$ taskset -c 1 env -i PYTHONHASHSEED=3 VAR1=1 VAR2=2 VAR3=3 VAR4=4 ./python bm_call_simple.py -n 1
0.202
$ taskset -c 1 env -i PYTHONHASHSEED=3 VAR1=1 VAR2=2 VAR3=3 VAR4=4 VAR5=5 ./python bm_call_simple.py -n 1
0.198
&lt;/pre&gt;
&lt;p&gt;Using &lt;tt class="docutils literal"&gt;strace&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;ltrace&lt;/tt&gt;, I saw the memory addresses are different when
something (command line, env var, etc.) changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="average-and-standard-deviation"&gt;
&lt;h2&gt;Average and standard deviation&lt;/h2&gt;
&lt;p&gt;Basically, it looks like a lot of &amp;quot;external factors&amp;quot; have an impact on the
exact memory addresses, even if ASRL is disabled and PYTHONHASHSEED is set. I
started to think how to get &lt;em&gt;exactly&lt;/em&gt; the same command line, the same
environment (easy), the same current directory (easy), etc. The problem is that
it's just not possible to control all external factors (having an effect on the
exact memory addresses).&lt;/p&gt;
&lt;p&gt;Maybe I was plain wrong from the beginning and ASLR must be enabled,
as the default on Linux:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.198
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.202
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.199
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.207
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.200
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py
0.201
&lt;/pre&gt;
&lt;p&gt;These results look &amp;quot;random&amp;quot;. Yes, they are. It's exactly the purpose of ASLR.&lt;/p&gt;
&lt;p&gt;But how can we compare performances if results are random? Take the minimum?&lt;/p&gt;
&lt;p&gt;No! You must never (ever again) use the minimum for benchmarking! Compute the
average and some statistics like the standard deviation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3
Python 3.4.3
&amp;gt;&amp;gt;&amp;gt; timings=[0.198, 0.202, 0.199, 0.207, 0.200, 0.201]
&amp;gt;&amp;gt;&amp;gt; import statistics
&amp;gt;&amp;gt;&amp;gt; statistics.mean(timings)
0.2011666666666667
&amp;gt;&amp;gt;&amp;gt; statistics.stdev(timings)
0.0031885210782848245
&lt;/pre&gt;
&lt;p&gt;On this example, the average is 201 ms +/- 3 ms. IMHO the standard deviation is
quite small (reliable) which means that my benchmark is stable. To get a good
distribution, it's better to have many samples. It looks like at least 25
processes are needed. Each process tests a different memory layout and a
different hash function.&lt;/p&gt;
&lt;p&gt;Result of 5 runs, each run uses 25 processes (ASLR enabled, random hash
function):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Average: 205.2 ms +/- 3.0 ms (min: 201.1 ms, max: 214.9 ms)&lt;/li&gt;
&lt;li&gt;Average: 205.6 ms +/- 3.3 ms (min: 201.4 ms, max: 216.5 ms)&lt;/li&gt;
&lt;li&gt;Average: 206.0 ms +/- 3.9 ms (min: 201.1 ms, max: 215.3 ms)&lt;/li&gt;
&lt;li&gt;Average: 205.7 ms +/- 3.6 ms (min: 201.5 ms, max: 217.8 ms)&lt;/li&gt;
&lt;li&gt;Average: 206.4 ms +/- 3.5 ms (min: 201.9 ms, max: 214.9 ms)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While memory layout and hash functions are random again, the result looks
&lt;em&gt;less&lt;/em&gt; random, and so more reliable, than before!&lt;/p&gt;
&lt;p&gt;With ASLR enabled, the effect of the environment variables, command line and
current directory is negligible on the (average) result.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-average-solves-issues-with-uniform-random-noises"&gt;
&lt;h2&gt;The average solves issues with uniform random noises&lt;/h2&gt;
&lt;p&gt;The user will run the application with default system settings which means
ASLR enabled and Python hash function randomized. Running a benchmark in one
specific environment is a mistake because it is not representative of the
performance in practice.&lt;/p&gt;
&lt;p&gt;Computing the average and standard deviation &amp;quot;fixes&amp;quot; the issue with hash
randomization. It's much better to use random hash functions and compute the
average, than using a fixed hash function (setting &lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; variable
to a value).&lt;/p&gt;
&lt;p&gt;Oh wow, already 3 big articles explaing how to get stable benchmarks. Please
tell me that it was the last one!  Nope, more is coming...&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="annex-why-only-n1"&gt;
&lt;h2&gt;Annex: why only -n1?&lt;/h2&gt;
&lt;p&gt;In this article, I ran &lt;tt class="docutils literal"&gt;bm_call_simple.py&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-n&lt;/span&gt; 1&lt;/tt&gt; with only run one
iteration.&lt;/p&gt;
&lt;p&gt;Usually, a single iteration is not reliable at all, at least 50 iterations are
needed. But thanks to system tuning, compilation with PGO, ASRL disabled and
&lt;tt class="docutils literal"&gt;PYTHONHASHSEED&lt;/tt&gt; set, a single iteration is enough.&lt;/p&gt;
&lt;p&gt;Example of 3 runs, each with 3 iterations:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 3
0.201
0.201
0.201
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 3
0.201
0.201
0.201
$ taskset -c 1 env -i PYTHONHASHSEED=3 ./python bm_call_simple.py -n 3
0.201
0.201
0.201
&lt;/pre&gt;
&lt;p&gt;Always the same timing!&lt;/p&gt;
&lt;/div&gt;
</content><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>My journey to stable benchmark, part 2 (deadcode)</title><link href="https://haypo.github.io/journey-to-stable-benchmark-deadcode.html" rel="alternate"></link><published>2016-05-22T22:00:00+02:00</published><updated>2016-05-22T22:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-05-22:/journey-to-stable-benchmark-deadcode.html</id><summary type="html">&lt;p class="first last"&gt;My journey to stable benchmark, part 2 (deadcode)&lt;/p&gt;
</summary><content type="html">&lt;a class="reference external image-reference" href="https://www.flickr.com/photos/uw67/16875152403/"&gt;&lt;img alt="Snail" src="https://haypo.github.io/images/snail.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;With &lt;a class="reference external" href="https://haypo.github.io/journey-to-stable-benchmark-system.html"&gt;the system tuning (part 1)&lt;/a&gt;, I
expected to get very stable benchmarks and so I started to benchmark seriously
my &lt;a class="reference external" href="https://bugs.python.org/issue26814"&gt;FASTCALL branch&lt;/a&gt; of CPython (a new
calling convention avoiding temporary tuples).&lt;/p&gt;
&lt;p&gt;I was disappointed to get many slowdowns in the CPython benchmark suite. I
started to analyze why my change introduced performance regressions.&lt;/p&gt;
&lt;p&gt;I took my overall patch and slowly reverted more and more code to check which
changes introduced most of the slowdowns.&lt;/p&gt;
&lt;p&gt;I focused on the &lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; benchmark which does only one thing: call
Python functions which do nothing.  Making Python function calls slower would
be a big and inacceptable mistake of my work.&lt;/p&gt;
&lt;div class="section" id="linux-perf"&gt;
&lt;h2&gt;Linux perf&lt;/h2&gt;
&lt;p&gt;I started to learn how to use the great &lt;a class="reference external" href="https://perf.wiki.kernel.org/index.php/Main_Page"&gt;Linux perf&lt;/a&gt; tool to analyze why
&lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; was slower. I tried to find a major difference between my
reference python and the patched python.&lt;/p&gt;
&lt;p&gt;I analyzed cache misses on L1 instruction and data caches.  I analyzed stallen
CPU cycles. I analyzed all memory events, branch events, etc. Basically, I tried
all perf events and spent a lot of time to run benchmarks multiple times.&lt;/p&gt;
&lt;p&gt;By the way, I strongly suggest to use &lt;tt class="docutils literal"&gt;perf stat&lt;/tt&gt; using the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--repeat&lt;/span&gt;&lt;/tt&gt;
command line option to get an average on multiple runs and see the standard
deviation. It helps to get more reliable numbers. I even wrote a Python script
implementing &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;--repeat&lt;/span&gt;&lt;/tt&gt; (run perf multiple times, parse the output), before
seeing that it was already a builtin feature!&lt;/p&gt;
&lt;p&gt;Use &lt;tt class="docutils literal"&gt;perf list&lt;/tt&gt; to list all available (pre-defined) events.&lt;/p&gt;
&lt;p&gt;After many days, I decided to give up with perf.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cachegrind"&gt;
&lt;h2&gt;Cachegrind&lt;/h2&gt;
&lt;a class="reference external image-reference" href="http://valgrind.org/"&gt;&lt;img alt="Logo of the Valgrind project" src="https://haypo.github.io/images/valgrind.png" /&gt;&lt;/a&gt;
&lt;p&gt;&lt;a class="reference external" href="http://valgrind.org/"&gt;Valgrind&lt;/a&gt; is a great tool known to detect memory
leaks, but it also contains gems like the &lt;a class="reference external" href="http://valgrind.org/docs/manual/cg-manual.html"&gt;Cachegrind tool&lt;/a&gt; which &lt;em&gt;simulates&lt;/em&gt; the
CPU caches.&lt;/p&gt;
&lt;p&gt;I used Cachegrind with the nice &lt;a class="reference external" href="http://kcachegrind.sourceforge.net/"&gt;Kcachegrind GUI&lt;/a&gt;. Sadly, I also failed to see anything
obvious in cache misses between the reference python and the patched python.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="strace-and-ltrace"&gt;
&lt;h2&gt;strace and ltrace&lt;/h2&gt;
&lt;img alt="strace and ltrace" src="https://haypo.github.io/images/strace_ltrace.png" /&gt;
&lt;p&gt;I also tried &lt;tt class="docutils literal"&gt;strace&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;ltrace&lt;/tt&gt; tools to try to see a difference in the
execution of the reference and the patched pythons. I saw different memory
addresses, but no major difference which can explain a difference of the
timing.&lt;/p&gt;
&lt;p&gt;Morever, the hotcode simply does not call any syscall nor library
function. It's pure CPU-bound code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="compiler-options"&gt;
&lt;h2&gt;Compiler options&lt;/h2&gt;
&lt;a class="reference external image-reference" href="https://gcc.gnu.org/"&gt;&lt;img alt="GCC logo" class="align-right" src="https://haypo.github.io/images/gcc.png" /&gt;&lt;/a&gt;
&lt;p&gt;I used &lt;a class="reference external" href="https://gcc.gnu.org/"&gt;GCC&lt;/a&gt; to build to code. Just in case, I tried
LLVM compiler, but it didn't &amp;quot;fix&amp;quot; the issue.&lt;/p&gt;
&lt;p&gt;I also tried different optimization levels: &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O0&lt;/span&gt;&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O1&lt;/span&gt;&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O2&lt;/span&gt;&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-O3&lt;/span&gt;&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I read that the exact address of functions can have an impact on the CPU L1
cache: &lt;a class="reference external" href="https://stackoverflow.com/questions/19470873/why-does-gcc-generate-15-20-faster-code-if-i-optimize-for-size-instead-of-speed"&gt;Why does gcc generate 15-20% faster code if I optimize for size instead
of speed?&lt;/a&gt;.
I tried various values of the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-falign-functions=N&lt;/span&gt;&lt;/tt&gt; option (1, 2, 6, 12).&lt;/p&gt;
&lt;p&gt;I also tried &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-fomit-pointer&lt;/span&gt;&lt;/tt&gt; (omit frame pointer) to record the callgraph with &lt;tt class="docutils literal"&gt;perf record&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I also tried &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;-flto&lt;/span&gt;&lt;/tt&gt;: Link Time Optimization (LTO).&lt;/p&gt;
&lt;p&gt;These compiler options didn't fix the issue.&lt;/p&gt;
&lt;p&gt;The truth is out there.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt; See also &lt;a class="reference external" href="https://lwn.net/Articles/534735/"&gt;Rethinking optimization for size&lt;/a&gt; article on Linux Weekly News (LWN):
&lt;em&gt;&amp;quot;Such an option has obvious value if one is compiling for a
space-constrained environment like a small device. But it turns out that, in
some situations, optimizing for space can also produce faster code.&amp;quot;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="when-cpython-performance-depends-on-dead-code"&gt;
&lt;h2&gt;When CPython performance depends on dead code&lt;/h2&gt;
&lt;p&gt;I continued to revert changes. At the end, my giant patch was reduced to very
few changes only adding code which was never called (at least, I was sure
that it was not called in the &lt;tt class="docutils literal"&gt;call_simple&lt;/tt&gt; benchmark).&lt;/p&gt;
&lt;p&gt;Let me rephase: &lt;em&gt;adding dead code&lt;/em&gt; makes Python slower. What?&lt;/p&gt;
&lt;p&gt;A colleague suggested me to remove the body (replace it with &lt;tt class="docutils literal"&gt;return;&lt;/tt&gt;) of
added function: the code became faster. Ok, now I'm completely lost. To be
clear, I don't expect that adding dead code would have &lt;em&gt;any&lt;/em&gt; impact on the
performance.&lt;/p&gt;
&lt;p&gt;My email &lt;a class="reference external" href="https://mail.python.org/pipermail/speed/2016-April/000341.html"&gt;When CPython performance depends on dead code...&lt;/a&gt; explains how
to reproduce the issue and contains many information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="solution-pgo"&gt;
&lt;h2&gt;Solution: PGO&lt;/h2&gt;
&lt;p&gt;The solution is called Profiled Guided Optimization, &amp;quot;PGO&amp;quot;. Python build system
supports it in a single command: &lt;tt class="docutils literal"&gt;make &lt;span class="pre"&gt;profile-opt&lt;/span&gt;&lt;/tt&gt;. It profiles the
execution of the Python test suite.&lt;/p&gt;
&lt;p&gt;Using PGO, adding dead code has no more impact on the performance.&lt;/p&gt;
&lt;p&gt;With system tuning and PGO compilation, benchmarks must now be stable this
time, no? ... No, sorry, not yet. We will see more sources of noise in
following articles ;-)&lt;/p&gt;
&lt;/div&gt;
</content><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>My journey to stable benchmark, part 1 (system)</title><link href="https://haypo.github.io/journey-to-stable-benchmark-system.html" rel="alternate"></link><published>2016-05-21T16:50:00+02:00</published><updated>2016-05-21T16:50:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-05-21:/journey-to-stable-benchmark-system.html</id><summary type="html">&lt;p class="first last"&gt;My journey to stable benchmark, part 1&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="background"&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;In the CPython development, it became common to require the result of the
&lt;a class="reference external" href="https://hg.python.org/benchmarks"&gt;CPython benchmark suite&lt;/a&gt; (&amp;quot;The Grand
Unified Python Benchmark Suite&amp;quot;) to evaluate the effect of an optimization
patch. The minimum requirement is to not introduce performance regressions.&lt;/p&gt;
&lt;p&gt;I used the CPython benchmark suite and I had many bad surprises when trying to
analyze (understand) results. A change expected to be faster makes some
benchmarks slower without any obvious reason. At least, the change is expected
to be faster on some specific benchmarks, but have no impact on the other
benchmarks. The slowdown is usually between 5% and 10% slower. I am not
confortable with any kind of slowdown.&lt;/p&gt;
&lt;p&gt;Many benchmarks look unstable. The problem is to trust the overall report.
Some developers started to say that they learnt to ignore some benchmarks known
to be unstable.&lt;/p&gt;
&lt;p&gt;It's not the first time that I am totally disappointed by microbenchmark
results, so I decided to analyze completely the issue and go as deep as
possible to really understand the problem.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-to-get-stable-benchmarks-on-a-busy-linux-system"&gt;
&lt;h2&gt;How to get stable benchmarks on a busy Linux system&lt;/h2&gt;
&lt;p&gt;A common advice to get stable benchmark is to stay away the keyboard
(&amp;quot;freeze!&amp;quot;) and stop all other applications to only run one application, the
benchmark.&lt;/p&gt;
&lt;p&gt;Well, I'm working on a single computer and the full CPython benchmark suite
take up to 2 hours in rigorous mode. I just cannot stop working during 2 hours
to wait for the result of the benchmark. I like running benchmarks locally. It
is convenient to run benchmarks on the same computer used to develop.&lt;/p&gt;
&lt;p&gt;The goal here is to &amp;quot;remove the noise of the system&amp;quot;. Get the same result on a
busy system than an idle system. My simple &lt;a class="reference external" href="https://bitbucket.org/haypo/misc/src/tip/bin/system_load.py"&gt;system_load.py&lt;/a&gt; program can be
used to increase the system load. For example, run &lt;tt class="docutils literal"&gt;system_load.py 10&lt;/tt&gt; in a
terminal to get at least a system load of 10 (busy system) and run the
benchmark in a different terminal. Use CTRL+c to stop &lt;tt class="docutils literal"&gt;system_load.py&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="cpu-isolation"&gt;
&lt;h2&gt;CPU isolation&lt;/h2&gt;
&lt;p&gt;In 2016, it is common to get a CPU with multiple physical cores. For example,
my Intel CPU has 4 physical cores and 8 logical cores thanks to
&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Hyper-threading"&gt;Hyper-Threading&lt;/a&gt;. It is
possible to configure the Linux kernel to not schedule processes on some CPUs
using the &amp;quot;CPU isolation&amp;quot; feature. It is the &lt;tt class="docutils literal"&gt;isolcpus&lt;/tt&gt; parameter of the
Linux command line, the value is a list of CPUs. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
isolcpus=2,3,6,7
&lt;/pre&gt;
&lt;p&gt;Check with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/isolated
2-3,6-7
&lt;/pre&gt;
&lt;p&gt;If you have Hyper-Threading, you must isolate the two logicial cores of each
isolated physical core. You can use the &lt;tt class="docutils literal"&gt;lscpu &lt;span class="pre"&gt;--all&lt;/span&gt; &lt;span class="pre"&gt;--extended&lt;/span&gt;&lt;/tt&gt; command to
identify physical cores. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ lscpu -a -e
CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE MAXMHZ    MINMHZ
0   0    0      0    0:0:0:0       yes    5900,0000 1600,0000
1   0    0      1    1:1:1:0       yes    5900,0000 1600,0000
2   0    0      2    2:2:2:0       yes    5900,0000 1600,0000
3   0    0      3    3:3:3:0       yes    5900,0000 1600,0000
4   0    0      0    0:0:0:0       yes    5900,0000 1600,0000
5   0    0      1    1:1:1:0       yes    5900,0000 1600,0000
6   0    0      2    2:2:2:0       yes    5900,0000 1600,0000
7   0    0      3    3:3:3:0       yes    5900,0000 1600,0000
&lt;/pre&gt;
&lt;p&gt;The physical core &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; (CORE column) is made of two logical cores (CPU
column): &lt;tt class="docutils literal"&gt;0&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;4&lt;/tt&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="nohz-mode"&gt;
&lt;h2&gt;NOHZ mode&lt;/h2&gt;
&lt;p&gt;By default, the Linux kernel uses a scheduling-clock which interrupts the
running application &lt;tt class="docutils literal"&gt;HZ&lt;/tt&gt; times per second to run the scheduler. &lt;tt class="docutils literal"&gt;HZ&lt;/tt&gt; is
usually between 100 and 1000: time slice between 1 ms and 10 ms.&lt;/p&gt;
&lt;p&gt;Linux supports a &lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/timers/NO_HZ.txt"&gt;NOHZ mode&lt;/a&gt; which is able to
disable the scheduling-clock when the system is idle to reduce the power
consumption. Linux 3.10 introduces a &lt;a class="reference external" href="https://lwn.net/Articles/549580/"&gt;full ticketless mode&lt;/a&gt;, NOHZ full, which is able to disable the
scheduling-clock when only one application is running on a CPU.&lt;/p&gt;
&lt;p&gt;NOHZ full is disabled by default. It can be enabled with the &lt;tt class="docutils literal"&gt;nohz_full&lt;/tt&gt;
parameter of the Linux command line, the value is a list of CPUs. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
nohz_full=2,3,6,7
&lt;/pre&gt;
&lt;p&gt;Check with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ cat /sys/devices/system/cpu/nohz_full
2-3,6-7
&lt;/pre&gt;
&lt;/div&gt;
&lt;div class="section" id="interrupts-irq"&gt;
&lt;h2&gt;Interrupts (IRQ)&lt;/h2&gt;
&lt;p&gt;The Linux kernel can also be configured to not run &lt;a class="reference external" href="https://en.wikipedia.org/wiki/Interrupt_request_%28PC_architecture%29"&gt;interruptions (IRQ)&lt;/a&gt;
handlers on some CPUs using &lt;tt class="docutils literal"&gt;/proc/irq/default_smp_affinity&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;/proc/irq/&amp;lt;number&amp;gt;/smp_affinity&lt;/span&gt;&lt;/tt&gt; files. The value is not a list of CPUs but
a bitmask.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;/proc/interrupts&lt;/tt&gt; file can be read to see the number of interruptions
per CPU.&lt;/p&gt;
&lt;p&gt;Read the &lt;a class="reference external" href="https://www.kernel.org/doc/Documentation/IRQ-affinity.txt"&gt;Linux SMP IRQ affinity&lt;/a&gt; documentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="example-of-effect-of-cpu-isolation-on-a-microbenchmark"&gt;
&lt;h2&gt;Example of effect of CPU isolation on a microbenchmark&lt;/h2&gt;
&lt;p&gt;Example with Linux parameters:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
isolcpus=2,3,6,7 nohz_full=2,3,6,7
&lt;/pre&gt;
&lt;p&gt;Microbenchmark on an idle system (without CPU isolation):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 229 msec per loop
&lt;/pre&gt;
&lt;p&gt;Result on a busy system using &lt;tt class="docutils literal"&gt;system_load.py 10&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;find /&lt;/tt&gt; commands
running in other terminals:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 372 msec per loop
&lt;/pre&gt;
&lt;p&gt;The microbenchmark is 56% slower because of the high system load!&lt;/p&gt;
&lt;p&gt;Result on the same busy system but using isolated CPUs. The &lt;tt class="docutils literal"&gt;taskset&lt;/tt&gt; command
allows to pin an application to specific CPUs:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ taskset -c 1,3 python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 230 msec per loop
&lt;/pre&gt;
&lt;p&gt;Just to check, new run without CPU isolation:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
$ python3 -m timeit 'sum(range(10**7))'
10 loops, best of 3: 357 msec per loop
&lt;/pre&gt;
&lt;p&gt;The result with CPU isolation on a busy system is the same than the result an
idle system! CPU isolation removes most of the noise of the system.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Great job Linux!&lt;/p&gt;
&lt;p&gt;Ok! Now, the benchmark is super stable, no? ...  Sorry, no, it's not stable yet.
I found a lot of other sources of &amp;quot;noise&amp;quot;.  We will see them in the following
articles ;-)&lt;/p&gt;
&lt;/div&gt;
</content><category term="optimization"></category><category term="benchmark"></category></entry><entry><title>Status of Python 3 in OpenStack Mitaka</title><link href="https://haypo.github.io/openstack_mitaka_python3.html" rel="alternate"></link><published>2016-03-02T14:00:00+01:00</published><updated>2016-03-02T14:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-03-02:/openstack_mitaka_python3.html</id><summary type="html">&lt;p class="first last"&gt;Status of Python 3 in OpenStack Mitaka&lt;/p&gt;
</summary><content type="html">&lt;p&gt;Now that most OpenStack services have reached feature freeze for the Mitaka
cycle (November 2015-April 2016), it's time to look back on the progress made
for Python 3 support.&lt;/p&gt;
&lt;p&gt;Previous status update: &lt;a class="reference external" href="http://techs.enovance.com/7807/python-3-status-openstack-liberty"&gt;Python 3 Status in OpenStack Liberty&lt;/a&gt;
(September 2015).&lt;/p&gt;
&lt;div class="section" id="services-ported-to-python-3"&gt;
&lt;h2&gt;Services ported to Python 3&lt;/h2&gt;
&lt;p&gt;13 services were ported to Python 3 during the Mitaka cycle:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Cinder&lt;/li&gt;
&lt;li&gt;Congress&lt;/li&gt;
&lt;li&gt;Designate&lt;/li&gt;
&lt;li&gt;Glance&lt;/li&gt;
&lt;li&gt;Heat&lt;/li&gt;
&lt;li&gt;Horizon&lt;/li&gt;
&lt;li&gt;Manila&lt;/li&gt;
&lt;li&gt;Mistral&lt;/li&gt;
&lt;li&gt;Octavia&lt;/li&gt;
&lt;li&gt;Searchlight&lt;/li&gt;
&lt;li&gt;Solum&lt;/li&gt;
&lt;li&gt;Watcher&lt;/li&gt;
&lt;li&gt;Zaqar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Red Hat contributed to the Cinder, Designate, Glance and Horizon service
porting efforts.&lt;/p&gt;
&lt;p&gt;&amp;quot;Ported to Python 3&amp;quot; means that all unit tests pass on Python 3.4 which is
verified by a voting gate job. It is not enough to run applications in
production with Python 3. Integration and functional tests are not run on
Python 3 yet. See the section dedicated to these tests below.&lt;/p&gt;
&lt;p&gt;See the &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Python3"&gt;Python 3 wiki page&lt;/a&gt; for the
current status of the OpenStack port to Python 3; especially the list of
services ported to Python 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="services-not-ported-yet"&gt;
&lt;h2&gt;Services not ported yet&lt;/h2&gt;
&lt;p&gt;It's become easier to list services which are not compatible with Python 3 than
listing services already ported to Python 3!&lt;/p&gt;
&lt;p&gt;9 services still need to be ported:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Work-in-progress:&lt;ul&gt;
&lt;li&gt;Magnum: 83% (959 unit tests/1,161)&lt;/li&gt;
&lt;li&gt;Cue: 81% (208 unit tests/257)&lt;/li&gt;
&lt;li&gt;Nova: 74% (10,859 unit tests/14,726)&lt;/li&gt;
&lt;li&gt;Barbican: 34% (392 unit tests/1168)&lt;/li&gt;
&lt;li&gt;Murano: 29% (133 unit tests/455)&lt;/li&gt;
&lt;li&gt;Keystone: 27% (1200 unit tests/4455)&lt;/li&gt;
&lt;li&gt;Swift: 0% (3 unit tests/4,435)&lt;/li&gt;
&lt;li&gt;Neutron-LBaaS: 0% (1 unit test/806)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Port not started yet:&lt;ul&gt;
&lt;li&gt;Trove: no python34 gate&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Red Hat contributed Python 3 patches to Cue, Neutron-LBaaS, Swift and Trove
during the Mitaka cycle.&lt;/p&gt;
&lt;p&gt;Trove developers are ready to start the port at the beginning of the next cycle
(Newton). The py34 test environment was blocked by the MySQL-Python dependency (it
was not possible to build the test environment), but this dependency is now
skipped on Python 3. Later, it will be &lt;a class="reference external" href="https://review.openstack.org/#/c/225915/"&gt;replaced with PyMySQL&lt;/a&gt; on Python 2 and Python 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-issues-in-eventlet"&gt;
&lt;h2&gt;Python 3 issues in Eventlet&lt;/h2&gt;
&lt;p&gt;Four Python 3 issues were fixed in Eventlet:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/eventlet/eventlet/issues/295"&gt;Issue #295: Python 3: wsgi doesn't handle correctly partial write of
socket send() when using writelines()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PR #275: &lt;a class="reference external" href="https://github.com/eventlet/eventlet/pull/275"&gt;Issue #274: Fix GreenSocket.recv_into()&lt;/a&gt;.
Issue: &lt;a class="reference external" href="https://github.com/eventlet/eventlet/issues/274"&gt;On Python 3, sock.makefile('rb').readline() doesn't handle blocking
errors correctly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PR #257: &lt;a class="reference external" href="https://github.com/eventlet/eventlet/pull/257"&gt;Fix GreenFileIO.readall() for regular file&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/eventlet/eventlet/issues/248"&gt;Issue #248: eventlet.monkey_patch() on Python 3.4 makes stdout
non-blocking&lt;/a&gt;: pull
request &lt;a class="reference external" href="https://github.com/eventlet/eventlet/pull/250"&gt;Fix GreenFileIO.write()&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="next-milestone-functional-and-integration-tests"&gt;
&lt;h2&gt;Next Milestone: Functional and integration tests&lt;/h2&gt;
&lt;p&gt;The next major milestone will be to run functional and integration tests on
Python 3.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;functional tests are restricted to one component (ex: only Glance)&lt;/li&gt;
&lt;li&gt;integration tests, like Tempest, test the integration of multiple components&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is now possible to install some packages on Python 3 in DevStack using
&lt;tt class="docutils literal"&gt;USE_PYTHON3&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;PYTHON3_VERSION&lt;/tt&gt; variables: &lt;a class="reference external" href="https://review.openstack.org/#/c/181165/"&gt;Enable optional Python 3
support&lt;/a&gt;. It means that it is
possible to run tests with some services running on Python 3, and the remaining
services on Python 2.&lt;/p&gt;
&lt;p&gt;The port to Python 3 of Glance, Heat and Neutron functional and integration
tests have already started.&lt;/p&gt;
&lt;p&gt;For Glance, 159 functional tests already pass on Python 3.4.&lt;/p&gt;
&lt;p&gt;Heat:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;project-config: &lt;a class="reference external" href="https://review.openstack.org/#/c/228194/"&gt;Add python34 integration test job for Heat&lt;/a&gt; (WIP)&lt;/li&gt;
&lt;li&gt;heat: &lt;a class="reference external" href="https://review.openstack.org/#/c/188033/"&gt;py34: integration tests&lt;/a&gt;
(WIP)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Neutron: the &lt;a class="reference external" href="https://review.openstack.org/#/c/231897/"&gt;Add the functional-py34 and dsvm-functional-py34 targets to
tox.ini&lt;/a&gt; change was merged, but a
gate job hasn't been added for it yet.&lt;/p&gt;
&lt;p&gt;Another pending project is to fix issues specific to Python 3.5, but the gate
doesn’t use Python 3.5 yet. There are some minor issues, probably easy to fix.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="how-to-port-remaining-code"&gt;
&lt;h2&gt;How to port remaining code?&lt;/h2&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://wiki.openstack.org/wiki/Python3"&gt;Python 3 wiki page&lt;/a&gt; contains
a lot of information about adding Python 3 support to Python 2 code.&lt;/p&gt;
&lt;p&gt;Join us in the &lt;tt class="docutils literal"&gt;&lt;span class="pre"&gt;#openstack-python3&lt;/span&gt;&lt;/tt&gt; IRC channel on Freenode to discuss
Python 3!&lt;/p&gt;
&lt;/div&gt;
</content><category term="openstack"></category><category term="python3"></category></entry><entry><title>Fast _PyAccu, _PyUnicodeWriter and_PyBytesWriter APIs to produce strings in CPython</title><link href="https://haypo.github.io/pybyteswriter.html" rel="alternate"></link><published>2016-03-01T16:00:00+01:00</published><updated>2016-03-01T16:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-03-01:/pybyteswriter.html</id><summary type="html">&lt;p class="first last"&gt;_PyBytesWriter API&lt;/p&gt;
</summary><content type="html">&lt;p&gt;This article described the _PyBytesWriter and _PyUnicodeWriter private APIs of
CPython. These APIs are design to optimize code producing strings when the
ouput size is not known in advance.&lt;/p&gt;
&lt;p&gt;I created the _PyUnicodeWriter API to reply to complains that Python 3 was much
slower than Python 2, especially with the new Unicode implementation (PEP 393).&lt;/p&gt;
&lt;div class="section" id="pyaccu-api"&gt;
&lt;h2&gt;_PyAccu API&lt;/h2&gt;
&lt;p&gt;Issue #12778: In 2011, Antoine Pitrou found a performance issue in the JSON
serializer when serializing many small objects: it used way too much memory for
temporary objects compared to the final output string.&lt;/p&gt;
&lt;p&gt;The JSON serializer used a list of strings and joined all strings at the end of
create a final output string. Pseudocode:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def serialize():
    pieces = [serialize(item) for item in self]
    return ''.join(pieces)
&lt;/pre&gt;
&lt;p&gt;Antoine introduced an accumulator compacting the temporary list of &amp;quot;small&amp;quot;
strings and put the result in a second list of &amp;quot;large&amp;quot; strings. At the end, the
list of &amp;quot;large&amp;quot; strings was also compacted to build the final output string.
Pseudo-code:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def serialize():
    small = []
    large = []
    for item in self:
        small.append(serialize(item))
        if len(small) &amp;gt; 10000:
            large.append(''.join(small))
            small.clear()
    if small
        large.append(''.join(small))
    return ''.join(large)
&lt;/pre&gt;
&lt;p&gt;The threshold of 10,000  strings is justified by this comment:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
/* Each item in a list of unicode objects has an overhead (in 64-bit
 * builds) of:
 *   - 8 bytes for the list slot
 *   - 56 bytes for the header of the unicode object
 * that is, 64 bytes.  100000 such objects waste more than 6MB
 * compared to a single concatenated string.
 */
&lt;/pre&gt;
&lt;p&gt;Issue #12911: Antoine Pitrou found a similar performance issue in repr(list),
and so proposed to convert its accumular code into a new private _PyAccu API.
He added the _PyAccu API to Python 2.7.5 and 3.2.3. Title of te repr(list)
change: &amp;quot;Fix memory consumption when calculating the repr() of huge tuples or
lists&amp;quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-pyunicodewriter-api"&gt;
&lt;h2&gt;The _PyUnicodeWriter API&lt;/h2&gt;
&lt;div class="section" id="inefficient-implementation-of-the-pep-393"&gt;
&lt;h3&gt;Inefficient implementation of the PEP 393&lt;/h3&gt;
&lt;p&gt;In 2010, Python 3.3 got a completly new Unicode implementation, the Python type
&lt;tt class="docutils literal"&gt;str&lt;/tt&gt;, with the PEP 393. The implementation of the PEP was the topic of a
Google Summer of Code 2011 with the student Torsten Becker menthored by Martin
v. Löwis (author of the PEP). The project was successful: the PEP 393 was
implemented, it worked!&lt;/p&gt;
&lt;p&gt;The first implementation of the PEP 393 used a lot of 32-bit character buffers
(&lt;tt class="docutils literal"&gt;Py_UCS4&lt;/tt&gt;) which uses a lot of memory and requires expensive conversion to
8-bit (&lt;tt class="docutils literal"&gt;Py_UCS1&lt;/tt&gt;, ASCII and Latin1) or 16-bit (&lt;tt class="docutils literal"&gt;Py_UCS2&lt;/tt&gt;, BMP) characters.&lt;/p&gt;
&lt;p&gt;The new internal structures for Unicode strings are now very complex and
require to be smart when building a new string to avoid memory copies. I
created the _PyUnicodeWriter API to try to reduce expensive memory copies, and
even completly avoid memory copies in best cases.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="design-of-the-pyunicodewriter-api"&gt;
&lt;h3&gt;Design of the _PyUnicodeWriter API&lt;/h3&gt;
&lt;p&gt;According to benchmarks, creating a &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt; buffer and then expand it
to &lt;tt class="docutils literal"&gt;Py_UCS2*&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;Py_UCS4*&lt;/tt&gt; is more efficient, since &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt; is the
most common format.&lt;/p&gt;
&lt;p&gt;Python &lt;tt class="docutils literal"&gt;str&lt;/tt&gt; type is used for a wide range of usages. For example, it is used
for the name of variable names in the Python language itself. Variable names
are almost always ASCII.&lt;/p&gt;
&lt;p&gt;The worst case for _PyUnicodeWriter is when a long &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt; buffer must be
converted to &lt;tt class="docutils literal"&gt;Py_UCS2*&lt;/tt&gt;, and then converted to &lt;tt class="docutils literal"&gt;Py_UCS4*&lt;/tt&gt;. Each conversion
is expensive: need to allocate a second memory block and convert characters to
the new format.&lt;/p&gt;
&lt;p&gt;_PyUnicodeWriter features:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Optional overallocation: overallocate the buffer by 50% on Windows and 25%
on Linux. The ratio changes depending on the OS, it is a raw heuristic to get
the best performances depending on the &lt;tt class="docutils literal"&gt;malloc()&lt;/tt&gt; memory allocator.&lt;/li&gt;
&lt;li&gt;The buffer can be a shared read-only string if the buffer was only created
from a single string. Micro-optimization for &lt;tt class="docutils literal"&gt;&amp;quot;%s&amp;quot; % str&lt;/tt&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The API allows to disable overallocation before the last write. For example,
&lt;tt class="docutils literal"&gt;&amp;quot;%s%s&amp;quot; % ('abc', 'def')&lt;/tt&gt; disables the overallocation before writing
&lt;tt class="docutils literal"&gt;'def'&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;The _PyUnicodeWriter was introduced by the issue #14716 (change 7be716a47e9d):&lt;/p&gt;
&lt;blockquote&gt;
Close #14716: str.format() now uses the new &amp;quot;unicode writer&amp;quot; API instead
of the PyAccu API. For example, it makes str.format() from 25% to 30%
faster on Linux.&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="section" id="fast-path-for-ascii"&gt;
&lt;h3&gt;Fast-path for ASCII&lt;/h3&gt;
&lt;p&gt;The cool and &lt;em&gt;unexpected&lt;/em&gt; side-effect of the _PyUnicodeWriter is that many
intermediate operations got a fast-path for &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt;, especially for ASCII
strings. For example, padding a number with spaces on &lt;tt class="docutils literal"&gt;'%10i' % 123&lt;/tt&gt; is
implemented with &lt;tt class="docutils literal"&gt;memset()&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;Formating a floating point number uses the &lt;tt class="docutils literal"&gt;PyOS_double_to_string()&lt;/tt&gt; function
which creates an ASCII buffer. If the writer buffer uses Py_UCS1, a
&lt;tt class="docutils literal"&gt;memcpy()&lt;/tt&gt; is enough to copy the formatted number.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="avoid-temporary-buffers"&gt;
&lt;h3&gt;Avoid temporary buffers&lt;/h3&gt;
&lt;p&gt;Since the beginning, I had the idea of avoiding temporary buffers thanks
to an unified API to handle a &amp;quot;Unicode buffer&amp;quot;. Slowly, I spread my changes
to all functions producing Unicode strings.&lt;/p&gt;
&lt;p&gt;The obvious target were &lt;tt class="docutils literal"&gt;str % args&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;str.format(args)&lt;/tt&gt;. Both
instructions use very different code, but it was possible to share a few
functions especially the code to format integers in bases 2 (binary), 8
(octal), 10 (decimal) and 16 (hexadecimal).&lt;/p&gt;
&lt;p&gt;The function formatting an integer computes the exact size of the output,
requests a number of characters and then write characters. The characters are
written directly in the writer buffer. No temporary memory block is needed
anymore, and moreover no Py_UCS conversion is need: &lt;tt class="docutils literal"&gt;_PyLong_Format()&lt;/tt&gt; writes
directly characters into the character format (PyUCS1, Py_UCS2 or Py_UCS4) of
the buffer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="performance-compared-to-python-2"&gt;
&lt;h3&gt;Performance compared to Python 2&lt;/h3&gt;
&lt;p&gt;The PEP 393 uses a complex storage for strings, so the exact performances
now depends on the character set used in the benchmark. For benchmarks using
a character set different than ASCII, the result are more tricky to understand.&lt;/p&gt;
&lt;p&gt;To compare performances with Python 2, I focused my benchmarks on ASCII.  I
compared Python 3 str with Python 2 unicode, but also sometimes to Python 2 str
(bytes). On ASCII, Python 3.3 was as fast as Python 2, or even faster on some
very specific cases, but these cases are probably artificial and never seen in
real applications.&lt;/p&gt;
&lt;p&gt;In the best case, Python 3 str (Unicode) was faster than Python 2 bytes.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="pybyteswriter-api-first-try-big-fail"&gt;
&lt;h2&gt;_PyBytesWriter API: first try, big fail&lt;/h2&gt;
&lt;p&gt;Since Python was &lt;em&gt;much&lt;/em&gt; faster with _PyUnicodeWriter, I expected to get good
speedup with a similar API for bytes. The graal would be to share code for
bytes and Unicode (Spoiler alert! I reached this goal, but only for a single
function: format an integer to decimal).&lt;/p&gt;
&lt;p&gt;My first attempt of a _PyBytesWriter API was in 2013: &lt;a class="reference external" href="https://bugs.python.org/issue17742"&gt;Issue #17742: Add
_PyBytesWriter API&lt;/a&gt;. But quickly, I
noticed with microbenchmarks that my change made Python slower! I spent hours
to understand why GCC produced less efficient machine code. When I started to
dig the &amp;quot;strict aliasing&amp;quot; optimization issue, I realized that I reached a
deadend.&lt;/p&gt;
&lt;p&gt;Extract of the _PyBytesWriter structure:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
typedef struct {
    /* Current position in the buffer */
    char *str;

    /* Start of the buffer */
    char *start;

    /* End of the buffer */
    char *end;

    ...
} _PyBytesWriter;
&lt;/pre&gt;
&lt;p&gt;The problem is that GCC emited less efficient machine code for the C code (see
my &lt;a class="reference external" href="https://bugs.python.org/issue17742#msg187595"&gt;msg187595&lt;/a&gt;):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
while (collstart++&amp;lt;collend)
    *writer.str++ = '?';
&lt;/pre&gt;
&lt;p&gt;For the &lt;tt class="docutils literal"&gt;writer.str++&lt;/tt&gt; instruction, the new pointer value is written
immediatly in the structure. The pointer value is read again at each iteration.
So we have 1 LOAD and 1 STORE per iteration.&lt;/p&gt;
&lt;p&gt;GCC emits better code for the original C code:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
while (collstart++&amp;lt;collend)
    *str++ = '?';
&lt;/pre&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;str&lt;/tt&gt; variable is stored in a register and the new value of &lt;tt class="docutils literal"&gt;str&lt;/tt&gt; is
only written &lt;em&gt;once&lt;/em&gt;, at the end of loop (instead of writing it at each
iteration). The pointer value is &lt;em&gt;only read once&lt;/em&gt; before the loop. So we have 0
LOAD and 0 STORE (related to the pointer value) in the loop body.&lt;/p&gt;
&lt;p&gt;It looks like an aliasing issue, but I didn't find how to say to GCC that the
new value of &lt;tt class="docutils literal"&gt;writer.str&lt;/tt&gt; can be written only once at the end of the loop. I
tried the &lt;tt class="docutils literal"&gt;__restrict__&lt;/tt&gt; keyword: the LOAD (get the pointer value) was moved
out of the loop. But the STORE was still in the loop body.&lt;/p&gt;
&lt;p&gt;I wrote to gcc-help: &lt;a class="reference external" href="https://gcc.gnu.org/ml/gcc-help/2013-04/msg00192.html"&gt;Missed optimization when using a structure&lt;/a&gt;, but I didn't get any
reply. I just gave up.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="pybyteswriter-api-new-try-the-good-one"&gt;
&lt;h2&gt;_PyBytesWriter API: new try, the good one&lt;/h2&gt;
&lt;p&gt;In 2015, I created the &lt;a class="reference external" href="https://bugs.python.org/issue25318"&gt;Issue #25318: Add _PyBytesWriter API to optimize
Unicode encoders&lt;/a&gt;. I redesigned the API
to avoid the aliasing issue.&lt;/p&gt;
&lt;p&gt;The new _PyBytesWriter doesn't contain the &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; pointers anymore: they are
now local variables in functions. Instead, functions of API requires two
parameters: the bytes writer and a &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; parameter. Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
PyObject * _PyBytesWriter_Finish(_PyBytesWriter *writer, char *str)
&lt;/pre&gt;
&lt;p&gt;The idea is to keep &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; pointers in functions to keep the most efficient
machine code in loops. The compiler doesn't have to compute complex aliasing
rules to decide if a CPU register can be used or not.&lt;/p&gt;
&lt;p&gt;_PyBytesWriter features:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Optional overallocation: overallocate the buffer by 25% on Windows and 50%
on Linux. Same idea than _PyUnicodeWriter.&lt;/li&gt;
&lt;li&gt;Support &lt;tt class="docutils literal"&gt;bytes&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;bytearray&lt;/tt&gt; type as output format to avoid an expensive
memory copy from &lt;tt class="docutils literal"&gt;bytes&lt;/tt&gt; to &lt;tt class="docutils literal"&gt;bytearray&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Small buffer of 512 bytes allocated on the stack to avoid the need of a
buffer allocated on the heap, before creating the final
&lt;tt class="docutils literal"&gt;bytes&lt;/tt&gt;/&lt;tt class="docutils literal"&gt;bytearray&lt;/tt&gt; object.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A _PyBytesWriter structure must always be allocated on the stack (to get fast
memory allocation of the smaller buffer).&lt;/p&gt;
&lt;p&gt;While _PyUnicodeWriter has a 5 functions and 1 macro to write a single
character, write strings, write a substring, etc. _PyBytesWriter has a single
_PyBytesWriter_WriteBytes() function to write a string, since all other writes
are done directly with regular C code on &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; pointers.&lt;/p&gt;
&lt;p&gt;The API itself doesn't make the code faster. Disabling overallocation on the
last write and the usage of the small buffer allocated on the stack may be
faster.&lt;/p&gt;
&lt;p&gt;In Python 3.6, I optimized error handlers on various codecs: ASCII, Latin1
and UTF-8. For example, the UTF-8 encoder is now up to 75 times as fast for
error handlers: &lt;tt class="docutils literal"&gt;ignore&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;replace&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;surrogateescape&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;surrogatepass&lt;/tt&gt;. The &lt;tt class="docutils literal"&gt;bytes % int&lt;/tt&gt; instruction became between 30% and 50%
faster on a microbenchmark.&lt;/p&gt;
&lt;p&gt;Later, I replaced &lt;tt class="docutils literal"&gt;char*&lt;/tt&gt; type with &lt;tt class="docutils literal"&gt;void*&lt;/tt&gt; to avoid compiler warnings
in functions using &lt;tt class="docutils literal"&gt;Py_UCS1*&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;unsigned char*&lt;/tt&gt;, unsigned types.&lt;/p&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2015 Q4</title><link href="https://haypo.github.io/contrib-cpython-2015q4.html" rel="alternate"></link><published>2016-03-01T15:00:00+01:00</published><updated>2016-03-01T15:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-03-01:/contrib-cpython-2015q4.html</id><summary type="html">&lt;p class="first last"&gt;My contributions to CPython during 2015 Q4&lt;/p&gt;
</summary><content type="html">&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2015 Q4
(october, november, december):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg log -r 'date(&amp;quot;2015-10-01&amp;quot;):date(&amp;quot;2015-12-31&amp;quot;)' --no-merges -u Stinner
&lt;/pre&gt;
&lt;p&gt;Statistics: 100 non-merge commits + 25 merge commits (total: 125 commits).&lt;/p&gt;
&lt;p&gt;As usual, I pushed changes of various contributors and helped them to polish
their change.&lt;/p&gt;
&lt;p&gt;I fighted against a recursion error, a regression introduced by my recent work
on the Python test suite.&lt;/p&gt;
&lt;p&gt;I focused on optimizing the bytes type during this quarter. It started with the
issue #24870 opened by &lt;strong&gt;INADA Naoki&lt;/strong&gt; who works on PyMySQL: decoding bytes
using the surrogateescape error handler was the bottleneck of this benchmark.
For me, it was an opportunity for a new attempt to implement a fast &amp;quot;bytes
writer API&amp;quot;.&lt;/p&gt;
&lt;p&gt;I pushed my first change related to &lt;a class="reference external" href="http://faster-cpython.readthedocs.org/fat_python.html"&gt;FAT Python&lt;/a&gt;! Fix parser and AST:
fill lineno and col_offset of &amp;quot;arg&amp;quot; node when compiling AST from Python
objects.&lt;/p&gt;
&lt;p&gt;Previous report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2015q3.html"&gt;My contributions to CPython during 2015 Q3&lt;/a&gt;. Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2016q1.html"&gt;My contributions to
CPython during 2016 Q1&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="recursion-error"&gt;
&lt;h2&gt;Recursion error&lt;/h2&gt;
&lt;div class="section" id="the-bug-issue-25274"&gt;
&lt;h3&gt;The bug: issue #25274&lt;/h3&gt;
&lt;p&gt;During the previous quarter, I refactored Lib/test/regrtest.py huge file (1,600
lines) into a new Lib/test/libregrtest/ library (8 files). The problem is that
test_sys started to crash with &amp;quot;Fatal Python error: Cannot recover from stack
overflow&amp;quot; on test_recursionlimit_recovery(). The regression was introduced by a
change on regrtest which indirectly added one more Python frame in the code
executing test_sys.&lt;/p&gt;
&lt;p&gt;CPython has a limit on the depth of a call stack: &lt;tt class="docutils literal"&gt;sys.getrecursionlimit()&lt;/tt&gt;,
1000 by default. The limit is a weak protection against overflow of the C
stack. Weak because it only counts Python frames, intermediate C functions may
allocate a lot of memory on the stack.&lt;/p&gt;
&lt;p&gt;When we reach the limit, an &amp;quot;overflow&amp;quot; flag is set, but we still allow up to
limit+50 frames, because handling a RecursionError may need a few more frames.
The overflow flag is cleared when the stack level goes below a &amp;quot;low-water
mark&amp;quot;.&lt;/p&gt;
&lt;p&gt;After the regrtest change, test_recursionlimit_recovery() was called at stack
level 36. Before, it was called at level 35. The test triggers a RecursionError.
The problem is that we never goes again below the low-water mark, so the
overflow flag is never cleared.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-fix"&gt;
&lt;h3&gt;The fix&lt;/h3&gt;
&lt;p&gt;Another problem is that the function used to compute the &amp;quot;low-level mark&amp;quot; was
not monotonic:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
if limit &amp;gt; 100:
    low_water_mark = limit - 50
else:
    low_water_mark = 3 * limit // 4
&lt;/pre&gt;
&lt;p&gt;The gap occurs near a limit of 100 frames:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;limit = 99 =&amp;gt; low_level_mark = 74&lt;/li&gt;
&lt;li&gt;limit = 100 =&amp;gt; low_level_mark = 75&lt;/li&gt;
&lt;li&gt;limit = 101 =&amp;gt; low_level_mark = 51&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The formula was replaced with:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
if limit &amp;gt; 200:
    low_water_mark = limit - 50
else:
    low_water_mark = 3 * limit // 4
&lt;/pre&gt;
&lt;p&gt;The fix (&lt;a class="reference external" href="https://hg.python.org/cpython/rev/eb0c76442cee"&gt;change eb0c76442cee&lt;/a&gt;) modified the
&lt;tt class="docutils literal"&gt;sys.setrecursionlimit()&lt;/tt&gt; function to raise a &lt;tt class="docutils literal"&gt;RecursionError&lt;/tt&gt; exception if
the new limit is too low depending on the &lt;em&gt;current&lt;/em&gt; stack depth.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizations"&gt;
&lt;h2&gt;Optimizations&lt;/h2&gt;
&lt;p&gt;As usual for performance, Serhiy Storchaka was very helpful on reviews, to run
independant benchmarks, etc.&lt;/p&gt;
&lt;p&gt;Optimizations on the &lt;tt class="docutils literal"&gt;bytes&lt;/tt&gt; type, ASCII, Latin1 and UTF-8 codecs:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #25318: Add _PyBytesWriter API. Add a new private API to optimize
Unicode encoders. It uses a small buffer of 512 bytes allocated on the stack
and supports configurable overallocation.&lt;/li&gt;
&lt;li&gt;Use _PyBytesWriter API for UCS1 (ASCII and Latin1) and UTF-8 encoders. Enable
overallocation for the UTF-8 encoder with error handlers.&lt;/li&gt;
&lt;li&gt;unicode_encode_ucs1(): initialize collend to collstart+1 to not check the
current character twice, we already know that it is not ASCII.&lt;/li&gt;
&lt;li&gt;Issue #25267: The UTF-8 encoder is now up to 75 times as fast for error
handlers: &lt;tt class="docutils literal"&gt;ignore&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;replace&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;surrogateescape&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;surrogatepass&lt;/tt&gt;.
Patch co-written with &lt;strong&gt;Serhiy Storchaka&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Issue #25301: The UTF-8 decoder is now up to 15 times as fast for error
handlers: &lt;tt class="docutils literal"&gt;ignore&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;replace&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;surrogateescape&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Issue #25318: Optimize backslashreplace and xmlcharrefreplace error handlers
in UTF-8 encoder. Optimize also backslashreplace error handler for ASCII and
Latin1 encoders.&lt;/li&gt;
&lt;li&gt;Issue #25349: Optimize bytes % args using the new private _PyBytesWriter API&lt;/li&gt;
&lt;li&gt;Optimize error handlers of ASCII and Latin1 encoders when the replacement
string is pure ASCII: use _PyBytesWriter_WriteBytes(), don't check individual
character.&lt;/li&gt;
&lt;li&gt;Issue #25349: Optimize bytes % int. Formatting is between 30% and 50% faster
on a microbenchmark.&lt;/li&gt;
&lt;li&gt;Issue #25357: Add an optional newline paramer to binascii.b2a_base64().
base64.b64encode() uses it to avoid a memory copy.&lt;/li&gt;
&lt;li&gt;Issue #25353: Optimize unicode escape and raw unicode escape encoders: use
the new _PyBytesWriter API.&lt;/li&gt;
&lt;li&gt;Rewrite PyBytes_FromFormatV() using _PyBytesWriter API&lt;/li&gt;
&lt;li&gt;Issue #25399: Optimize bytearray % args. Most formatting operations are now
between 2.5 and 5 times faster.&lt;/li&gt;
&lt;li&gt;Issue #25401: Optimize bytes.fromhex() and bytearray.fromhex(): they are now
between 2x and 3.5x faster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="changes"&gt;
&lt;h2&gt;Changes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #25003: On Solaris 11.3 or newer, os.urandom() now uses the getrandom()
function instead of the getentropy() function. The getentropy() function is
blocking to generate very good quality entropy, os.urandom() doesn't need
such high-quality entropy.&lt;/li&gt;
&lt;li&gt;Issue #22806: Add &lt;tt class="docutils literal"&gt;python &lt;span class="pre"&gt;-m&lt;/span&gt; test &lt;span class="pre"&gt;--list-tests&lt;/span&gt;&lt;/tt&gt; command to list tests.&lt;/li&gt;
&lt;li&gt;Issue #25670: Remove duplicate getattr() in ast.NodeTransformer&lt;/li&gt;
&lt;li&gt;Issue #25557: Refactor _PyDict_LoadGlobal(). Don't fallback to
PyDict_GetItemWithError() if the hash is unknown: compute the hash instead.
Add also comments to explain the _PyDict_LoadGlobal() optimization.&lt;/li&gt;
&lt;li&gt;Issue #25868: Try to make test_eintr.test_sigwaitinfo() more reliable
especially on slow buildbots&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="changes-specific-to-python-2-7"&gt;
&lt;h2&gt;Changes specific to Python 2.7&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Closes #25742: locale.setlocale() now accepts a Unicode string for its second
parameter.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="bugfixes"&gt;
&lt;h2&gt;Bugfixes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Fix regrtest --coverage on Windows&lt;/li&gt;
&lt;li&gt;Fix pytime on OpenBSD&lt;/li&gt;
&lt;li&gt;More fixes for test_eintr on FreeBSD&lt;/li&gt;
&lt;li&gt;Close #25373: Fix regrtest --slow with interrupted test&lt;/li&gt;
&lt;li&gt;Issue #25555: Fix parser and AST: fill lineno and col_offset of &amp;quot;arg&amp;quot; node
when compiling AST from Python objects. First contribution related
to FAT Python ;-)&lt;/li&gt;
&lt;li&gt;Issue #25696: Fix installation of Python on UNIX with make -j9.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>My contributions to CPython during 2015 Q3</title><link href="https://haypo.github.io/contrib-cpython-2015q3.html" rel="alternate"></link><published>2016-02-18T01:00:00+01:00</published><updated>2016-02-18T01:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-02-18:/contrib-cpython-2015q3.html</id><summary type="html">&lt;p class="first last"&gt;My contributions to CPython during 2015 Q3&lt;/p&gt;
</summary><content type="html">&lt;p&gt;A few years ago, someone asked me: &amp;quot;Why do you contribute to CPython? Python is
perfect, there are no more bugs, right?&amp;quot;. The article list most of my
contributions to CPython during 2015 Q3 (july, august, september). It gives an
idea of which areas of Python are not perfect yet :-)&lt;/p&gt;
&lt;p&gt;My contributions to &lt;a class="reference external" href="https://www.python.org/"&gt;CPython&lt;/a&gt; during 2015 Q3
(july, august, september):&lt;/p&gt;
&lt;pre class="literal-block"&gt;
hg log -r 'date(&amp;quot;2015-07-01&amp;quot;):date(&amp;quot;2015-09-30&amp;quot;)' --no-merges -u Stinner
&lt;/pre&gt;
&lt;p&gt;Statistics: 153 non-merge commits + 75 merge commits (total: 228 commits).&lt;/p&gt;
&lt;p&gt;The major event in Python of this quarter was the release of Python 3.5.0.&lt;/p&gt;
&lt;p&gt;As usual, I helped various contributors to refine their changes and I pushed
their final changes.&lt;/p&gt;
&lt;p&gt;Next report: &lt;a class="reference external" href="https://haypo.github.io/contrib-cpython-2015q4.html"&gt;My contributions to CPython during 2015 Q4&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="freebsd-kernel-bug"&gt;
&lt;h2&gt;FreeBSD kernel bug&lt;/h2&gt;
&lt;p&gt;It took me a while to polish the implementation of the &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0475/"&gt;PEP 475 (retry syscall
on EINTR)&lt;/a&gt; especially its unit
test &lt;tt class="docutils literal"&gt;test_eintr&lt;/tt&gt;. The unit test is supposed to test Python, but as usual,
it also tests indirectly the operating system.&lt;/p&gt;
&lt;p&gt;I spent some days investigating a random hang on the FreeBSD buildbots: &lt;a class="reference external" href="https://bugs.python.org/issue25122"&gt;issue
#25122&lt;/a&gt;. I quickly found the guilty test
(test_eintr.test_open), but it took me a while to understand that it was a
kernel bug in the FIFO driver. Hopefully at the end, I was able to reproduce
the bug with a short C program in my FreeBSD VM. It is the best way to ask a
fix upstream.&lt;/p&gt;
&lt;p&gt;My &lt;a class="reference external" href="https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=203162"&gt;FreeBSD bug report #203162&lt;/a&gt; (&amp;quot;when close(fd)
on a fifo fails with EINTR, the file descriptor is not really closed&amp;quot;) was
quickly fixed. The FreeBSD team is reactive!&lt;/p&gt;
&lt;p&gt;I like free softwares because it's possible to investigate bugs deep in the
code, and it's usually quick to get a fix.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="timestamp-rounding-issue"&gt;
&lt;h2&gt;Timestamp rounding issue&lt;/h2&gt;
&lt;p&gt;Even if the &lt;a class="reference external" href="http://bugs.python.org/issue23517"&gt;issue #23517&lt;/a&gt; is well defined
and simple to fix, it took me days (weeks?) to understand exactly how
timestamps are supposed to be rounded and agree on the &amp;quot;right&amp;quot; rounding method.
Alexander Belopolsky reminded me the important property:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
(datetime(1970,1,1) + timedelta(seconds=t)) == datetime.utcfromtimestamp(t)
&lt;/pre&gt;
&lt;p&gt;Tim Peters helped me to understand why Python rounds to nearest with ties going
away from zero (ROUND_HALF_UP) in &lt;tt class="docutils literal"&gt;round(float)&lt;/tt&gt; and other functions. At
the first look, the rounding method doesn't look natural nor logical:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
&amp;gt;&amp;gt;&amp;gt; round(0.5)
0
&amp;gt;&amp;gt;&amp;gt; round(1.5)
2
&lt;/pre&gt;
&lt;p&gt;See my previous article on the _PyTime API for the long story of rounding
methods between Python 3.2 and Python 3.6: &lt;a class="reference external" href="https://haypo.github.io/pytime.html"&gt;History of the Python private C API
_PyTime&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="enhancements"&gt;
&lt;h2&gt;Enhancements&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;type_call() now detect C bugs in type __new__() and __init__() methods.&lt;/li&gt;
&lt;li&gt;Issue #25220: Enhancements of the test runner: add more info when regrtest runs
tests in parallel, fix some features of regrtest, add functional tests to
test_regrtest.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="optimizations"&gt;
&lt;h2&gt;Optimizations&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Issue #25227: Optimize ASCII and latin1 encoders with the &lt;tt class="docutils literal"&gt;surrogateescape&lt;/tt&gt;
error handler: the encoders are now up to 3 times as fast.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="changes"&gt;
&lt;h2&gt;Changes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Polish the implementation of the PEP 475 (retry syscall on EINTR)&lt;/li&gt;
&lt;li&gt;Work on the &amp;quot;What's New in Python 3.5&amp;quot; document: add my changes
(PEP 475, socket timeout, os.urandom)&lt;/li&gt;
&lt;li&gt;Work on asyncio: fix ResourceWarning warnings, fixes specific to Windows&lt;/li&gt;
&lt;li&gt;test_time: rewrite rounding tests of the private pytime API&lt;/li&gt;
&lt;li&gt;Issue #24707: Remove an assertion in monotonic clock. Don't check anymore at
runtime that the monotonic clock doesn't go backward.  Yes, it happens! It
occurs sometimes each month on a Debian buildbot slave running in a VM.&lt;/li&gt;
&lt;li&gt;test_eintr: replace os.fork() with subprocess (fork+exec) to make the test
more reliable&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="changes-specific-to-python-2-7"&gt;
&lt;h2&gt;Changes specific to Python 2.7&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Backport python-gdb.py changes: enhance py-bt command&lt;/li&gt;
&lt;li&gt;Issue #23375: Fix test_py3kwarn for modules implemented in C&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="bug-fixes"&gt;
&lt;h2&gt;Bug fixes&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Closes #23247: Fix a crash in the StreamWriter.reset() of CJK codecs&lt;/li&gt;
&lt;li&gt;Issue #24732, #23834: Fix sock_accept_impl() on Windows. Regression of the
PEP 475 (retry syscall on EINTR)&lt;/li&gt;
&lt;li&gt;test_gdb: fix regex to parse the GDB version and fix ResourceWarning on error&lt;/li&gt;
&lt;li&gt;Fix test_warnings: don't modify warnings.filters to fix random failures of
the test.&lt;/li&gt;
&lt;li&gt;Issue #24891: Fix a race condition at Python startup if the file descriptor
of stdin (0), stdout (1) or stderr (2) is closed while Python is creating
sys.stdin, sys.stdout and sys.stderr objects.&lt;/li&gt;
&lt;li&gt;Issue #24684: socket.socket.getaddrinfo() now calls
PyUnicode_AsEncodedString() instead of calling the encode() method of the
host, to handle correctly custom string with an encode() method which doesn't
return a byte string. The encoder of the IDNA codec is now called directly
instead of calling the encode() method of the string.&lt;/li&gt;
&lt;li&gt;Issue #25118: Fix a regression of Python 3.5.0 in os.waitpid() on Windows.
Add an unit test on os.waitpid()&lt;/li&gt;
&lt;li&gt;Issue #25122: Fix test_eintr, kill child process on error&lt;/li&gt;
&lt;li&gt;Issue #25155: Add _PyTime_AsTimevalTime_t() function to fix a regression:
support again years after 2038.&lt;/li&gt;
&lt;li&gt;Issue #25150: Hide the private _Py_atomic_xxx symbols from the public
Python.h header to fix a compilation error with OpenMP. PyThreadState_GET()
becomes an alias to PyThreadState_Get() to avoid ABI incompatibilies.&lt;/li&gt;
&lt;li&gt;Issue #25003: On Solaris 11.3 or newer, os.urandom() now uses the getrandom()
function instead of the getentropy() function.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>History of the Python private C API _PyTime</title><link href="https://haypo.github.io/pytime.html" rel="alternate"></link><published>2016-02-17T22:00:00+01:00</published><updated>2016-02-17T22:00:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-02-17:/pytime.html</id><summary type="html">&lt;p class="first last"&gt;History of the Python private C API _PyTime&lt;/p&gt;
</summary><content type="html">&lt;p&gt;I added functions to the private &amp;quot;pytime&amp;quot; library to convert timestamps from/to
various formats. I expected to spend a few days, at the end I spent 3 years
(2012-2015) on them!&lt;/p&gt;
&lt;div class="section" id="python-3-3"&gt;
&lt;h2&gt;Python 3.3&lt;/h2&gt;
&lt;p&gt;In 2012, I proposed the &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0410/"&gt;PEP 410 -- Use decimal.Decimal type for timestamps&lt;/a&gt; because storing timestamps as
floating point numbers looses precision. The PEP was rejected because it
modified many functions and had a bad API. At least, os.stat() got 3 new fields
(atime_ns, mtime_ns, ctime_ns): timestamps  as a number of nanoseconds
(&lt;tt class="docutils literal"&gt;int&lt;/tt&gt;).&lt;/p&gt;
&lt;p&gt;My &lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0418/"&gt;PEP 418 -- Add monotonic time, performance counter, and process time
functions&lt;/a&gt; was accepted, Python
3.3 got a new &lt;tt class="docutils literal"&gt;time.monotonic()&lt;/tt&gt; function (and a few others). Again, I spent
much more time than I expected on a problem which looked simple at the first
look.&lt;/p&gt;
&lt;p&gt;With the &lt;a class="reference external" href="http://bugs.python.org/issue14180"&gt;issue #14180&lt;/a&gt;, I added functions
to convert timestamps to the private &amp;quot;pytime&amp;quot; API to factorize the code of
various modules. Timestamps were rounded towards +infinity (ROUND_CEILING), but
it was not a deliberate choice.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-4"&gt;
&lt;h2&gt;Python 3.4&lt;/h2&gt;
&lt;p&gt;To fix correctly a performance issue in asyncio (&lt;a class="reference external" href="https://bugs.python.org/issue20311"&gt;issue20311&lt;/a&gt;), I added two rounding modes to the
pytime API: _PyTime_ROUND_DOWN (round towards zero), and _PyTime_ROUND_UP
(round away from zero). Polling for events (ex: using &lt;tt class="docutils literal"&gt;select.select()&lt;/tt&gt;) with
a non-zero timestamp must not call the underlying C level in non-blocking mode.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-5"&gt;
&lt;h2&gt;Python 3.5&lt;/h2&gt;
&lt;p&gt;When working on the &lt;a class="reference external" href="https://bugs.python.org/issue22117"&gt;issue #22117&lt;/a&gt;, I
noticed that the implementation of rounding methods was buggy for negative
timestamps. I replaced the _PyTime_ROUND_DOWN with _PyTime_ROUND_FLOOR (round
towards minus infinity), and _PyTime_ROUND_UP with _PyTime_ROUND_CEILING (round
towards infinity).&lt;/p&gt;
&lt;p&gt;This issue also introduced a new private &lt;tt class="docutils literal"&gt;_PyTime_t&lt;/tt&gt; type to support
nanosecond resolution.  The type is an opaque integer type to store timestamps.
In practice, it's a signed 64-bit integer. Since it's an integer, it's easy and
natural to compute the sum or differecence of two timestamps: &lt;tt class="docutils literal"&gt;t1 + t2&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;t2 - t1&lt;/tt&gt;. I added _PyTime_XXX() functions to create a timestamp and
_PyTime_AsXXX() functions to convert a timestamp to a different format.&lt;/p&gt;
&lt;p&gt;I had to keep three _PyTime_ObjectToXXX() functions for fromtimestamp() methods
of the datetime module. These methods must support extreme timestamps (year
1..9999), whereas _PyTime_t is &amp;quot;limited&amp;quot; to a delta of +/- 292 years (year
1678..2262).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-3-6"&gt;
&lt;h2&gt;Python 3.6&lt;/h2&gt;
&lt;p&gt;In 2015, the &lt;a class="reference external" href="http://bugs.python.org/issue23517"&gt;issue #23517&lt;/a&gt; reported that
Python 2 and Python 3 don't use the same rounding method in
datetime.datetime.fromtimestamp(): there was a difference of 1 microsecond.&lt;/p&gt;
&lt;p&gt;After a long discussion, I modified fromtimestamp() methods of the datetime
module to round to nearest with ties going away from zero (ROUND_HALF_UP), as
done in Python 2.7, as round() in all Python versions.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;It took me three years to stabilize the API and fix all issues. Well, I didn't
spend all my days on it, but it shows that handling time is not a simple issue.&lt;/p&gt;
&lt;p&gt;At the Python level, nothing changed, timestamps are still stored as float
(except of the 3 new fieleds of os.stat()).&lt;/p&gt;
&lt;p&gt;Python 3.5 only supports timezones with fixed offset, it does not support the
locale timestamp for example. Timezones are still an hot topic: the
&lt;a class="reference external" href="https://mail.python.org/mailman/listinfo/datetime-sig"&gt;datetime-sig mailing list&lt;/a&gt; was created to
enhance timezone support in Python.&lt;/p&gt;
&lt;/div&gt;
</content><category term="cpython"></category></entry><entry><title>Status of the FAT Python project, January 12, 2016</title><link href="https://haypo.github.io/fat-python-status-janv12-2016.html" rel="alternate"></link><published>2016-01-12T13:42:00+01:00</published><updated>2016-01-12T13:42:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2016-01-12:/fat-python-status-janv12-2016.html</id><summary type="html">&lt;p class="first last"&gt;Status of the FAT Python project, January 12, 2016&lt;/p&gt;
</summary><content type="html">&lt;a class="reference external image-reference" href="http://faster-cpython.readthedocs.org/fat_python.html"&gt;&lt;img alt="FAT Python project" class="align-right" src="https://haypo.github.io/images/fat_python.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;Previous status: &lt;a class="reference external" href="https://haypo.github.io/fat-python-status-nov26-2015.html"&gt;Status of the FAT Python project, November 26, 2015&lt;/a&gt;.&lt;/p&gt;
&lt;div class="section" id="summary"&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;New optimizations implemented:&lt;ul&gt;
&lt;li&gt;constant propagation&lt;/li&gt;
&lt;li&gt;constant folding&lt;/li&gt;
&lt;li&gt;dead code elimination&lt;/li&gt;
&lt;li&gt;simplify iterable&lt;/li&gt;
&lt;li&gt;replace builtin __debug__ variable with its value&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Major API refactoring to make the API more generic and reusable by other
projects, and maybe different use case.&lt;/li&gt;
&lt;li&gt;Work on 3 different Python Enhancement Proposals (PEP): API for pluggable
static optimizers and function specialization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The two previously known major bugs, &amp;quot;Wrong Line Numbers (and Tracebacks)&amp;quot; and
&amp;quot;exec(code, dict)&amp;quot;, are now fixed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="python-enhancement-proposals-pep"&gt;
&lt;h2&gt;Python Enhancement Proposals (PEP)&lt;/h2&gt;
&lt;p&gt;I proposed an API for to support function specialization and static optimizers.
I splitted changes in 3 different Python Enhancement Proposals (PEP):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0509/"&gt;PEP 509 - Add a private version to dict&lt;/a&gt;: &amp;quot;Add a new private version to
builtin &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; type, incremented at each change, to implement fast guards
on namespaces.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0510/"&gt;PEP 510 - Specialize functions&lt;/a&gt;: &amp;quot;Add functions to the Python C
API to specialize pure Python functions: add specialized codes with guards.
It allows to implement static optimizers respecting the Python semantics.&amp;quot;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://www.python.org/dev/peps/pep-0511/"&gt;PEP 511 - API for AST transformers&lt;/a&gt;: &amp;quot;Propose an API to
support AST transformers.&amp;quot;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The PEP 509 was sent to the python-ideas mailing list for a first round, and
then to python-dev mailing list.  The PEP 510 was sent to python-ideas to a
first round. The last PEP was not published yet, I'm still working on it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="major-api-refactor"&gt;
&lt;h2&gt;Major API refactor&lt;/h2&gt;
&lt;p&gt;The API has been deeply refactored to write the Python Enhancement Proposals.&lt;/p&gt;
&lt;p&gt;First set of changes for function specialization (PEP 510):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;astoptimizer now adds &lt;tt class="docutils literal"&gt;import fat&lt;/tt&gt; to optimized code when specialization is
used&lt;/li&gt;
&lt;li&gt;Remove the function subtype: add directly the &lt;tt class="docutils literal"&gt;specialize()&lt;/tt&gt; method to
functions&lt;/li&gt;
&lt;li&gt;Add support of any callable object to &lt;tt class="docutils literal"&gt;func.specialize()&lt;/tt&gt;, not only code
object (bytecode)&lt;/li&gt;
&lt;li&gt;Create guard objects:&lt;ul&gt;
&lt;li&gt;fat.Guard&lt;/li&gt;
&lt;li&gt;fat.GuardArgType&lt;/li&gt;
&lt;li&gt;fat.GuardBuiltins&lt;/li&gt;
&lt;li&gt;fat.GuardDict&lt;/li&gt;
&lt;li&gt;fat.GuardFunc&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add functions to create guards:&lt;ul&gt;
&lt;li&gt;fat.GuardGlobals&lt;/li&gt;
&lt;li&gt;fat.GuardTypeDict&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Move code.replace_consts() to fat.replace_consts()&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Second set of changes for AST transformers (PEP 511):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Add sys.implementation.ast_transformers and sys.implementation.optim_tag&lt;/li&gt;
&lt;li&gt;Rename sys.asthook to sys.ast_transformers&lt;/li&gt;
&lt;li&gt;Add -X fat command line option to enable the FAT mode: register the
astoptimizer in AST transformers&lt;/li&gt;
&lt;li&gt;Replace -F command line option with -o OPTIM_TAG&lt;/li&gt;
&lt;li&gt;Remove sys.flags.fat (Python flag) and Py_FatPython (C variable)&lt;/li&gt;
&lt;li&gt;Rewrite how an AST transformer is registered&lt;/li&gt;
&lt;li&gt;importlib skips .py if optim_tag is not 'opt' and required AST transformers
are missing. Raise ImportError if the .pyc file is missing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Third set of changes for dictionary versionning, updates after the first round
of the PEP 509 on python-ideas:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Remove dict.__version__ read-only property: the version is now only
accessible from the C API&lt;/li&gt;
&lt;li&gt;Change the type of the C field &lt;tt class="docutils literal"&gt;ma_version&lt;/tt&gt; from &lt;tt class="docutils literal"&gt;size_t&lt;/tt&gt; to &lt;tt class="docutils literal"&gt;unsigned
PY_INT64_T&lt;/tt&gt; to also use 64-bit unsigned integer on 32-bit platforms. The
risk of missing a change in a guard with a 32-bit version is too high,
whereas the risk with a 64-bit version is very very low.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Fourth set of changes for function specialization, updates after the first round
of the PEP 510 on python-ideas:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Remove func.specialize() and func.get_specialized() at the Python level,
replace them with C functions. Expose them again as fat.specialize(func, ...)
and fat.get_specialized(func)&lt;/li&gt;
&lt;li&gt;fat.get_specialized() now returns a list of tuples, instead of a list of dict&lt;/li&gt;
&lt;li&gt;Make fat.Guard type private: rename it to fat._Guard&lt;/li&gt;
&lt;li&gt;Add fat.PyGuard: toy to implement a guard in pure Python&lt;/li&gt;
&lt;li&gt;Guard C API: rename first_check to init and support reporting errors&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="change-log"&gt;
&lt;h2&gt;Change log&lt;/h2&gt;
&lt;p&gt;Detailed changes of the FAT Python between November 24, 2015 and January 12,
2016.&lt;/p&gt;
&lt;div class="section" id="end-of-november"&gt;
&lt;h3&gt;End of november&lt;/h3&gt;
&lt;p&gt;Major change:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Add a __version__ read-only property to dict, remove the verdict subtype of
dict. As a consequence, dictionary guards now hold a strong reference to the
dict value&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Minor changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Allocate dynamically memory for specialized code and guards, don't use fixed-size
arrays anymore&lt;/li&gt;
&lt;li&gt;astoptimizer: enhance scope detection&lt;/li&gt;
&lt;li&gt;optimize astoptimizer: don't copy a whole AST tree anymore with
copy.deepcopy(), only copy modified nodes.&lt;/li&gt;
&lt;li&gt;Add Config.max_constant_size&lt;/li&gt;
&lt;li&gt;Reenable checks on cell variables: allow cell variables if they are the same&lt;/li&gt;
&lt;li&gt;Reenable optimizations on methods calling super(), but never copy super()
builtin to constants. If super() is replaced with a string, the required free
variable (reference to the current class) is not created by the compiler&lt;/li&gt;
&lt;li&gt;Add PureBuiltin config&lt;/li&gt;
&lt;li&gt;NodeVisitor now calls generic_visit() before visit_XXX()&lt;/li&gt;
&lt;li&gt;Loop unrolling now also optimizes tuple iterators&lt;/li&gt;
&lt;li&gt;At the end of Python initialization, create a copy of the builtins dictionary
to be able later to detect if a builtin name was replaced.&lt;/li&gt;
&lt;li&gt;Implement collections.UserDict.__version__&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="december-first-half"&gt;
&lt;h3&gt;December (first half)&lt;/h3&gt;
&lt;p&gt;Major changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Implement 4 new optimizations:&lt;ul&gt;
&lt;li&gt;constant propagation&lt;/li&gt;
&lt;li&gt;constant folding&lt;/li&gt;
&lt;li&gt;replace builtin __debug__ variable with its value&lt;/li&gt;
&lt;li&gt;dead code elimination&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Add support of per module configuration using an __astoptimizer__ variable&lt;/li&gt;
&lt;li&gt;code.co_lnotab now supports negative line number delta.  Change the type of
line number delta in co_lnotab from unsigned 8-bit integer to signed 8-bit
integer. This change fixes almost all issues about line numbers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Minor changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Change .pyc magic number to 3600&lt;/li&gt;
&lt;li&gt;Remove unused fat.specialized_method() function&lt;/li&gt;
&lt;li&gt;Remove Lib/fat.py, rename Modules/_fat.c to Modules/fat.c: fat module is now
only implemented in C&lt;/li&gt;
&lt;li&gt;Fix more tests of the Python test suite&lt;/li&gt;
&lt;li&gt;A builtin guard now adds a guard on globals. Ignore also the specialization
if globals()[name] already exists.&lt;/li&gt;
&lt;li&gt;Ignore duplicated guards&lt;/li&gt;
&lt;li&gt;Implement namespace following the control flow for constant propagation&lt;/li&gt;
&lt;li&gt;Config.max_int_bits becomes a simple integer&lt;/li&gt;
&lt;li&gt;Fix bytecode compilation for tuple constants. Don't merge (0, 0) and (0.0,
0.0) constants, they are different.&lt;/li&gt;
&lt;li&gt;Call more builtin functions&lt;/li&gt;
&lt;li&gt;Optimize the optimizer: write a metaclass to discover visitors when the class
is created, not when the class is instanciated&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="december-second-half"&gt;
&lt;h3&gt;December (second half)&lt;/h3&gt;
&lt;p&gt;Major changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Implement &amp;quot;simplify iterable&amp;quot; optimization. The loop unrolling optimization
now relies on it to replace &lt;tt class="docutils literal"&gt;range(n)&lt;/tt&gt;.&lt;/li&gt;
&lt;li&gt;Split the function optimization in two stages: first apply optimizations
which don't require specialization, then apply optimizations which
require specialization.&lt;/li&gt;
&lt;li&gt;Replace the builtin __fat__ variable with a new sys.flags.fat flag&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Minor changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Extend optimizations to optimize more cases (more builtins, more loop
unrolling, remove more dead code, etc.)&lt;/li&gt;
&lt;li&gt;Add Config.logger attribute. astoptimize logs into sys.stderr when Python is
started in verbose mode (python3 -v)&lt;/li&gt;
&lt;li&gt;Move func.patch_constants() to code.replace_consts()&lt;/li&gt;
&lt;li&gt;Enhance marshal to fix tests: call frozenset() to get the empty frozenset
singleton&lt;/li&gt;
&lt;li&gt;Don't remove code which must raise a SyntaxError. Don't remove code
containing the continue instruction.&lt;/li&gt;
&lt;li&gt;Restrict GlobalNonlocalVisitor to the current namespace&lt;/li&gt;
&lt;li&gt;Emit logs when optimizations are skipped&lt;/li&gt;
&lt;li&gt;Use some maths to avoid optimization pow() if result is an integer and will
be larger than the configuration. For example, don't optimize 2 ** (2**100).&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="january"&gt;
&lt;h3&gt;January&lt;/h3&gt;
&lt;p&gt;Major changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;astoptimizer now produces a single builtin guard with all names,
instead of a guard per name.&lt;/li&gt;
&lt;li&gt;Major API refactoring detailed in a dedicated section above&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Minor changes:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Start to write PEPs&lt;/li&gt;
&lt;li&gt;Dictionary guards now expect a list of names, instead of a single name, to
reduce the cost of guards.&lt;/li&gt;
&lt;li&gt;GuardFunc now uses a strong reference to the function, instead of a weak
reference to simplify the code&lt;/li&gt;
&lt;li&gt;Initialize dictionary version to 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="optimization"></category><category term="fatpython"></category></entry><entry><title>Status of the FAT Python project, November 26, 2015</title><link href="https://haypo.github.io/fat-python-status-nov26-2015.html" rel="alternate"></link><published>2015-11-26T17:30:00+01:00</published><updated>2015-11-26T17:30:00+01:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2015-11-26:/fat-python-status-nov26-2015.html</id><summary type="html">&lt;p class="first last"&gt;Status of the FAT Python project, November 26, 2015&lt;/p&gt;
</summary><content type="html">&lt;a class="reference external image-reference" href="http://faster-cpython.readthedocs.org/fat_python.html"&gt;&lt;img alt="FAT Python project" class="align-right" src="https://haypo.github.io/images/fat_python.jpg" /&gt;&lt;/a&gt;
&lt;p&gt;Previous status: [python-dev] &lt;a class="reference external" href="https://mail.python.org/pipermail/python-dev/2015-November/142113.html"&gt;Second milestone of FAT Python&lt;/a&gt;
(Nov 4, 2015).&lt;/p&gt;
&lt;div class="section" id="documentation"&gt;
&lt;h2&gt;Documentation&lt;/h2&gt;
&lt;p&gt;I combined the documentation of various optimizations projects into a single
documentation: &lt;a class="reference external" href="http://faster-cpython.readthedocs.org/"&gt;Faster CPython&lt;/a&gt;.
My previous optimizations projects:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://faster-cpython.readthedocs.org/old_ast_optimizer.html"&gt;&amp;quot;old&amp;quot; astoptimizer&lt;/a&gt; (now
replaced with a &amp;quot;new&amp;quot; astoptimizer included in the FAT Python)&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://faster-cpython.readthedocs.org/registervm.html"&gt;registervm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://faster-cpython.readthedocs.org/readonly.html"&gt;read-only Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The FAT Python project has its own page: &lt;a class="reference external" href="http://faster-cpython.readthedocs.org/fat_python.html"&gt;FAT Python project&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="copy-builtins-to-constants-optimization"&gt;
&lt;h2&gt;Copy builtins to constants optimization&lt;/h2&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;LOAD_GLOBAL&lt;/tt&gt; instruction is used to load a builtin function.  The
instruction requires two dictionary lookup: one in the global namespace (which
almost always fail) and then in the builtin namespaces.&lt;/p&gt;
&lt;p&gt;It's rare to replace builtins, so the idea here is to replace the dynamic
&lt;tt class="docutils literal"&gt;LOAD_GLOBAL&lt;/tt&gt; instruction with a static &lt;tt class="docutils literal"&gt;LOAD_CONST&lt;/tt&gt; instruction which
loads the function from a C array, a fast O(1) lookup.&lt;/p&gt;
&lt;p&gt;It is not possible to inject a builtin function during the compilation. Python
code objects are serialized by the marshal module which only support simple
types like integers, strings and tuples, not functions. The trick is to modify
the constants at runtime when the module is loaded. I added a new
&lt;tt class="docutils literal"&gt;patch_constants()&lt;/tt&gt; method to functions.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def log(message):
    print(message)
&lt;/pre&gt;
&lt;p&gt;This function is specialized to:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def log(message):
    'LOAD_GLOBAL print'(message)
log.patch_constants({'LOAD_GLOBAL print': print})
&lt;/pre&gt;
&lt;p&gt;The specialized bytecode uses two guards on builtin and global namespaces to
disable the optimization if the builtin function is replaced.&lt;/p&gt;
&lt;p&gt;See &lt;a class="reference external" href="https://faster-cpython.readthedocs.org/fat_python.html#copy-builtin-functions-to-constants"&gt;Copy builtin functions to constants&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="loop-unrolling-optimization"&gt;
&lt;h2&gt;Loop unrolling optimization&lt;/h2&gt;
&lt;p&gt;A simple optimization is to &amp;quot;unroll&amp;quot; a loop to reduce the cost of loops. The
optimization generates assignement statements (for the loop index variable)
and duplicates the loop body.&lt;/p&gt;
&lt;p&gt;Example with a &lt;tt class="docutils literal"&gt;range()&lt;/tt&gt; iterator:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def func():
    for i in (1, 2, 3):
        print(i)
&lt;/pre&gt;
&lt;p&gt;The function is specialized to:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
def func():
    i = 1
    print(i)

    i = 2
    print(i)

    i = 3
    print(i)
&lt;/pre&gt;
&lt;p&gt;If the iterator uses the builtin &lt;tt class="docutils literal"&gt;range&lt;/tt&gt; function, two guards are
required on builtin and global namespaces.&lt;/p&gt;
&lt;p&gt;The optimization also handles tuple iterator. No guard is needed in this case
(the code is always optimized).&lt;/p&gt;
&lt;p&gt;See &lt;a class="reference external" href="https://faster-cpython.readthedocs.org/fat_python.html#loop-unrolling"&gt;Loop unrolling&lt;/a&gt;
for more information.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="lot-of-enhancements-of-the-ast-optimizer"&gt;
&lt;h2&gt;Lot of enhancements of the AST optimizer&lt;/h2&gt;
&lt;p&gt;New optimizations helped to find bugs in the &lt;a class="reference external" href="https://faster-cpython.readthedocs.org/new_ast_optimizer.html"&gt;AST optimizer&lt;/a&gt;. Many fixes
and various enhancements were done in the AST optimizer.&lt;/p&gt;
&lt;p&gt;The number of lines of code more than doubled: 500 to 1200 lines.&lt;/p&gt;
&lt;p&gt;Optimization: &lt;tt class="docutils literal"&gt;copy.deepcopy()&lt;/tt&gt; is no more used to duplicate a full tree. The
new &lt;tt class="docutils literal"&gt;NodeTransformer&lt;/tt&gt; class now only copies a single node, if at least one
field is modified.&lt;/p&gt;
&lt;p&gt;The &lt;tt class="docutils literal"&gt;VariableVisitor&lt;/tt&gt; class which detects local and global variables was
heavily modified. It understands much more kinds of AST node: &lt;tt class="docutils literal"&gt;For&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;AugAssign&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;AsyncFunctionDef&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;ClassDef&lt;/tt&gt;, etc. It now also detects non-local
variables (&lt;tt class="docutils literal"&gt;nonlocal&lt;/tt&gt; keyword). The scope is now limited to the current
function, it doesn't enter inside nested &lt;tt class="docutils literal"&gt;DictComp&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;FunctionDef&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;Lambda&lt;/tt&gt;, etc. These nodes create a new separated namespace.&lt;/p&gt;
&lt;p&gt;The optimizer is now able to optimize a function without guards: it's needed to
unroll a loop using a tuple as iterator.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="known-bugs"&gt;
&lt;h2&gt;Known bugs&lt;/h2&gt;
&lt;p&gt;See the &lt;a class="reference external" href="https://hg.python.org/sandbox/fatpython/file/0d30dba5fa64/TODO.rst"&gt;TODO.rst file&lt;/a&gt; for
known bugs.&lt;/p&gt;
&lt;div class="section" id="wrong-line-numbers-and-tracebacks"&gt;
&lt;h3&gt;Wrong Line Numbers (and Tracebacks)&lt;/h3&gt;
&lt;p&gt;AST nodes have &lt;tt class="docutils literal"&gt;lineno&lt;/tt&gt; and &lt;tt class="docutils literal"&gt;col_offset&lt;/tt&gt; fields, so an AST optimizer is not
&amp;quot;supposed&amp;quot; to break line numbers. In practice, line numbers, and so tracebacks,
are completly wrong in FAT mode. The problem is probably that AST optimizer can
copy and move instructions. Line numbers are no more motononic. CPython
probably don't handle this case (negative line delta).&lt;/p&gt;
&lt;p&gt;It should be possible to fix it, but right now I prefer to focus on new
optimizations and fix other bugs.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="exec-code-dict"&gt;
&lt;h3&gt;exec(code, dict)&lt;/h3&gt;
&lt;p&gt;In FAT mode, some optimizations require guards on the global namespace.
If &lt;tt class="docutils literal"&gt;exec()&lt;/tt&gt; if called with a Python &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; for globals, an exception
is raised because &lt;tt class="docutils literal"&gt;func.specialize()&lt;/tt&gt; requires a &lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt; for
globals.&lt;/p&gt;
&lt;p&gt;It's not possible to convert implicitly the &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; to a &lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt;,
because the &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; is expected to be mutated, and the guards be will on
&lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt; not on the original &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;I worked around the bug by creating manually a &lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt; in FAT mode,
instead of a &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;This bug will go avoid if the versionning feature is moved directly into
the builtin &lt;tt class="docutils literal"&gt;dict&lt;/tt&gt; type (and the &lt;tt class="docutils literal"&gt;fat.verdict&lt;/tt&gt; type is removed).&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</content><category term="optimization"></category><category term="fatpython"></category></entry><entry><title>Port your Python 2 applications to Python 3 with sixer</title><link href="https://haypo.github.io/python3-sixer.html" rel="alternate"></link><published>2015-06-16T15:00:00+02:00</published><updated>2015-06-16T15:00:00+02:00</updated><author><name>Victor Stinner</name></author><id>tag:haypo.github.io,2015-06-16:/python3-sixer.html</id><summary type="html">&lt;p class="first last"&gt;Port your Python 2 applications to Python 3 with sixer&lt;/p&gt;
</summary><content type="html">&lt;div class="section" id="from-2to3-to-2to6"&gt;
&lt;h2&gt;From 2to3 to 2to6&lt;/h2&gt;
&lt;p&gt;When Python 3.0 was released, the official statement was to port your
application using &lt;a class="reference external" href="https://docs.python.org/3.5/library/2to3.html"&gt;2to3&lt;/a&gt; and
drop Python 2 support. It didn't work because you had to port all libraries
first. If a library drops Python 2 support, existing applications running on
Python 2 cannot use this library anymore.&lt;/p&gt;
&lt;p&gt;This chicken-and-egg issue was solved by the creation of the &lt;a class="reference external" href="https://pythonhosted.org/six/"&gt;six module&lt;/a&gt; by &lt;a class="reference external" href="https://benjamin.pe/"&gt;Benjamin Peterson&lt;/a&gt;. Thank you so much Benjamin! Using the six module, it
is possible to write a single code base working on Python 2 and Python 3.&lt;/p&gt;
&lt;p&gt;2to3 was hacked to create the &lt;a class="reference external" href="http://python-modernize.readthedocs.org/"&gt;modernize&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/limodou/2to6"&gt;2to6&lt;/a&gt; projects to &lt;em&gt;add Python 3 support&lt;/em&gt; without
loosing Python 2 support. Problem solved!&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="creation-of-the-sixer-tool"&gt;
&lt;h2&gt;Creation of the sixer tool&lt;/h2&gt;
&lt;p&gt;Problem solved? Well, not for my specific use case. I'm porting the huge
OpenStack project to Python 3. modernize and 2to6 modify a lot of things at
once, add unwanted changes (ex: add &lt;tt class="docutils literal"&gt;from __future__ import absolute_import&lt;/tt&gt;
at the top of each file), and don't respect the OpenStack coding style
(especially the &lt;a class="reference external" href="http://docs.openstack.org/developer/hacking/#imports"&gt;complex rules to sort and group Python imports&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;I wrote the &lt;a class="reference external" href="https://pypi.python.org/pypi/sixer"&gt;sixer&lt;/a&gt; project to
&lt;em&gt;generate&lt;/em&gt; patches for OpenStack. The problem is that OpenStack code changes
very quickly, so it's common to have to fix conflicts the day after submiting
a change. At the beginning, it took at least one week to get Python 3 changes
merged, whereas many changes are merged every day, so being able to regenerate
patches helped a lot.&lt;/p&gt;
&lt;p&gt;I created the &lt;a class="reference external" href="https://pypi.python.org/pypi/sixer"&gt;sixer&lt;/a&gt; tool using a list
of regular expressions to replace a pattern with another. For example, it
replaces &lt;tt class="docutils literal"&gt;dict.itervalues()&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;six.itervalues(dict)&lt;/tt&gt;. The code was
very simple.  The most difficult part was to respect the OpenStack coding
style for Python imports.&lt;/p&gt;
&lt;p&gt;sixer is a success since its creationg, it helped me to fix the all obvious
Python 3 issues: replace &lt;tt class="docutils literal"&gt;unicode(x)&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;six.text_type(x)&lt;/tt&gt;, replace
&lt;tt class="docutils literal"&gt;dict.itervalues()&lt;/tt&gt; with &lt;tt class="docutils literal"&gt;six.itervalues(dict)&lt;/tt&gt;, etc. These changes are
simple, but it's boring to have to modify manually many files. The OpenStack
Nova project has almost 1500 Python files for example.&lt;/p&gt;
&lt;p&gt;The development version of sixer supports the following operations:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;all&lt;/li&gt;
&lt;li&gt;basestring&lt;/li&gt;
&lt;li&gt;dict0&lt;/li&gt;
&lt;li&gt;dict_add&lt;/li&gt;
&lt;li&gt;iteritems&lt;/li&gt;
&lt;li&gt;iterkeys&lt;/li&gt;
&lt;li&gt;itertools&lt;/li&gt;
&lt;li&gt;itervalues&lt;/li&gt;
&lt;li&gt;long&lt;/li&gt;
&lt;li&gt;next&lt;/li&gt;
&lt;li&gt;raise&lt;/li&gt;
&lt;li&gt;six_moves&lt;/li&gt;
&lt;li&gt;stringio&lt;/li&gt;
&lt;li&gt;unicode&lt;/li&gt;
&lt;li&gt;urllib&lt;/li&gt;
&lt;li&gt;xrange&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="creation-of-the-sixer-test-suite"&gt;
&lt;h2&gt;Creation of the Sixer Test Suite&lt;/h2&gt;
&lt;p&gt;Slowly, I added more and more patterns to sixer. The code became too complex
to be able to check regressions manually, so I also started to write unit
tests. Now each operation has at least one unit test. Some complex operations
have four tests or more.&lt;/p&gt;
&lt;p&gt;At the beginning, tests called directly the Python function. It is fast and
convenient, but it failed to catch regressions on the command line program.
So I added tests running sixer has a blackbox: pass an input file and check
the output file. Then I added specific tests on the code parsing command line
options.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="the-new-all-operation"&gt;
&lt;h2&gt;The new &amp;quot;all&amp;quot; operation&lt;/h2&gt;
&lt;p&gt;At the beginning, I used sixer to generate a patch for a single pattern. For
example, replace &lt;tt class="docutils literal"&gt;unicode()&lt;/tt&gt; in a whole project.&lt;/p&gt;
&lt;p&gt;Later, I started to use it differently: I fixed all Python 3 issues at once,
but only in some selected files. I did that when we reached a minimum set of
tests which pass on Python 3 to have a green py34 check on Jenkins. Then we
ported tests one by one. It's better to write short patches, they are easier
and faster to review. And the review process is the bottlebeck of the
OpenStack development process.&lt;/p&gt;
&lt;p&gt;To fix all Python 3 at once, I added an &lt;tt class="docutils literal"&gt;all&lt;/tt&gt; operation which simply applies
sequentially each operation. So &lt;tt class="docutils literal"&gt;sixer&lt;/tt&gt; can now be used as &lt;tt class="docutils literal"&gt;modernize&lt;/tt&gt; and
&lt;tt class="docutils literal"&gt;2to6&lt;/tt&gt; to fix all Python 3 issues at once in a whole project.&lt;/p&gt;
&lt;p&gt;I also added the ability to pass filenames instead of having to pass a
directory to modify all files in all subdirectories.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="new-urllib-six-moves-and-stringio-operations"&gt;
&lt;h2&gt;New urllib, six_moves and stringio operations&lt;/h2&gt;
&lt;div class="section" id="urllib"&gt;
&lt;h3&gt;urllib&lt;/h3&gt;
&lt;p&gt;I tried to keep the sixer code simple. But some changes are boring to write,
like replacing &lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt; imports &lt;tt class="docutils literal"&gt;six.moves.urllib&lt;/tt&gt; imports. Python 2 has 3
modules (&lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;urllib2&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;urlparse&lt;/tt&gt;), whereas Pytohn 3 uses a
single &lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt; namespace with submodules (&lt;tt class="docutils literal"&gt;urllib.request&lt;/tt&gt;,
&lt;tt class="docutils literal"&gt;urllib.parse&lt;/tt&gt;, &lt;tt class="docutils literal"&gt;urllib.error&lt;/tt&gt;). Some Python 2 functions moved to one
submodule, whereas others moved to another submodules. It required to know
well the old and new layout.&lt;/p&gt;
&lt;p&gt;After loosing many hours to write manually patches for &lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt;, I decided
to add a &lt;tt class="docutils literal"&gt;urllib&lt;/tt&gt; operation. In fact, it was so not long to implement it,
compared to the time taken to write patches manually.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="stringio"&gt;
&lt;h3&gt;stringio&lt;/h3&gt;
&lt;p&gt;Handling StringIO is also a little bit tricky because String.StringIO and
String.cStringIO don't have the same performance on Python 2. Producing
patches without killing performances require to pick the right module or
symbol from six: &lt;tt class="docutils literal"&gt;six.StringIO()&lt;/tt&gt; or &lt;tt class="docutils literal"&gt;six.moves.cStringIO.StringIO&lt;/tt&gt; for
example.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="six-moves"&gt;
&lt;h3&gt;six_moves&lt;/h3&gt;
&lt;p&gt;The generic &lt;tt class="docutils literal"&gt;six_moves&lt;/tt&gt; operation replaces various Python 2 imports with
imports from &lt;tt class="docutils literal"&gt;six.moves&lt;/tt&gt;:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;BaseHTTPServer&lt;/li&gt;
&lt;li&gt;ConfigParser&lt;/li&gt;
&lt;li&gt;Cookie&lt;/li&gt;
&lt;li&gt;HTMLParser&lt;/li&gt;
&lt;li&gt;Queue&lt;/li&gt;
&lt;li&gt;SimpleHTTPServer&lt;/li&gt;
&lt;li&gt;SimpleXMLRPCServer&lt;/li&gt;
&lt;li&gt;__builtin__&lt;/li&gt;
&lt;li&gt;cPickle&lt;/li&gt;
&lt;li&gt;cookielib&lt;/li&gt;
&lt;li&gt;htmlentitydefs&lt;/li&gt;
&lt;li&gt;httplib&lt;/li&gt;
&lt;li&gt;repr&lt;/li&gt;
&lt;li&gt;xmlrpclib&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class="section" id="kiss-emit-warnings-instead-of-complex-implementation"&gt;
&lt;h2&gt;KISS: emit warnings instead of complex implementation&lt;/h2&gt;
&lt;p&gt;As I wrote, I tried to keep sixer simple (KISS principle: Keep It Simple,
Stupid). I'm also lazy, I didn't try to write a perfect tool. I don't want to
spend hours on the sixer project.&lt;/p&gt;
&lt;p&gt;When it was too tricky to make a decision or to implement a pattern, sixer
emits &amp;quot;warnings&amp;quot; instead. For example, a warning is emitted on
&lt;tt class="docutils literal"&gt;def next(self):&lt;/tt&gt; to remind that a &lt;tt class="docutils literal"&gt;__next__ = next&lt;/tt&gt; alias is probably
needed on this class for Python 3.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="conclusion"&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The sixer tool is incomplete and generates invalid changes. For example, it
replaces patterns in comments, docstrings and strings, whereas usually these
changes don't make sense. But I'm happy because the tool helped me a lot
for to port OpenStack, it saved me hours.&lt;/p&gt;
&lt;p&gt;I hope that the tool will now be useful to others! Don't hesitate to give me
feedback.&lt;/p&gt;
&lt;/div&gt;
</content><category term="python3"></category><category term="sixer"></category></entry></feed>